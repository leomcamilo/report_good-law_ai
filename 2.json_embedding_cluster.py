import pandas as pd
import numpy as np
import os
import json
import nltk
from sentence_transformers import SentenceTransformer
import umap
import hdbscan
from pathlib import Path

def carregar_dados_json(caminho_json):
    """Carrega dados de um arquivo JSON existente"""
    try:
        with open(caminho_json, 'r', encoding='utf-8') as f:
            dados_json = json.load(f)
        
        # Converter JSON para DataFrame
        df_projetos_unico = pd.DataFrame(dados_json)
        print(f"Dados carregados do JSON: {df_projetos_unico.shape[0]} projetos")
        print(f"Colunas dispon√≠veis: {list(df_projetos_unico.columns)}")
        
        return df_projetos_unico
        
    except FileNotFoundError:
        raise FileNotFoundError(f"Arquivo JSON n√£o encontrado: {caminho_json}")
    except json.JSONDecodeError as e:
        raise ValueError(f"Erro ao decodificar JSON: {str(e)}")
    except Exception as e:
        raise Exception(f"Erro ao carregar dados do JSON: {str(e)}")

def criar_embeddings(df):
    """Cria embeddings otimizados focando apenas no conte√∫do t√©cnico do projeto"""
    
    # Verificar se as colunas existem
    colunas_necessarias = ['empresa', 'projeto', 'projeto_multianual']
    colunas_faltantes = [col for col in colunas_necessarias if col not in df.columns]
    
    if colunas_faltantes:
        raise ValueError(f"Colunas faltantes no DataFrame: {colunas_faltantes}")
    
    # Baixar stopwords do NLTK se necess√°rio
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        print("Baixando stopwords do NLTK...")
        nltk.download('stopwords')
    
    # Carregar stopwords do NLTK
    stopwords_nltk = set(nltk.corpus.stopwords.words('portuguese'))
    
    # LISTA DE STOP WORDS CUSTOMIZADA PARA PROJETOS DE P&D
    stop_words_personalizadas = {
        # Stop words b√°sicas em portugu√™s
        'de', 'da', 'do', 'das', 'dos', 'a', 'o', 'as', 'os', 'e', 'ou', 'para', 'por', 'com', 'em', 'no', 'na', 'nos', 'nas',
        'um', 'uma', 'uns', 'umas', 'se', 'que', 'como', 'quando', 'onde', 'qual', 'quais', 'todo', 'toda', 'todos', 'todas',
        'esse', 'essa', 'esses', 'essas', 'este', 'esta', 'estes', 'estas', 'aquele', 'aquela', 'aqueles', 'aquelas',
        'seu', 'sua', 'seus', 'suas', 'meu', 'minha', 'meus', 'minhas', 'nosso', 'nossa', 'nossos', 'nossas',
        'muito', 'mais', 'menos', 'bem', 'j√°', 'ainda', 'tamb√©m', 'apenas', 'sempre', 'nunca', 'sim', 'n√£o',
        
        # Stop words espec√≠ficas para contexto de P&D e Lei do Bem
        'projeto', 'projetos', 'empresa', 'desenvolvimento', 'pesquisa', 'inova√ß√£o', 'tecnologia', 'tecnol√≥gico', 'tecnol√≥gica',
        'atividade', 'atividades', 'processo', 'processos', 'produto', 'produtos', 'sistema', 'sistemas',
        'm√©todo', 'm√©todos', 't√©cnica', 't√©cnicas', 'metodologia', 'metodologias', 'an√°lise', 'estudo', 'estudos',
        'resultado', 'resultados', 'objetivo', 'objetivos', 'meta', 'metas', 'finalidade', 'prop√≥sito',
        'aplica√ß√£o', 'aplica√ß√µes', 'utiliza√ß√£o', 'uso', 'implementa√ß√£o', 'execu√ß√£o', 'realiza√ß√£o',
        '√°rea', 'setor', 'segmento', 'campo', 'dom√≠nio', 'ramo', 'categoria', 'tipo', 'tipos',
        'forma', 'formas', 'modo', 'modos', 'maneira', 'maneiras', 'meio', 'meios', 'atrav√©s', 'mediante',
        'sendo', 'tendo', 'fazendo', 'realizando', 'desenvolvendo', 'criando', 'produzindo', 'gerando',
        'assim', 'ent√£o', 'portanto', 'dessa', 'desta', 'nesta', 'nessa', 'neste', 'nesse',
        'partir', 'base', 'bases', 'fundamento', 'fundamenta√ß√£o', 'princ√≠pio', 'princ√≠pios',
        'elemento', 'elementos', 'componente', 'componentes', 'parte', 'partes', 'aspecto', 'aspectos',
        'caracter√≠stica', 'caracter√≠sticas', 'propriedade', 'propriedades', 'atributo', 'atributos',
        
        # Palavras muito gen√©ricas do contexto empresarial
        'mercado', 'cliente', 'clientes', 'consumidor', 'consumidores', 'setor', 'ind√∫stria', 'industrial',
        'comercial', 'econ√¥mico', 'econ√¥mica', 'neg√≥cio', 'neg√≥cios', 'valor', 'custo', 'custos',
        'receita', 'lucro', 'investimento', 'recurso', 'recursos', 'capital', 'financeiro', 'financeira',
        
        # Conectivos e preposi√ß√µes comuns
        'sobre', 'sob', 'entre', 'durante', 'ap√≥s', 'antes', 'depois', 'dentro', 'fora', 'acima', 'abaixo',
        'contra', 'sem', 'at√©', 'desde', 'perante', 'conforme', 'segundo', 'inclusive', 'exceto', 'salvo',
        
        # Artigos e pronomes adicionais
        'isto', 'isso', 'aquilo', 'algo', 'algu√©m', 'ningu√©m', 'nada', 'tudo', 'cada', 'outro', 'outra', 'outros', 'outras',
        'primeiro', 'primeira', '√∫ltimo', '√∫ltima', '√∫nico', '√∫nica', 'mesmo', 'mesma', 'pr√≥prio', 'pr√≥pria',
        
        # Palavras temporais gen√©ricas
        'ano', 'anos', 'm√™s', 'meses', 'dia', 'dias', 'hora', 'horas', 'tempo', 'per√≠odo', 'per√≠odos',
        'momento', 'momentos', 'fase', 'fases', 'etapa', 'etapas', 'in√≠cio', 'fim', 'final'
    }
    
    # Unir stopwords do NLTK com as personalizadas (sem duplicatas)
    stop_words_completas = stopwords_nltk.union(stop_words_personalizadas)
    
    def remover_stop_words(texto):
        """Remove stop words do texto preservando contexto t√©cnico"""
        if pd.isna(texto) or not texto:
            return ""
        
        # Converter para min√∫sculas e dividir em palavras
        palavras = str(texto).lower().split()
        
        # Filtrar stop words, mas manter palavras t√©cnicas importantes
        palavras_filtradas = []
        for palavra in palavras:
            # Remover pontua√ß√£o b√°sica
            palavra_limpa = palavra.strip('.,!?;:()"\'[]{}')
            
            # Manter palavra se:
            # 1. N√£o √© stop word
            # 2. Tem mais de 3 caracteres (palavras t√©cnicas tendem a ser maiores)
            # 3. Cont√©m n√∫meros (c√≥digos, vers√µes, etc.)
            if (palavra_limpa not in stop_words_completas and 
                (len(palavra_limpa) > 3 or any(c.isdigit() for c in palavra_limpa))):
                palavras_filtradas.append(palavra_limpa)
        
        return ' '.join(palavras_filtradas)
    
    # ESTRAT√âGIA: Focar apenas no conte√∫do t√©cnico do projeto
    def extrair_conteudo_tecnico(projeto_texto):
        """Extrai apenas o conte√∫do t√©cnico relevante SEM aplicar stop words ainda"""
        import re
        
        # Extrair descri√ß√£o, elemento tecnol√≥gico, desafio tecnol√≥gico
        patterns = {
            'descricao': r'DESCRI√á√ÉO:\s*(.*?)\s*TIPO \(',
            'elemento_tech': r'ELEMENTO TECNOL√ìGICO:\s*(.*?)\s*DESAFIO',
            'desafio_tech': r'DESAFIO TECNOL√ìGICO:\s*(.*?)\s*METODOLOGIA',
            'metodologia': r'METODOLOGIA:\s*(.*?)\s*INFORMA√á√ÉO',
            'area': r'AREA:\s*(.*?)\s*PALAVRAS',
            'palavras_chave': r'PALAVRAS CHAVE:\s*(.*?)\s*NATUREZA'
        }
        
        conteudo = []
        for key, pattern in patterns.items():
            match = re.search(pattern, str(projeto_texto), re.DOTALL)
            if match:
                texto = match.group(1).strip()
                if texto and len(texto) > 10:  # S√≥ incluir se tiver conte√∫do significativo
                    conteudo.append(texto)
        
        return ' '.join(conteudo) if conteudo else str(projeto_texto)
    
    # Criar texto focado no conte√∫do t√©cnico (SEM stop words ainda)
    print("Extraindo conte√∫do t√©cnico dos projetos...")
    df['texto_tecnico_bruto'] = df['projeto'].apply(extrair_conteudo_tecnico)
    
    # AGORA aplicar limpeza de stop words 
    print("Aplicando remo√ß√£o de stop words...")
    df['texto_tecnico'] = df['texto_tecnico_bruto'].apply(remover_stop_words)
    
    # Estat√≠sticas da limpeza
    tamanho_medio_antes = df['texto_tecnico_bruto'].str.len().mean()
    tamanho_medio_depois = df['texto_tecnico'].str.len().mean()
    reducao_percentual = ((tamanho_medio_antes - tamanho_medio_depois) / tamanho_medio_antes) * 100
    
    print(f"üìä Estat√≠sticas da limpeza:")
    print(f"   Tamanho m√©dio antes: {tamanho_medio_antes:.0f} caracteres")
    print(f"   Tamanho m√©dio depois: {tamanho_medio_depois:.0f} caracteres")
    print(f"   Redu√ß√£o: {reducao_percentual:.1f}%")
    
    # Mostrar exemplo de transforma√ß√£o
    print(f"\nüîç Exemplo de transforma√ß√£o:")
    idx_exemplo = 0
    if len(df) > 0:
        print(f"   ANTES: {df['texto_tecnico_bruto'].iloc[idx_exemplo][:200]}...")
        print(f"   DEPOIS: {df['texto_tecnico'].iloc[idx_exemplo][:200]}...")
    
    # Carregar modelo de embeddings - SERAFIM (portugu√™s especializado)
    print("Carregando modelo de embeddings...")
    model = SentenceTransformer("PORTULAN/serafim-335m-portuguese-pt-sentence-encoder-ir")
    
    # Gerar embeddings apenas do conte√∫do t√©cnico
    print("Gerando embeddings do conte√∫do t√©cnico...")
    embeddings = model.encode(df['texto_tecnico'].tolist(), show_progress_bar=True)
    
    print(f"Embeddings gerados: {embeddings.shape}")
    
    return df, embeddings

def fazer_clustering(embeddings, n_projetos):
    """
    Aplica clustering aglomerativo otimizado para projetos da Lei do Bem.
    Escal√°vel para at√© 75k projetos.
    """
    
    print("=== APLICANDO CLUSTERING AGLOMERATIVO ===")
    print(f"üìä Processando {n_projetos:,} projetos")
    
    # Threshold otimizado baseado na an√°lise anterior
    threshold = 0.58
    
    from sklearn.cluster import AgglomerativeClustering
    
    print(f"üéØ Usando threshold: {threshold}")
    print("‚öôÔ∏è  Configura√ß√£o: linkage='average', metric='cosine'")
    
    # Clustering aglomerativo otimizado
    clusterer = AgglomerativeClustering(
        n_clusters=None,           # Deixa threshold decidir o n√∫mero
        distance_threshold=threshold,    # Threshold otimizado
        linkage='average',         # Melhor para embeddings sem√¢nticos
        metric='cosine'           # Ideal para SentenceTransformers
    )
    
    print("üîÑ Executando clustering...")
    clusters = clusterer.fit_predict(embeddings)
    
    # Estat√≠sticas dos resultados
    n_clusters = len(set(clusters))
    cobertura = 100.0  # Aglomerativo sempre cobre 100%
    
    print(f"‚úÖ Clustering conclu√≠do!")
    print(f"   üìà Cobertura: {cobertura}%")
    print(f"   üéØ N√∫mero de clusters: {n_clusters}")
    
    # Estat√≠sticas adicionais por cluster
    cluster_sizes = {}
    for cluster_id in set(clusters):
        size = list(clusters).count(cluster_id)
        cluster_sizes[cluster_id] = size
    
    # Ordenar clusters por tamanho
    sorted_clusters = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)
    
    print(f"\nüìã TOP 10 CLUSTERS POR TAMANHO:")
    for i, (cluster_id, size) in enumerate(sorted_clusters[:10]):
        percentage = (size / n_projetos) * 100
        print(f"   Cluster {cluster_id}: {size:,} projetos ({percentage:.1f}%)")
    
    # Estat√≠sticas de distribui√ß√£o
    avg_size = n_projetos / n_clusters
    print(f"\nüìä ESTAT√çSTICAS DE DISTRIBUI√á√ÉO:")
    print(f"   Tamanho m√©dio por cluster: {avg_size:.1f} projetos")
    print(f"   Maior cluster: {max(cluster_sizes.values()):,} projetos")
    print(f"   Menor cluster: {min(cluster_sizes.values()):,} projetos")
    
    return clusters

def salvar_resultado_json(df, nome_arquivo='projetos_com_clusters_final.json'):
    """Salva o resultado em formato JSON na pasta json_outputs"""
    # Criar pasta json_outputs se n√£o existir
    pasta_output = Path("json_outputs")
    pasta_output.mkdir(exist_ok=True)
    
    # Caminho completo do arquivo
    caminho_completo = pasta_output / nome_arquivo
    
    # Converter DataFrame para lista de dicion√°rios
    dados_json = df.to_dict('records')
    
    # Salvar como JSON
    with open(caminho_completo, 'w', encoding='utf-8') as f:
        json.dump(dados_json, f, ensure_ascii=False, indent=2)
    
    print(f"üìÅ Arquivo JSON salvo: {caminho_completo}")
    return str(caminho_completo)

def salvar_resultado_json_head(df, nome_arquivo='projetos_com_clusters_final_head.json', n_linhas=20):
    """Salva apenas as primeiras n linhas do resultado em formato JSON na pasta json_outputs"""
    # Criar pasta json_outputs se n√£o existir
    pasta_output = Path("json_outputs")
    pasta_output.mkdir(exist_ok=True)
    
    # Caminho completo do arquivo
    caminho_completo = pasta_output / nome_arquivo
    
    # Pegar apenas as primeiras n linhas e converter para lista de dicion√°rios
    dados_json_head = df.head(n_linhas).to_dict('records')
    
    # Salvar como JSON
    with open(caminho_completo, 'w', encoding='utf-8') as f:
        json.dump(dados_json_head, f, ensure_ascii=False, indent=2)
    
    print(f"üìÅ Arquivo JSON (head) salvo: {caminho_completo}")
    print(f"   üìä Cont√©m {len(dados_json_head)} registros")
    return str(caminho_completo)

def main():
    """Fun√ß√£o principal otimizada para usar JSON"""
    try:
        # 1. Carregar dados do JSON
        print("1. Carregando dados do JSON...")
        # Buscar arquivo JSON na pasta json_outputs
        pasta_json = Path("json_outputs")
        arquivos_json = list(pasta_json.glob("*.json"))
        
        if not arquivos_json:
            raise FileNotFoundError("Nenhum arquivo JSON encontrado na pasta json_outputs")
        
        # Usar o primeiro arquivo encontrado (ou voc√™ pode especificar um arquivo espec√≠fico)
        caminho_json = arquivos_json[0]
        print(f"Usando arquivo: {caminho_json}")
        
        df_projetos_unico = carregar_dados_json(caminho_json)
        
        # 2. Criar embeddings otimizados
        print("2. Criando embeddings otimizados...")
        df_projetos_unico, embeddings = criar_embeddings(df_projetos_unico)
        
        # 3. Fazer clustering
        print("3. Fazendo clustering aglomerativo...")
        clusters = fazer_clustering(embeddings, len(df_projetos_unico))
        
        # 4. Adicionar clusters ao dataframe
        df_projetos_unico['cluster'] = clusters
        
        # 5. Salvar resultado completo em JSON
        print("4. Salvando resultado completo em JSON...")
        nome_arquivo = 'projetos_com_clusters_final.json'
        caminho_salvo = salvar_resultado_json(df_projetos_unico, nome_arquivo)
        
        # 6. Salvar resultado head (20 primeiras observa√ß√µes) em JSON
        print("5. Salvando resultado head (20 primeiras observa√ß√µes) em JSON...")
        nome_arquivo_head = 'projetos_com_clusters_final_head.json'
        caminho_salvo_head = salvar_resultado_json_head(df_projetos_unico, nome_arquivo_head, 20)
        
        print(f"\n‚úÖ Processo conclu√≠do com sucesso!")
        print(f"üìÅ Arquivo completo salvo: {caminho_salvo}")
        print(f"üìÅ Arquivo head salvo: {caminho_salvo_head}")
        
        # Estat√≠sticas finais
        cluster_stats = df_projetos_unico['cluster'].value_counts().sort_index()
        n_clusters_final = len(cluster_stats)
        
        print(f"\nüìä ESTAT√çSTICAS FINAIS:")
        print(f"   Total de projetos: {len(df_projetos_unico):,}")
        print(f"   Projetos clusterizados: {len(df_projetos_unico):,}")
        print(f"   Taxa de cobertura: 100.0%")
        print(f"   N√∫mero de clusters: {n_clusters_final}")
        
        print("üéØ META ATINGIDA: Cobertura de 100%!")
        
    except Exception as e:
        print(f"‚ùå Erro durante execu√ß√£o: {str(e)}")
        raise

if __name__ == "__main__":
    main()