{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e901ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentando carregar o dataset...\n",
      "‚úì Dados carregados\n",
      "\n",
      "================================================================================\n",
      "DADOS CARREGADOS COM SUCESSO\n",
      "================================================================================\n",
      "Total de registros: 74,502\n",
      "Total de colunas: 26\n",
      "\n",
      "================================================================================\n",
      "APLICANDO FILTRO CR√çTICO - REMOVENDO 2023\n",
      "================================================================================\n",
      "Registros por ano ANTES do filtro:\n",
      "ano_referencia\n",
      "2018    10876\n",
      "2019    12168\n",
      "2020    11660\n",
      "2021    13198\n",
      "2022    13786\n",
      "2023    12814\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úì Removidos 12,814 registros de 2023 (17.2%)\n",
      "‚úì Dataset filtrado: 61,688 registros (2018-2022)\n",
      "\n",
      "Registros por ano AP√ìS filtro:\n",
      "ano_referencia\n",
      "2018    10876\n",
      "2019    12168\n",
      "2020    11660\n",
      "2021    13198\n",
      "2022    13786\n",
      "Name: count, dtype: int64\n",
      "================================================================================\n",
      "‚úì Todas as colunas esperadas encontradas\n",
      "\n",
      "Empresas √∫nicas: 4,885\n",
      "Anos dispon√≠veis: [np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022)]\n",
      "\n",
      "Projetos multianuais: 46,238 (75.0%)\n",
      "\n",
      "Distribui√ß√£o por setor:\n",
      "setor_analise\n",
      "TIC                          16272\n",
      "Qu√≠mica e Farm√°cia           11708\n",
      "Mec√¢nica e Transporte         7990\n",
      "Agroind√∫stria e Alimentos     7499\n",
      "Transversal                   7090\n",
      "Eletroeletr√¥nica              6438\n",
      "Metalurgia e Minera√ß√£o        4656\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Projetos por ano:\n",
      "ano_referencia\n",
      "2018    10876\n",
      "2019    12168\n",
      "2020    11660\n",
      "2021    13198\n",
      "2022    13786\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "INFORMA√á√ïES DO DATASET\n",
      "================================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 61688 entries, 0 to 61687\n",
      "Data columns (total 28 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   id_empresa_ano            61688 non-null  int64 \n",
      " 1   ano_referencia            61688 non-null  int64 \n",
      " 2   empresa_razao_social      61688 non-null  object\n",
      " 3   cnpj                      61688 non-null  int64 \n",
      " 4   atividade_economica       61688 non-null  object\n",
      " 5   codigo_atividade_ibge     61688 non-null  object\n",
      " 6   porte_empresa             61688 non-null  object\n",
      " 7   numero_projeto            61688 non-null  int64 \n",
      " 8   id_projeto                61688 non-null  int64 \n",
      " 9   nome_projeto              61686 non-null  object\n",
      " 10  descricao_projeto         61619 non-null  object\n",
      " 11  tipo_projeto              61688 non-null  object\n",
      " 12  area_projeto              50786 non-null  object\n",
      " 13  palavras_chave            43514 non-null  object\n",
      " 14  natureza                  61687 non-null  object\n",
      " 15  elemento_tecnologico      61687 non-null  object\n",
      " 16  desafio_tecnologico       61686 non-null  object\n",
      " 17  metodologia               61677 non-null  object\n",
      " 18  informacao_complementar   33805 non-null  object\n",
      " 19  resultado_economico       9716 non-null   object\n",
      " 20  resultado_inovacao        9768 non-null   object\n",
      " 21  descricao_rh              9134 non-null   object\n",
      " 22  descricao_materiais       5946 non-null   object\n",
      " 23  ciclo_maior_1_ano         61652 non-null  object\n",
      " 24  atividade_pdi_continuada  0 non-null      object\n",
      " 25  setor_analise             61653 non-null  object\n",
      " 26  projeto_multianual        61688 non-null  bool  \n",
      " 27  projeto_chave             61688 non-null  object\n",
      "dtypes: bool(1), int64(5), object(22)\n",
      "memory usage: 13.2+ MB\n",
      "None\n",
      "\n",
      "Primeiras 3 linhas do dataset:\n",
      "   id_empresa_ano  ano_referencia                empresa_razao_social  \\\n",
      "0               1            2018  ABBOTT LABORATORIOS DO BRASIL LTDA   \n",
      "1               1            2018  ABBOTT LABORATORIOS DO BRASIL LTDA   \n",
      "2               2            2018  ACHE LABORATORIOS FARMACEUTICOS SA   \n",
      "\n",
      "             cnpj                                atividade_economica  \\\n",
      "0  56998701000116  Fabrica√ß√£o de medicamentos alop√°ticos para uso...   \n",
      "1  56998701000116  Fabrica√ß√£o de medicamentos alop√°ticos para uso...   \n",
      "2  60659463000191  Fabrica√ß√£o de medicamentos alop√°ticos para uso...   \n",
      "\n",
      "  codigo_atividade_ibge porte_empresa  numero_projeto  id_projeto  \\\n",
      "0          C.21.21-1/01        Demais               1        7911   \n",
      "1          C.21.21-1/01        Demais               2        7912   \n",
      "2          C.21.21-1/01        Demais               1        5796   \n",
      "\n",
      "                                        nome_projeto  ...  \\\n",
      "0  Desenvolvimento de uma nova linha de medicamen...  ...   \n",
      "1  Melhorias em produtos farmac√™uticos e novas me...  ...   \n",
      "2  Estudos, s√≠nteses, descoberta e aplica√ß√µes de ...  ...   \n",
      "\n",
      "  informacao_complementar resultado_economico resultado_inovacao descricao_rh  \\\n",
      "0                     NaN                 NaN                NaN          NaN   \n",
      "1                     NaN                 NaN                NaN          NaN   \n",
      "2                     NaN                 NaN                NaN          NaN   \n",
      "\n",
      "  descricao_materiais ciclo_maior_1_ano atividade_pdi_continuada  \\\n",
      "0                 NaN               Sim                      NaN   \n",
      "1                 NaN               Sim                      NaN   \n",
      "2                 NaN               Sim                      NaN   \n",
      "\n",
      "        setor_analise projeto_multianual  \\\n",
      "0  Qu√≠mica e Farm√°cia               True   \n",
      "1  Qu√≠mica e Farm√°cia               True   \n",
      "2  Qu√≠mica e Farm√°cia               True   \n",
      "\n",
      "                                       projeto_chave  \n",
      "0  56998701000116_Desenvolvimento de uma nova lin...  \n",
      "1  56998701000116_Melhorias em produtos farmac√™ut...  \n",
      "2  60659463000191_Estudos, s√≠nteses, descoberta e...  \n",
      "\n",
      "[3 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Tentar diferentes abordagens para carregar o CSV problem√°tico\n",
    "print(\"Tentando carregar o dataset...\")\n",
    "\n",
    "# M√©todo 1: Tentar com error_bad_lines para pular linhas problem√°ticas\n",
    "try:\n",
    "    df = pd.read_csv('csv_longo/projetos_resultados_pessoas_valores.csv', \n",
    "                     encoding='UTF-8', \n",
    "                     low_memory=False,\n",
    "                     sep=';',  # Garantir que est√° usando v√≠rgula\n",
    "                     quotechar='\"')  # Tratar aspas\n",
    "    print(\"‚úì Dados carregados\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro no m√©todo 1: {e}\")\n",
    "    \n",
    "    # M√©todo 2: Ler linha por linha para identificar o problema\n",
    "    try:\n",
    "        # Primeiro, descobrir quantas colunas deveria ter\n",
    "        with open('csv_longo/projetos_resultados_pessoas_valores.csv', 'r', encoding='latin-1') as f:\n",
    "            header = f.readline()\n",
    "            n_cols = len(header.split(','))\n",
    "            print(f\"N√∫mero esperado de colunas: {n_cols}\")\n",
    "        \n",
    "        # Carregar pulando linhas ruins\n",
    "        df = pd.read_csv('csv_longo/projetos_resultados_pessoas_valores.csv',\n",
    "                        encoding='latin-1',\n",
    "                        low_memory=False,\n",
    "                        on_bad_lines='warn',\n",
    "                        engine='python',\n",
    "                        sep=',',\n",
    "                        quotechar='\"',\n",
    "                        escapechar='\\\\')\n",
    "        print(\"‚úì Dados carregados com engine='python'\")\n",
    "    except:\n",
    "        # M√©todo 3: √öltima tentativa - ler em chunks\n",
    "        chunks = []\n",
    "        chunk_size = 10000\n",
    "        problematic_lines = []\n",
    "        \n",
    "        for i, chunk in enumerate(pd.read_csv('csv_longo/projetos_resultados_pessoas_valores.csv',\n",
    "                                             encoding='latin-1',\n",
    "                                             low_memory=False,\n",
    "                                             on_bad_lines='skip',\n",
    "                                             chunksize=chunk_size)):\n",
    "            chunks.append(chunk)\n",
    "            print(f\"  Chunk {i+1} carregado ({len(chunk)} linhas)\")\n",
    "            \n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        print(\"‚úì Dados carregados em chunks\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DADOS CARREGADOS COM SUCESSO\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total de registros: {len(df):,}\")\n",
    "print(f\"Total de colunas: {df.shape[1]}\")\n",
    "\n",
    "# ============================================================\n",
    "# FILTRO CR√çTICO: REMOVER DADOS DE 2023 (DADOS INCOMPLETOS)\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"APLICANDO FILTRO CR√çTICO - REMOVENDO 2023\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar distribui√ß√£o antes do filtro\n",
    "if 'ano_referencia' in df.columns:\n",
    "    print(f\"Registros por ano ANTES do filtro:\")\n",
    "    print(df['ano_referencia'].value_counts().sort_index())\n",
    "    \n",
    "    # Contagem antes\n",
    "    total_antes = len(df)\n",
    "    registros_2023 = (df['ano_referencia'] == 2023).sum()\n",
    "    \n",
    "    # APLICAR FILTRO - REMOVER 2023\n",
    "    df = df[df['ano_referencia'] != 2023].copy()\n",
    "    \n",
    "    print(f\"\\n‚úì Removidos {registros_2023:,} registros de 2023 ({registros_2023/total_antes*100:.1f}%)\")\n",
    "    print(f\"‚úì Dataset filtrado: {len(df):,} registros (2018-2022)\")\n",
    "    \n",
    "    print(f\"\\nRegistros por ano AP√ìS filtro:\")\n",
    "    print(df['ano_referencia'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Coluna 'ano_referencia' n√£o encontrada - verificar estrutura dos dados\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Verificar colunas esperadas\n",
    "colunas_esperadas = ['id_projeto', 'ano_referencia', 'empresa_razao_social', 'cnpj', \n",
    "                     'nome_projeto', 'descricao_projeto', 'tipo_projeto', 'natureza',\n",
    "                     'elemento_tecnologico', 'desafio_tecnologico', 'metodologia',\n",
    "                     'setor_analise', 'ciclo_maior_1_ano']\n",
    "\n",
    "colunas_faltando = [col for col in colunas_esperadas if col not in df.columns]\n",
    "if colunas_faltando:\n",
    "    print(f\"\\n‚ö†Ô∏è Colunas faltando: {colunas_faltando}\")\n",
    "    print(\"Colunas dispon√≠veis:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i:3d}. {col}\")\n",
    "else:\n",
    "    print(\"‚úì Todas as colunas esperadas encontradas\")\n",
    "\n",
    "# Continuar com a an√°lise se temos as colunas principais\n",
    "if all(col in df.columns for col in ['cnpj', 'ano_referencia', 'nome_projeto']):\n",
    "    print(f\"\\nEmpresas √∫nicas: {df['cnpj'].nunique():,}\")\n",
    "    print(f\"Anos dispon√≠veis: {sorted(df['ano_referencia'].dropna().unique())}\")\n",
    "    \n",
    "    # An√°lise de projetos multianuais\n",
    "    if 'ciclo_maior_1_ano' in df.columns:\n",
    "        df['projeto_multianual'] = df['ciclo_maior_1_ano'] == 'Sim'\n",
    "        print(f\"\\nProjetos multianuais: {df['projeto_multianual'].sum():,} ({df['projeto_multianual'].mean()*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Coluna 'ciclo_maior_1_ano' n√£o encontrada\")\n",
    "    \n",
    "    # Criar identificador √∫nico para rastrear projetos entre anos\n",
    "    df['projeto_chave'] = df['cnpj'].astype(str) + '_' + df['nome_projeto'].fillna('').astype(str).str[:50]\n",
    "    \n",
    "    # Distribui√ß√£o por setor\n",
    "    if 'setor_analise' in df.columns:\n",
    "        print(\"\\nDistribui√ß√£o por setor:\")\n",
    "        print(df['setor_analise'].value_counts())\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Coluna 'setor_analise' n√£o encontrada\")\n",
    "    \n",
    "    # Verificar anos com dados\n",
    "    print(\"\\nProjetos por ano:\")\n",
    "    print(df['ano_referencia'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"\\n‚ùå Colunas essenciais n√£o encontradas. Verifique o arquivo CSV.\")\n",
    "\n",
    "# Mostrar informa√ß√µes sobre o dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INFORMA√á√ïES DO DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(df.info())\n",
    "\n",
    "# Verificar primeiras linhas\n",
    "print(\"\\nPrimeiras 3 linhas do dataset:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89fcb803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AN√ÅLISE DE SIMILARIDADE COM GRANITE IBM - THRESHOLD 0.85\n",
      "================================================================================\n",
      "\n",
      "Filtrando dados de Eletroeletr√¥nica - 2021 - Processo...\n",
      "‚úì Projetos filtrados: 366\n",
      "\n",
      "================================================================================\n",
      "PREPARA√á√ÉO DOS DADOS\n",
      "================================================================================\n",
      "\n",
      "Estat√≠sticas dos textos:\n",
      "  - Total de projetos: 366\n",
      "  - Comprimento m√©dio: 8058 caracteres\n",
      "  - Comprimento m√≠nimo: 1077 caracteres\n",
      "  - Comprimento m√°ximo: 16166 caracteres\n",
      "\n",
      "================================================================================\n",
      "GERANDO EMBEDDINGS COM GRANITE IBM\n",
      "================================================================================\n",
      "Carregando modelo ibm-granite/granite-embedding-278m-multilingual...\n",
      "‚úì Modelo Granite carregado com sucesso\n",
      "\n",
      "Gerando embeddings para 366 projetos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029dc9d0e77e44ea929e60e1be0a808b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Embeddings gerados: shape (366, 768)\n",
      "\n",
      "================================================================================\n",
      "CALCULANDO SIMILARIDADES\n",
      "================================================================================\n",
      "Calculando matriz de similaridade...\n",
      "\n",
      "‚úì Matriz calculada: (366, 366)\n",
      "  - Total de compara√ß√µes √∫nicas: 66,795\n",
      "\n",
      "Estat√≠sticas de similaridade:\n",
      "  - M√©dia: 0.6096\n",
      "  - Desvio padr√£o: 0.0578\n",
      "  - M√≠nima: 0.4090\n",
      "  - M√°xima: 1.0000\n",
      "  - Mediana: 0.6069\n",
      "\n",
      "Percentis:\n",
      "  - 75%: 0.6459\n",
      "  - 80%: 0.6556\n",
      "  - 85%: 0.6678\n",
      "  - 90%: 0.6831\n",
      "  - 95%: 0.7071\n",
      "  - 99%: 0.7563\n",
      "\n",
      "================================================================================\n",
      "IDENTIFICANDO PARES SIMILARES (THRESHOLD >= 0.85)\n",
      "================================================================================\n",
      "Procurando pares com similaridade >= 0.85...\n",
      "\n",
      "‚úì Pares encontrados: 70\n",
      "\n",
      "Estat√≠sticas dos pares similares:\n",
      "  - Similaridade m√°xima: 1.0000\n",
      "  - Similaridade m√©dia: 0.9478\n",
      "  - Similaridade m√≠nima: 0.8500\n",
      "\n",
      "Distribui√ß√£o por faixas:\n",
      "  - [0.98, 1.00): 33 pares (47.1%)\n",
      "  - [0.95, 0.98): 10 pares (14.3%)\n",
      "  - [0.90, 0.95): 8 pares (11.4%)\n",
      "  - [0.85, 0.90): 19 pares (27.1%)\n",
      "\n",
      "Top 10 pares mais similares:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [68310] Repovoamento do bagre-sapo: Aplica√ß√£o da biotecnol...\n",
      "  Projeto 2: [68319] Repovoamento do bagre-sapo: Aplica√ß√£o da biotecnol...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [62849] Desenvolvimento experimental de solu√ß√µes tecnol√≥gi...\n",
      "  Projeto 2: [62887] Desenvolvimento experimental de solu√ß√µes tecnol√≥gi...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [60401] Desenvolvimento experimental de solu√ß√µes para apri...\n",
      "  Projeto 2: [60905] Desenvolvimento experimental de solu√ß√µes para apri...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [68927] Limpeza de Isoladores - Sistema inteligente de pla...\n",
      "  Projeto 2: [68963] Limpeza de Isoladores - Sistema inteligente de pla...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [68824] Ind√∫stria 4.0: moderniza√ß√£o de planta industrial, ...\n",
      "  Projeto 2: [68828] Ind√∫stria 4.0: moderniza√ß√£o de planta industrial, ...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [68304] Desenvolvimento experimental de estrat√©gias para c...\n",
      "  Projeto 2: [68315] Desenvolvimento experimental de estrat√©gias para c...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [68929] Manuten√ß√£o preditiva - Sistema inteligente de manu...\n",
      "  Projeto 2: [68965] Manuten√ß√£o preditiva - Sistema inteligente de manu...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [68931] Torre M√≥vel - Desenvolvimento de Torre de Emerg√™nc...\n",
      "  Projeto 2: [68967] Torre M√≥vel - Desenvolvimento de Torre de Emerg√™nc...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [63106] Laborat√≥rio Vivo de Mobilidade El√©trica em Campus ...\n",
      "  Projeto 2: [63142] Laborat√≥rio Vivo de Mobilidade El√©trica em Campus ...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "Similaridade: 1.0000\n",
      "  Projeto 1: [68305] Desenvolvimento de solu√ß√µes para a eletromobilidad...\n",
      "  Projeto 2: [68316] Desenvolvimento de solu√ß√µes para a eletromobilidad...\n",
      "  Tipo: EMPRESAS DIFERENTES\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISE POR TIPO DE PAR\n",
      "================================================================================\n",
      "  - Pares da mesma empresa: 11 (15.7%)\n",
      "  - Pares entre empresas diferentes: 59 (84.3%)\n",
      "\n",
      "Empresas com mais projetos similares:\n",
      "  - ITUMBIARA TRANSMISSORA DE ENERGIA S/A: 11 apari√ß√µes em pares similares\n",
      "  - EXPANSION TRANSMISSAO DE ENERGIA ELETRICA S/A: 9 apari√ß√µes em pares similares\n",
      "  - SERRA DA MESA TRANSMISSORA DE ENERGIA S. A.: 9 apari√ß√µes em pares similares\n",
      "  - WEG EQUIPAMENTOS ELETRICOS S/A: 8 apari√ß√µes em pares similares\n",
      "  - SCHWEITZER ENGINEERING LABORATORIES COMERCIAL LTDA: 7 apari√ß√µes em pares similares\n",
      "\n",
      "================================================================================\n",
      "SALVANDO RESULTADOS\n",
      "================================================================================\n",
      "‚úì Dataset filtrado salvo: eletr_2021_proc.csv\n",
      "  - Total de projetos: 366\n",
      "  - Total de colunas: 28\n",
      "\n",
      "‚úì Pares similares salvos: eletr_2021_proc_similar.csv\n",
      "  - Total de pares: 70\n",
      "  - Colunas inclu√≠das: IdProjeto_1, NomeProjeto_1, IdProjeto_2, NomeProjeto_2, Similaridade_score, Empresa_1, Empresa_2, Mesma_Empresa, CNPJ_1, CNPJ_2\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISE CONCLU√çDA COM SUCESSO\n",
      "================================================================================\n",
      "\n",
      "üìä RESUMO EXECUTIVO:\n",
      "  ‚Ä¢ Modelo utilizado: Granite IBM Multilingual\n",
      "  ‚Ä¢ Projetos analisados: 366\n",
      "  ‚Ä¢ Threshold aplicado: 0.85\n",
      "  ‚Ä¢ Pares similares identificados: 70\n",
      "  ‚Ä¢ Taxa de pares similares: 0.105%\n",
      "\n",
      "üìÅ Arquivos gerados:\n",
      "  1. eletr_2021_proc.csv - Base completa filtrada\n",
      "  2. eletr_2021_proc_similar.csv - Pares com similaridade >= 0.85\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHUNK 2: AN√ÅLISE DEFINITIVA COM GRANITE - THRESHOLD 0.85\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AN√ÅLISE DE SIMILARIDADE COM GRANITE IBM - THRESHOLD 0.85\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filtrar dados\n",
    "print(\"\\nFiltrando dados de Eletroeletr√¥nica - 2021 - Processo...\")\n",
    "filtro = (\n",
    "    (df['setor_analise'] == 'Eletroeletr√¥nica') & \n",
    "    (df['ano_referencia'] == 2021) & \n",
    "    (df['natureza'] == 'Processo')\n",
    ")\n",
    "\n",
    "eletr_2021_proc = df[filtro].copy()\n",
    "print(f\"‚úì Projetos filtrados: {len(eletr_2021_proc)}\")\n",
    "\n",
    "# ============================================================\n",
    "# PREPARA√á√ÉO DOS TEXTOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPARA√á√ÉO DOS DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Campos de texto principais\n",
    "campos_texto = ['nome_projeto', 'elemento_tecnologico', 'desafio_tecnologico', \n",
    "                'metodologia', 'descricao_projeto']\n",
    "\n",
    "# Limpar valores nulos\n",
    "for campo in campos_texto:\n",
    "    if campo in eletr_2021_proc.columns:\n",
    "        eletr_2021_proc[campo] = eletr_2021_proc[campo].fillna('').astype(str)\n",
    "\n",
    "# Criar texto completo sem truncamento\n",
    "def criar_texto_completo(row):\n",
    "    \"\"\"Concatena todos os campos dispon√≠veis\"\"\"\n",
    "    partes = []\n",
    "    for campo in campos_texto:\n",
    "        if campo in row and row[campo] and str(row[campo]).strip():\n",
    "            partes.append(str(row[campo]).strip())\n",
    "    return \" \".join(partes)\n",
    "\n",
    "eletr_2021_proc['texto_completo'] = eletr_2021_proc.apply(criar_texto_completo, axis=1)\n",
    "\n",
    "# Estat√≠sticas dos textos\n",
    "comprimentos = eletr_2021_proc['texto_completo'].str.len()\n",
    "print(f\"\\nEstat√≠sticas dos textos:\")\n",
    "print(f\"  - Total de projetos: {len(eletr_2021_proc)}\")\n",
    "print(f\"  - Comprimento m√©dio: {comprimentos.mean():.0f} caracteres\")\n",
    "print(f\"  - Comprimento m√≠nimo: {comprimentos.min():.0f} caracteres\")\n",
    "print(f\"  - Comprimento m√°ximo: {comprimentos.max():.0f} caracteres\")\n",
    "\n",
    "# Verificar e remover textos vazios\n",
    "textos_vazios = (comprimentos == 0).sum()\n",
    "if textos_vazios > 0:\n",
    "    print(f\"  ‚ö†Ô∏è Removendo {textos_vazios} projetos sem texto\")\n",
    "    eletr_2021_proc = eletr_2021_proc[comprimentos > 0].copy()\n",
    "\n",
    "# ============================================================\n",
    "# GERAR EMBEDDINGS COM GRANITE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GERANDO EMBEDDINGS COM GRANITE IBM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Carregar modelo Granite\n",
    "print(\"Carregando modelo ibm-granite/granite-embedding-278m-multilingual...\")\n",
    "try:\n",
    "    model = SentenceTransformer('ibm-granite/granite-embedding-278m-multilingual')\n",
    "    print(\"‚úì Modelo Granite carregado com sucesso\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Usando modelo alternativo multil√≠ngue...\")\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    print(\"‚úì Modelo alternativo carregado\")\n",
    "\n",
    "# Preparar textos\n",
    "textos = eletr_2021_proc['texto_completo'].tolist()\n",
    "ids_projetos = eletr_2021_proc['id_projeto'].tolist() if 'id_projeto' in eletr_2021_proc.columns else eletr_2021_proc.index.tolist()\n",
    "nomes_projetos = eletr_2021_proc['nome_projeto'].tolist()\n",
    "\n",
    "print(f\"\\nGerando embeddings para {len(textos)} projetos...\")\n",
    "\n",
    "# Gerar embeddings\n",
    "embeddings = model.encode(\n",
    "    textos,\n",
    "    batch_size=16,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"‚úì Embeddings gerados: shape {embeddings.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# CALCULAR MATRIZ DE SIMILARIDADE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULANDO SIMILARIDADES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calcular matriz de similaridade\n",
    "print(\"Calculando matriz de similaridade...\")\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Estat√≠sticas gerais\n",
    "upper_triangle = np.triu(similarity_matrix, k=1)\n",
    "all_similarities = upper_triangle[upper_triangle > 0]\n",
    "\n",
    "print(f\"\\n‚úì Matriz calculada: {similarity_matrix.shape}\")\n",
    "print(f\"  - Total de compara√ß√µes √∫nicas: {len(all_similarities):,}\")\n",
    "print(f\"\\nEstat√≠sticas de similaridade:\")\n",
    "print(f\"  - M√©dia: {all_similarities.mean():.4f}\")\n",
    "print(f\"  - Desvio padr√£o: {all_similarities.std():.4f}\")\n",
    "print(f\"  - M√≠nima: {all_similarities.min():.4f}\")\n",
    "print(f\"  - M√°xima: {all_similarities.max():.4f}\")\n",
    "print(f\"  - Mediana: {np.median(all_similarities):.4f}\")\n",
    "\n",
    "# Percentis importantes\n",
    "percentis = [75, 80, 85, 90, 95, 99]\n",
    "print(f\"\\nPercentis:\")\n",
    "for p in percentis:\n",
    "    valor = np.percentile(all_similarities, p)\n",
    "    print(f\"  - {p}%: {valor:.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# IDENTIFICAR PARES SIMILARES (>= 0.85)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFICANDO PARES SIMILARES (THRESHOLD >= 0.85)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "threshold = 0.85\n",
    "pares_similares = []\n",
    "\n",
    "# Iterar sobre triangular superior para evitar duplicatas\n",
    "print(\"Procurando pares com similaridade >= 0.85...\")\n",
    "for i in range(len(ids_projetos)):\n",
    "    for j in range(i + 1, len(ids_projetos)):\n",
    "        similaridade = similarity_matrix[i, j]\n",
    "        \n",
    "        if similaridade >= threshold:\n",
    "            # Coletar informa√ß√µes adicionais se dispon√≠veis\n",
    "            registro = {\n",
    "                'IdProjeto_1': ids_projetos[i],\n",
    "                'NomeProjeto_1': nomes_projetos[i],\n",
    "                'IdProjeto_2': ids_projetos[j],\n",
    "                'NomeProjeto_2': nomes_projetos[j],\n",
    "                'Similaridade_score': float(similaridade)\n",
    "            }\n",
    "            \n",
    "            # Adicionar informa√ß√µes de empresa se dispon√≠veis\n",
    "            if 'empresa_razao_social' in eletr_2021_proc.columns:\n",
    "                registro['Empresa_1'] = eletr_2021_proc.iloc[i]['empresa_razao_social']\n",
    "                registro['Empresa_2'] = eletr_2021_proc.iloc[j]['empresa_razao_social']\n",
    "                registro['Mesma_Empresa'] = registro['Empresa_1'] == registro['Empresa_2']\n",
    "            \n",
    "            if 'cnpj' in eletr_2021_proc.columns:\n",
    "                registro['CNPJ_1'] = eletr_2021_proc.iloc[i]['cnpj']\n",
    "                registro['CNPJ_2'] = eletr_2021_proc.iloc[j]['cnpj']\n",
    "            \n",
    "            pares_similares.append(registro)\n",
    "\n",
    "# Criar DataFrame com pares similares\n",
    "eletr_2021_proc_similar = pd.DataFrame(pares_similares)\n",
    "\n",
    "if len(eletr_2021_proc_similar) > 0:\n",
    "    # Ordenar por similaridade\n",
    "    eletr_2021_proc_similar = eletr_2021_proc_similar.sort_values('Similaridade_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Pares encontrados: {len(eletr_2021_proc_similar)}\")\n",
    "    \n",
    "    # Estat√≠sticas dos pares\n",
    "    print(f\"\\nEstat√≠sticas dos pares similares:\")\n",
    "    print(f\"  - Similaridade m√°xima: {eletr_2021_proc_similar['Similaridade_score'].max():.4f}\")\n",
    "    print(f\"  - Similaridade m√©dia: {eletr_2021_proc_similar['Similaridade_score'].mean():.4f}\")\n",
    "    print(f\"  - Similaridade m√≠nima: {eletr_2021_proc_similar['Similaridade_score'].min():.4f}\")\n",
    "    \n",
    "    # An√°lise por faixas\n",
    "    print(f\"\\nDistribui√ß√£o por faixas:\")\n",
    "    faixas = [(0.98, 1.00), (0.95, 0.98), (0.90, 0.95), (0.85, 0.90)]\n",
    "    for min_val, max_val in faixas:\n",
    "        if max_val == 1.00:\n",
    "            count = (eletr_2021_proc_similar['Similaridade_score'] >= min_val).sum()\n",
    "        else:\n",
    "            count = ((eletr_2021_proc_similar['Similaridade_score'] >= min_val) & \n",
    "                    (eletr_2021_proc_similar['Similaridade_score'] < max_val)).sum()\n",
    "        pct = count / len(eletr_2021_proc_similar) * 100 if len(eletr_2021_proc_similar) > 0 else 0\n",
    "        print(f\"  - [{min_val:.2f}, {max_val:.2f}): {count} pares ({pct:.1f}%)\")\n",
    "    \n",
    "    # Top 10 pares mais similares\n",
    "    print(f\"\\nTop 10 pares mais similares:\")\n",
    "    print(\"-\" * 80)\n",
    "    for idx, row in eletr_2021_proc_similar.head(10).iterrows():\n",
    "        print(f\"\\nSimilaridade: {row['Similaridade_score']:.4f}\")\n",
    "        print(f\"  Projeto 1: [{row['IdProjeto_1']}] {row['NomeProjeto_1'][:50]}...\")\n",
    "        print(f\"  Projeto 2: [{row['IdProjeto_2']}] {row['NomeProjeto_2'][:50]}...\")\n",
    "        if 'Mesma_Empresa' in row:\n",
    "            tipo = \"MESMA EMPRESA\" if row['Mesma_Empresa'] else \"EMPRESAS DIFERENTES\"\n",
    "            print(f\"  Tipo: {tipo}\")\n",
    "    \n",
    "    # An√°lise por empresa se dispon√≠vel\n",
    "    if 'Mesma_Empresa' in eletr_2021_proc_similar.columns:\n",
    "        mesma_empresa = eletr_2021_proc_similar['Mesma_Empresa'].sum()\n",
    "        diferentes = len(eletr_2021_proc_similar) - mesma_empresa\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(\"AN√ÅLISE POR TIPO DE PAR\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"  - Pares da mesma empresa: {mesma_empresa} ({mesma_empresa/len(eletr_2021_proc_similar)*100:.1f}%)\")\n",
    "        print(f\"  - Pares entre empresas diferentes: {diferentes} ({diferentes/len(eletr_2021_proc_similar)*100:.1f}%)\")\n",
    "        \n",
    "        # Empresas com mais pares similares\n",
    "        if 'Empresa_1' in eletr_2021_proc_similar.columns:\n",
    "            print(f\"\\nEmpresas com mais projetos similares:\")\n",
    "            todas_empresas = pd.concat([\n",
    "                eletr_2021_proc_similar['Empresa_1'],\n",
    "                eletr_2021_proc_similar['Empresa_2']\n",
    "            ])\n",
    "            top_empresas = todas_empresas.value_counts().head(5)\n",
    "            for empresa, count in top_empresas.items():\n",
    "                print(f\"  - {empresa[:50]}: {count} apari√ß√µes em pares similares\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Nenhum par encontrado com similaridade >= {threshold}\")\n",
    "\n",
    "# ============================================================\n",
    "# SALVAR RESULTADOS EM CSV\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SALVANDO RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Remover coluna tempor√°ria antes de salvar\n",
    "if 'texto_completo' in eletr_2021_proc.columns:\n",
    "    eletr_2021_proc_export = eletr_2021_proc.drop(columns=['texto_completo'])\n",
    "else:\n",
    "    eletr_2021_proc_export = eletr_2021_proc.copy()\n",
    "\n",
    "# Salvar dataset filtrado\n",
    "arquivo_projetos = 'eletr_2021_proc.csv'\n",
    "eletr_2021_proc_export.to_csv(arquivo_projetos, index=False, encoding='utf-8-sig', sep=';')\n",
    "print(f\"‚úì Dataset filtrado salvo: {arquivo_projetos}\")\n",
    "print(f\"  - Total de projetos: {len(eletr_2021_proc_export)}\")\n",
    "print(f\"  - Total de colunas: {eletr_2021_proc_export.shape[1]}\")\n",
    "\n",
    "# Salvar pares similares\n",
    "if len(eletr_2021_proc_similar) > 0:\n",
    "    # Selecionar colunas essenciais para o CSV\n",
    "    colunas_essenciais = ['IdProjeto_1', 'NomeProjeto_1', 'IdProjeto_2', 'NomeProjeto_2', 'Similaridade_score']\n",
    "    colunas_extras = ['Empresa_1', 'Empresa_2', 'Mesma_Empresa', 'CNPJ_1', 'CNPJ_2']\n",
    "    \n",
    "    # Adicionar colunas extras se existirem\n",
    "    colunas_export = colunas_essenciais + [col for col in colunas_extras if col in eletr_2021_proc_similar.columns]\n",
    "    eletr_2021_proc_similar_export = eletr_2021_proc_similar[colunas_export]\n",
    "    \n",
    "    arquivo_similares = 'eletr_2021_proc_similar.csv'\n",
    "    eletr_2021_proc_similar_export.to_csv(arquivo_similares, index=False, encoding='utf-8-sig', sep=';')\n",
    "    print(f\"\\n‚úì Pares similares salvos: {arquivo_similares}\")\n",
    "    print(f\"  - Total de pares: {len(eletr_2021_proc_similar_export)}\")\n",
    "    print(f\"  - Colunas inclu√≠das: {', '.join(colunas_export)}\")\n",
    "else:\n",
    "    # Criar arquivo vazio com estrutura b√°sica\n",
    "    df_vazio = pd.DataFrame(columns=['IdProjeto_1', 'NomeProjeto_1', 'IdProjeto_2', 'NomeProjeto_2', 'Similaridade_score'])\n",
    "    df_vazio.to_csv('eletr_2021_proc_similar.csv', index=False, encoding='utf-8-sig', sep=';')\n",
    "    print(f\"\\n‚úì Arquivo de pares similares criado (vazio - nenhum par acima do threshold)\")\n",
    "\n",
    "# ============================================================\n",
    "# RESUMO FINAL\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISE CONCLU√çDA COM SUCESSO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESUMO EXECUTIVO:\")\n",
    "print(f\"  ‚Ä¢ Modelo utilizado: Granite IBM Multilingual\")\n",
    "print(f\"  ‚Ä¢ Projetos analisados: {len(eletr_2021_proc)}\")\n",
    "print(f\"  ‚Ä¢ Threshold aplicado: {threshold}\")\n",
    "print(f\"  ‚Ä¢ Pares similares identificados: {len(eletr_2021_proc_similar)}\")\n",
    "\n",
    "if len(eletr_2021_proc_similar) > 0:\n",
    "    taxa = len(eletr_2021_proc_similar) / (len(eletr_2021_proc) * (len(eletr_2021_proc) - 1) / 2) * 100\n",
    "    print(f\"  ‚Ä¢ Taxa de pares similares: {taxa:.3f}%\")\n",
    "\n",
    "print(f\"\\nüìÅ Arquivos gerados:\")\n",
    "print(f\"  1. eletr_2021_proc.csv - Base completa filtrada\")\n",
    "print(f\"  2. eletr_2021_proc_similar.csv - Pares com similaridade >= 0.85\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d82d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDA√á√ÉO ASS√çNCRONA DE SIMILARIDADE COM LLM LOCAL (OLLAMA)\n",
      "================================================================================\n",
      "\n",
      "Carregando dados...\n",
      "‚úì Projetos carregados: 366\n",
      "‚úì Pares similares carregados: 70\n",
      "\n",
      "================================================================================\n",
      "CONFIGURA√á√ÉO DO OLLAMA\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CRIT√âRIOS DE INOVA√á√ÉO - LEI DO BEM\n",
      "================================================================================\n",
      "\n",
      "CRIT√âRIOS DE AVALIA√á√ÉO DE INOVA√á√ÉO (Lei 11.196/2005):\n",
      "\n",
      "1. **Novidade ou Aperfei√ßoamento** - O projeto introduz algo novo ou melhora significativamente algo existente?\n",
      "2. **Risco Tecnol√≥gico** - Existe incerteza quanto √† viabilidade t√©cnica?\n",
      "3. **Incremento Tecnol√≥gico** - H√° avan√ßo no conhecimento ou capacita√ß√£o tecnol√≥gica?\n",
      "4. **Aplica√ß√£o Industrial** - O resultado pode ser aplicado em processos produtivos?\n",
      "5. **Esfor√ßo Tecnol√≥gico** - Demanda conhecimento t√©cnico-cient√≠fico especializado?\n",
      "\n",
      "CLASSIFICA√á√ÉO DE SIMILARIDADE:\n",
      "- ID√äNTICOS: Mesma solu√ß√£o t√©cnica, mesmo problema, mesma abordagem\n",
      "- MUITO SIMILARES: Problema similar, solu√ß√µes compar√°veis, mesmo n√≠vel de inova√ß√£o\n",
      "- SIMILARES: Mesma √°rea tecnol√≥gica, complexidade equivalente\n",
      "- DISTINTOS: Apesar de vocabul√°rio similar, s√£o projetos tecnicamente diferentes\n",
      "\n",
      "\n",
      "Iniciando processamento ass√≠ncrono...\n",
      "\n",
      "================================================================================\n",
      "INICIANDO AN√ÅLISE ASS√çNCRONA DOS PARES\n",
      "================================================================================\n",
      "\n",
      "Analisando 20 pares em lotes de 5...\n",
      "‚úì Conex√£o estabelecida com modelo gpt-oss:120b-cloud\n",
      "\n",
      "Processando lote 1/4 (pares 1-5)...\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "\n",
      "Processando lote 2/4 (pares 6-10)...\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "\n",
      "Processando lote 3/4 (pares 11-15)...\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "\n",
      "Processando lote 4/4 (pares 16-20)...\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "  ‚ùå Erro no par: 200, message='Attempt to decode JSON with unexpected mimetype: text/plain; charset=utf-8', url='http://localhost:11434/api/generate'\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISE DOS RESULTADOS DA VALIDA√á√ÉO\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "SALVANDO RESULTADOS\n",
      "================================================================================\n",
      "‚úì Resultados salvos em: validacao_llm_pares_similares.csv\n",
      "\n",
      "üìä RESUMO EXECUTIVO:\n",
      "  Total processado: 20\n",
      "  Processamento bem-sucedido: 0\n",
      "  Erros: 20\n",
      "\n",
      "================================================================================\n",
      "AN√ÅLISE ASS√çNCRONA CONCLU√çDA\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHUNK 3: VALIDA√á√ÉO ASS√çNCRONA COM LLM LOCAL (OLLAMA)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "from tqdm.asyncio import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VALIDA√á√ÉO ASS√çNCRONA DE SIMILARIDADE COM LLM LOCAL (OLLAMA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Carregar os dados\n",
    "print(\"\\nCarregando dados...\")\n",
    "eletr_2021_proc = pd.read_csv('eletr_2021_proc.csv', sep=';', encoding='utf-8-sig')\n",
    "eletr_2021_proc_similar = pd.read_csv('eletr_2021_proc_similar.csv', sep=';', encoding='utf-8-sig')\n",
    "\n",
    "print(f\"‚úì Projetos carregados: {len(eletr_2021_proc)}\")\n",
    "print(f\"‚úì Pares similares carregados: {len(eletr_2021_proc_similar)}\")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURA√á√ÉO DO OLLAMA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONFIGURA√á√ÉO DO OLLAMA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Configura√ß√£o do endpoint\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL = \"gpt-oss:120b-cloud\"\n",
    "BATCH_SIZE = 5  # Processar 5 requisi√ß√µes simult√¢neas\n",
    "\n",
    "# ============================================================\n",
    "# DEFINIR CRIT√âRIOS DE AVALIA√á√ÉO\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRIT√âRIOS DE INOVA√á√ÉO - LEI DO BEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "criterios_avaliacao = \"\"\"\n",
    "CRIT√âRIOS DE AVALIA√á√ÉO DE INOVA√á√ÉO (Lei 11.196/2005):\n",
    "\n",
    "1. **Novidade ou Aperfei√ßoamento** - O projeto introduz algo novo ou melhora significativamente algo existente?\n",
    "2. **Risco Tecnol√≥gico** - Existe incerteza quanto √† viabilidade t√©cnica?\n",
    "3. **Incremento Tecnol√≥gico** - H√° avan√ßo no conhecimento ou capacita√ß√£o tecnol√≥gica?\n",
    "4. **Aplica√ß√£o Industrial** - O resultado pode ser aplicado em processos produtivos?\n",
    "5. **Esfor√ßo Tecnol√≥gico** - Demanda conhecimento t√©cnico-cient√≠fico especializado?\n",
    "\n",
    "CLASSIFICA√á√ÉO DE SIMILARIDADE:\n",
    "- ID√äNTICOS: Mesma solu√ß√£o t√©cnica, mesmo problema, mesma abordagem\n",
    "- MUITO SIMILARES: Problema similar, solu√ß√µes compar√°veis, mesmo n√≠vel de inova√ß√£o\n",
    "- SIMILARES: Mesma √°rea tecnol√≥gica, complexidade equivalente\n",
    "- DISTINTOS: Apesar de vocabul√°rio similar, s√£o projetos tecnicamente diferentes\n",
    "\"\"\"\n",
    "\n",
    "print(criterios_avaliacao)\n",
    "\n",
    "# ============================================================\n",
    "# FUN√á√ÉO ASS√çNCRONA PARA AN√ÅLISE VIA LLM\n",
    "# ============================================================\n",
    "\n",
    "async def analisar_par_projetos(session, projeto1_data, projeto2_data, empresa1, empresa2, par_id):\n",
    "    \"\"\"\n",
    "    Analisa um par de projetos usando o LLM local (ass√≠ncrono)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Montar o prompt estruturado\n",
    "    prompt = f\"\"\"\n",
    "Voc√™ √© um especialista em avalia√ß√£o de projetos de P&D para a Lei do Bem (Lei 11.196/2005). \n",
    "Analise os dois projetos abaixo e determine se devem ser tratados com os mesmos crit√©rios de avalia√ß√£o.\n",
    "\n",
    "{criterios_avaliacao}\n",
    "\n",
    "**PROJETO 1** - Empresa: {empresa1}\n",
    "Nome: {projeto1_data['nome_projeto']}\n",
    "Elemento Tecnol√≥gico: {projeto1_data['elemento_tecnologico'][:1000]}\n",
    "Desafio Tecnol√≥gico: {projeto1_data['desafio_tecnologico'][:1000]}\n",
    "Metodologia: {projeto1_data['metodologia'][:800]}\n",
    "\n",
    "**PROJETO 2** - Empresa: {empresa2}\n",
    "Nome: {projeto2_data['nome_projeto']}\n",
    "Elemento Tecnol√≥gico: {projeto2_data['elemento_tecnologico'][:1000]}\n",
    "Desafio Tecnol√≥gico: {projeto2_data['desafio_tecnologico'][:1000]}\n",
    "Metodologia: {projeto2_data['metodologia'][:800]}\n",
    "\n",
    "RESPONDA EM FORMATO JSON:\n",
    "{{\n",
    "    \"classificacao\": \"ID√äNTICOS|MUITO_SIMILARES|SIMILARES|DISTINTOS\",\n",
    "    \"mesmo_tratamento\": true/false,\n",
    "    \"justificativa\": \"explica√ß√£o breve\",\n",
    "    \"nivel_inovacao_proj1\": 1-5,\n",
    "    \"nivel_inovacao_proj2\": 1-5,\n",
    "    \"risco_tecnologico_similar\": true/false,\n",
    "    \"area_tecnologica\": \"descri√ß√£o da √°rea\",\n",
    "    \"alerta_duplicacao\": true/false\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        async with session.post(\n",
    "            OLLAMA_URL,\n",
    "            json={\n",
    "                \"model\": MODEL,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 500\n",
    "            },\n",
    "            timeout=aiohttp.ClientTimeout(total=120)\n",
    "        ) as response:\n",
    "            \n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                response_text = result.get('response', '')\n",
    "                \n",
    "                # Tentar extrair JSON da resposta\n",
    "                try:\n",
    "                    import re\n",
    "                    json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        json_str = json_match.group()\n",
    "                        parsed_json = json.loads(json_str)\n",
    "                        parsed_json['par_id'] = par_id\n",
    "                        return parsed_json\n",
    "                    else:\n",
    "                        return {\"erro\": \"JSON n√£o encontrado\", \"par_id\": par_id}\n",
    "                except json.JSONDecodeError:\n",
    "                    return {\"erro\": \"Erro ao decodificar JSON\", \"par_id\": par_id}\n",
    "            else:\n",
    "                return {\"erro\": f\"Status HTTP: {response.status}\", \"par_id\": par_id}\n",
    "                \n",
    "    except asyncio.TimeoutError:\n",
    "        return {\"erro\": \"Timeout\", \"par_id\": par_id}\n",
    "    except Exception as e:\n",
    "        return {\"erro\": str(e), \"par_id\": par_id}\n",
    "\n",
    "# ============================================================\n",
    "# PROCESSAR LOTES DE PARES ASSINCRONAMENTE\n",
    "# ============================================================\n",
    "\n",
    "async def processar_lote(session, lote_pares, eletr_2021_proc):\n",
    "    \"\"\"\n",
    "    Processa um lote de pares simultaneamente\n",
    "    \"\"\"\n",
    "    tarefas = []\n",
    "    \n",
    "    for idx, row in lote_pares.iterrows():\n",
    "        try:\n",
    "            # Buscar dados completos dos projetos\n",
    "            if 'id_projeto' in eletr_2021_proc.columns:\n",
    "                proj1_data = eletr_2021_proc.loc[row['IdProjeto_1']]\n",
    "                proj2_data = eletr_2021_proc.loc[row['IdProjeto_2']]\n",
    "            else:\n",
    "                proj1_data = eletr_2021_proc[eletr_2021_proc['nome_projeto'] == row['NomeProjeto_1']].iloc[0]\n",
    "                proj2_data = eletr_2021_proc[eletr_2021_proc['nome_projeto'] == row['NomeProjeto_2']].iloc[0]\n",
    "            \n",
    "            empresa1 = row.get('Empresa_1', 'Empresa 1')[:50] if 'Empresa_1' in row else 'Empresa 1'\n",
    "            empresa2 = row.get('Empresa_2', 'Empresa 2')[:50] if 'Empresa_2' in row else 'Empresa 2'\n",
    "            \n",
    "            # Criar tarefa ass√≠ncrona\n",
    "            tarefa = analisar_par_projetos(\n",
    "                session, proj1_data, proj2_data, \n",
    "                empresa1, empresa2, idx\n",
    "            )\n",
    "            tarefas.append((idx, row, tarefa))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao preparar par {idx}: {e}\")\n",
    "    \n",
    "    # Executar todas as tarefas do lote simultaneamente\n",
    "    resultados = []\n",
    "    for idx, row, tarefa in tarefas:\n",
    "        resultado = await tarefa\n",
    "        \n",
    "        # Adicionar informa√ß√µes do par\n",
    "        resultado.update({\n",
    "            'id_projeto_1': row['IdProjeto_1'],\n",
    "            'id_projeto_2': row['IdProjeto_2'],\n",
    "            'nome_projeto_1': row['NomeProjeto_1'],\n",
    "            'nome_projeto_2': row['NomeProjeto_2'],\n",
    "            'similaridade_coseno': row['Similaridade_score'],\n",
    "            'mesma_empresa': row.get('Mesma_Empresa', False)\n",
    "        })\n",
    "        resultados.append(resultado)\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "async def processar_todos_pares():\n",
    "    \"\"\"\n",
    "    Fun√ß√£o principal para processar todos os pares em lotes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INICIANDO AN√ÅLISE ASS√çNCRONA DOS PARES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Preparar √≠ndice para busca\n",
    "    if 'id_projeto' in eletr_2021_proc.columns:\n",
    "        eletr_2021_proc.set_index('id_projeto', inplace=True)\n",
    "    \n",
    "    # Limitar an√°lise para teste\n",
    "    MAX_ANALISES = min(20, len(eletr_2021_proc_similar))\n",
    "    pares_para_analisar = eletr_2021_proc_similar.head(MAX_ANALISES)\n",
    "    \n",
    "    print(f\"\\nAnalisando {MAX_ANALISES} pares em lotes de {BATCH_SIZE}...\")\n",
    "    \n",
    "    # Criar sess√£o HTTP ass√≠ncrona\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Testar conex√£o primeiro\n",
    "        try:\n",
    "            async with session.post(\n",
    "                OLLAMA_URL,\n",
    "                json={\"model\": MODEL, \"prompt\": \"OK\", \"stream\": False},\n",
    "                timeout=aiohttp.ClientTimeout(total=10)\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    print(f\"‚úì Conex√£o estabelecida com modelo {MODEL}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Erro na conex√£o. Status: {response.status}\")\n",
    "                    return []\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao conectar: {e}\")\n",
    "            print(\"Certifique-se que o Ollama est√° rodando: ollama serve\")\n",
    "            return []\n",
    "        \n",
    "        # Dividir em lotes\n",
    "        n_lotes = (len(pares_para_analisar) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "        todos_resultados = []\n",
    "        \n",
    "        # Processar cada lote\n",
    "        for i in range(n_lotes):\n",
    "            inicio = i * BATCH_SIZE\n",
    "            fim = min(inicio + BATCH_SIZE, len(pares_para_analisar))\n",
    "            lote = pares_para_analisar.iloc[inicio:fim]\n",
    "            \n",
    "            print(f\"\\nProcessando lote {i+1}/{n_lotes} (pares {inicio+1}-{fim})...\")\n",
    "            \n",
    "            # Processar lote assincronamente\n",
    "            resultados_lote = await processar_lote(session, lote, eletr_2021_proc)\n",
    "            todos_resultados.extend(resultados_lote)\n",
    "            \n",
    "            # Mostrar progresso\n",
    "            for res in resultados_lote:\n",
    "                if 'erro' not in res:\n",
    "                    print(f\"  ‚úì Par analisado: {res.get('classificacao', 'N/A')}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå Erro no par: {res.get('erro')}\")\n",
    "            \n",
    "            # Pequena pausa entre lotes para n√£o sobrecarregar\n",
    "            if i < n_lotes - 1:\n",
    "                await asyncio.sleep(1)\n",
    "        \n",
    "        return todos_resultados\n",
    "\n",
    "# ============================================================\n",
    "# EXECUTAR AN√ÅLISE ASS√çNCRONA\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nIniciando processamento ass√≠ncrono...\")\n",
    "\n",
    "# Em Jupyter Notebook, usar await diretamente\n",
    "resultados_analise = await processar_todos_pares()\n",
    "\n",
    "# ============================================================\n",
    "# AN√ÅLISE DOS RESULTADOS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISE DOS RESULTADOS DA VALIDA√á√ÉO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if resultados_analise:\n",
    "    # Converter para DataFrame\n",
    "    df_resultados = pd.DataFrame(resultados_analise)\n",
    "    \n",
    "    # An√°lise se tivermos resultados v√°lidos\n",
    "    if 'classificacao' in df_resultados.columns:\n",
    "        # Remover linhas com erro\n",
    "        df_validos = df_resultados[~df_resultados['classificacao'].isna()]\n",
    "        \n",
    "        print(f\"\\nResultados v√°lidos: {len(df_validos)}/{len(df_resultados)}\")\n",
    "        \n",
    "        # Estat√≠sticas de classifica√ß√£o\n",
    "        print(\"\\nDistribui√ß√£o de Classifica√ß√µes:\")\n",
    "        classificacao_counts = df_validos['classificacao'].value_counts()\n",
    "        for classif, count in classificacao_counts.items():\n",
    "            print(f\"  - {classif}: {count} ({count/len(df_validos)*100:.1f}%)\")\n",
    "        \n",
    "        # An√°lise de tratamento similar\n",
    "        if 'mesmo_tratamento' in df_validos.columns:\n",
    "            mesmo_trat = df_validos['mesmo_tratamento'].sum()\n",
    "            print(f\"\\nProjetos que devem ter MESMO tratamento: {mesmo_trat}/{len(df_validos)} ({mesmo_trat/len(df_validos)*100:.1f}%)\")\n",
    "        \n",
    "        # Alertas de duplica√ß√£o\n",
    "        if 'alerta_duplicacao' in df_validos.columns:\n",
    "            alertas = df_validos['alerta_duplicacao'].sum()\n",
    "            print(f\"Alertas de poss√≠vel duplica√ß√£o: {alertas}\")\n",
    "        \n",
    "        # Top pares cr√≠ticos\n",
    "        print(\"\\nPares mais cr√≠ticos:\")\n",
    "        criticos = df_validos[\n",
    "            (df_validos.get('classificacao', '') == 'ID√äNTICOS') | \n",
    "            (df_validos.get('alerta_duplicacao', False) == True)\n",
    "        ]\n",
    "        \n",
    "        for _, row in criticos.head(5).iterrows():\n",
    "            print(f\"\\n  ‚Ä¢ {row['nome_projeto_1'][:40]}...\")\n",
    "            print(f\"    vs {row['nome_projeto_2'][:40]}...\")\n",
    "            print(f\"    Classifica√ß√£o: {row.get('classificacao', 'N/A')}\")\n",
    "            print(f\"    Similaridade: {row['similaridade_coseno']:.3f}\")\n",
    "    \n",
    "    # ============================================================\n",
    "    # SALVAR RESULTADOS\n",
    "    # ============================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SALVANDO RESULTADOS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    arquivo_validacao = 'validacao_llm_pares_similares.csv'\n",
    "    df_resultados.to_csv(arquivo_validacao, index=False, encoding='utf-8-sig', sep=';')\n",
    "    print(f\"‚úì Resultados salvos em: {arquivo_validacao}\")\n",
    "    \n",
    "    print(\"\\nüìä RESUMO EXECUTIVO:\")\n",
    "    print(f\"  Total processado: {len(df_resultados)}\")\n",
    "    print(f\"  Processamento bem-sucedido: {len(df_validos) if 'df_validos' in locals() else 0}\")\n",
    "    print(f\"  Erros: {(df_resultados['erro'].notna()).sum() if 'erro' in df_resultados.columns else 0}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum resultado obtido. Verifique a conex√£o com Ollama.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISE ASS√çNCRONA CONCLU√çDA\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 16:02:12.363770: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-17 16:02:12.421713: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/leomcamilo/projects/test_pgai/.venv/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m function\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_util\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/feature_column/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.feature_column namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenseColumn \u001b[38;5;66;03m# line: 1777\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureTransformationCache \u001b[38;5;66;03m# line: 1962\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SequenceDenseColumn \u001b[38;5;66;03m# line: 1941\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/python/feature_column/feature_column_v2.py:38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m readers\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column \u001b[38;5;28;01mas\u001b[39;00m fc_old\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column_v2_types \u001b[38;5;28;01mas\u001b[39;00m fc_types\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialization\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/python/feature_column/feature_column.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m sparse_tensor_lib\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/python/layers/base.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy_tf_layers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[1;32m     18\u001b[0m InputSpec \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mInputSpec\n\u001b[1;32m     20\u001b[0m keras_style_scope \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mkeras_style_scope\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/python/keras/__init__.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/python/keras/models.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sequential\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_v1\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AddMetric\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Layer\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/python/keras/engine/training_v1.py:46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_arrays_v1\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_distributed_v1\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_eager_v1\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m   issparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/scipy/sparse/__init__.py:294\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/scipy/sparse/_base.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisibleDeprecationWarning\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[1;32m      8\u001b[0m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[1;32m      9\u001b[0m                        matrix, validateaxis,)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/scipy/_lib/_util.py:18\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     Optional,\n\u001b[1;32m     12\u001b[0m     Union,\n\u001b[1;32m     13\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     14\u001b[0m     TypeVar,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace\n\u001b[1;32m     21\u001b[0m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n\u001b[1;32m     22\u001b[0m ComplexWarning: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mWarning\u001b[39;00m]\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/scipy/_lib/_array_api.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     is_array_api_obj,\n\u001b[1;32m     19\u001b[0m     size,\n\u001b[1;32m     20\u001b[0m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray_namespace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_asarray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/scipy/_lib/array_api_compat/numpy/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1410\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/projects/test_pgai/.venv/lib/python3.12/site-packages/numpy/__init__.py:373\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`numpy.array_api` is not available from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy 2.0 onwards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m core\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1187\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid\n",
    "\n",
    "\n",
    "def my_softmax(z):  \n",
    "    \"\"\" Softmax converts a vector of values to a probability distribution.\n",
    "    Args:\n",
    "      z (ndarray (N,))  : input data, N features\n",
    "    Returns:\n",
    "      a (ndarray (N,))  : softmax of z\n",
    "    \"\"\"    \n",
    "    ### START CODE HERE ### \n",
    "    ez_k = sum(np.exp(z))\n",
    "    a = np.zeros(len(z))\n",
    "    \n",
    "    for j in range(len(z)):\n",
    "        a[j] = np.exp(z[j])/ez_k\n",
    "            \n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    return a\n",
    "\n",
    "\n",
    "z = np.array([1., 2., 3., 4.])\n",
    "a = my_softmax(z)\n",
    "atf = tf.nn.softmax(z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analise-lei-do-bem (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
