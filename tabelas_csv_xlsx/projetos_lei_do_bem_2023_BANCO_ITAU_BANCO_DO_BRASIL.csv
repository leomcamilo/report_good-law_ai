id_empresa_ano;ano_referencia;empresa;projeto;projeto_multianual
27262;2023;CNPJ: 00000000000191 RAZÃO SOCIAL :BANCO DO BRASIL SA ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27262;NÚMERO: 1 ID ÚNICO: 107477 NOME: Novas tecnologias auxiliares DESCRIÇÃO: O Banco do Brasil (BB) é um banco brasileiro, constituído na forma de sociedade de economia mista, com participação do Governo Federal do Brasil em 50% das ações. O Banco possui 11.115 pontos de atendimento distribuídos pelo país, entre agências e postos próprios, sendo que 95% de suas agências possuem salas de autoatendimento (são mais de 40 mil terminais), que funcionam praticamente sem interrupção, além de operar uma rede de quase 56.000 pontos de atendimento, entre rede de correspondentes MaisBB, Banco24Horas e Bancos parceiros. Possui ainda opções de acesso via internet, telefone e telefone celular. Está presente em mais de 13 países e possui correspondentes em outros 94 países. Com cerca de 4.000 agências, estão presente em grande parte dos municípios do país, contando com aproximadamente 86.000 funcionários.A crescente concorrência no setor financeiro tem impulsionado uma demanda constante por inovação e modernização. Nesse contexto, o Banco do Brasil reconhece a importância de investir em tecnologias auxiliares que não apenas atendam às expectativas dos clientes, mas também garantam a segurança e a eficiência operacional do negócio. O presente programa de projetos é uma resposta estratégica a este problema técnico científico, focando na automatização de processos antifraude, modernização de aplicativos de investimento, e na consolidação de aplicativos mobile alinhados às estratégias de plataformização do Banco.A inovação no setor financeiro é fundamental para a competitividade e sustentabilidade das instituições. Dados recentes indicam que o investimento global em tecnologias financeiras (FinTech) alcançou um valor recorde de US$ 210 bilhões em 2021, destacando a crescente importância dessas soluções para bancos e instituições financeiras. Inclusive, o setor enfrenta desafios significativos, como a necessidade de combater fraudes cada vez mais sofisticadas, adaptar-se rapidamente às mudanças regulatórias e atender às expectativas dos consumidores por serviços digitais eficientes e seguros. Nesse contexto, o BB se apresenta uma solução para a problemática com o desenvolvimento de novas técnicas para a monitoração antifraude de pagamentos de boletos. Na qual, não apenas reduz perdas financeiras, mas também melhora a confiança do cliente e a reputação da instituição. Adicionalmente, este projeto apresenta a pesquisa para identificação de novas características  de risco em seu aplicativo de Investimentos (App de Investimentos), consequentemente e experimentar novas tecnologias e técnicas no software, tão essencial para atrair e reter investidores ao Banco, proporcionando uma experiência de usuário aprimorada e acessível. Em complemento, o Banco do Brasil explorou a estratégia de plataformização, levantando a hipótese de consolidação de seus aplicativos móveis (mobile banking), o que permite uma integração mais eficiente de seus serviços via internet, facilitando o autoatendimento de usuários com o acesso a uma gama diversificada de produtos financeiros, sem deslocamento até as agências físicas. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: Software PALAVRAS CHAVE: Mobile, plataforma, automatização, dados, aplicativo, modernização, pagamento, desenvolvimento. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Este programa de projetos se refere ao desenvolvimento experimental e estudo de novas técnicas sobre software com o objetivo de suportar e auxiliar o coração dos negócios do Banco do Brasil. Portanto, foi essencial a adicionar novas características de software no aplicativo mobile de investimentos do Banco, que até então tinha o seu foco voltado apenas para fornecer aos usuários ferramentas para realizar investimentos. Agora, com a modernização, o objetivo foi codificar por meio da reengenharia de sistemas, o aplicativo em um ecossistema de assessoria digital de investimentos, auxiliando o usuário no momento da tomada de decisão. Adicionalmente, foi realizado o estudo sobre o fluxo comportamental do usuário no aplicativo, além do mapeamento e monitoração de eventos. Tecnicamente,  foi possível por meio da implementação da ferramenta Matomo Analytics - ferramenta open source voltada para a análise web desenvolvida em linguagem PHP. Sua função é rastrear visitas online para um ou mais website e exibe relatórios dessas visitas para análise -, na qual possibilitou a monitoração dos cliques realizados pelos clientes que, em conjunto com o Firebase (plataforma que fornece infraestrutura de back-end para aplicativos móveis), permite que o BB consiga ter a visão das ações individuais do cliente, garantindo maior assertividade na personalização de indicação de investimentos. A melhoria da arquitetura de gerenciamento de dependências, modularização e evolução de bibliotecas fizeram parte dos estudos deste projeto. A partir desta nova técnica, fez com que aumentasse a produtividade da equipe de desenvolvimento com menor curva de aprendizado negocial para o desenvolvedor, menor tempo de build (compilação de um programa de computador a partir do código-fonte), além de definições claras de responsabilidades nas classes e módulos do aplicativo. Adicionalmente, destaca-se a integração com bases de dados para criação e exibição de relatórios importantes para o cliente, como distribuição de portfólio de investimentos, rentabilidade e evolução das diversas carteiras do cliente, incluindo o Open Finance. Ademais, houve uma evolução completa do módulo de “Renda Variável” com a adição de novos serviços digiatis como: ordem a mercado, preço médio de custódia e consulta de proventos. Tais melhorias de qualidade no aplicativo foram cruciais para alcançar um novo público com perfil investidor, previsto nos estudos iniciais do projeto além de manter fidelizados os usuários que já são clientes.É de extrema  importância manter a estrutura do Banco segura e confiável, a evolução do processo de monitoração antifraude de pagamentos de boletos teve como objetivo a realização desse processo em tempo de transação, de forma similar à monitoração utilizada no PIX. O objetivo foi reduzir ao máximo o tempo de processamento, garantindo a segurança e a eficiência nas transações. Com a solução deste problema técnico-científico, diversos foram os avanços tecnológicos. Visto que antes, o processamento era distribuído entre nuvem e mainframe (50/50) e, agora, passou para uma nova distribuição que reduz a carga do mainframe para 20%, com 80% do processamento em computação em nuvem. Logo, além de reduzir consumo e custos, o avanço que permitiu o processamento em tempo real, eliminando o tempo de compensação de um dia e exigindo análise imediata das transações. Para que isso fosse possível, foi desenvolvida uma arquitetura de microsserviços com Kubernetes, em busca da “conteinerização” da aplicação para evoluir a escalabilidade e facilitar a manutenção. A pesquisa de evolução do software de monitoração antifraude para pagamentos de boletos foi essencial para atender às novas demandas de encriptação, segurança cibernética e regulamentações legais. DESAFIO TECNOLÓGICO: Durante a concepção do projeto de modernização do App de Investimentos, o monitoramento de cliques demandou uma grande carga de estudo acerca das novas tecnologias utilizadas (Matomo Analytics e Firebase) para que fosse possível o monitoramento e análise dos cliques processados e pudessem trazer uma análise mais acurada sob as caraterísticas e fenômenos de usabilidade para personalização e indicação de investimentos. Outra barreira técnica constatada no desenvolvimento está relacionada com as mudanças de arquitetura de desenvolvimento, concentrando-se principalmente no Android. O aplicativo legado baseava-se em desenvolvimento de metodologia MVP e passou para MVVM, utilizando técnicas de clean arquiteture, para tal foi necessário realizar estudos em relação a nova MMVM, utilizando como base o estudo de casos. Outro ponto importante para a quebra de  paradigma de pensamento, foi conceber esta reengenharia de software, pensando em componentes e suporte acessíveis às pessoas com deficiência (PCD), o que demandou muito tempo e esforços devido as inúmeras variáveis. A metodologia de trabalho entre a equipe de negócios com a equipe de TI foi de suma importância para a implementação do projeto, devido a sua complexidade de informações e adaptações necessárias na interface de usuário.Também foram enfrentados desafios relacionados ao ambiente de nuvem, na qual demandou estudo voltados para entender melhor o ambiente e as tecnologias envolvidas para a migração e seu correto funcionamento. Por se tratar de ambientes distintos a análise de viabilidade se fez necessária para levantar as hipóteses de como a ferramenta se comportaria em diversas situações, com as suas devidas alterações e a integração com os demais sistemas. O destaque maior em relação a complexidade, concentrou-se em desenvolver um novo modelo de negócios, tendo em vista que atualmente existe uma segregação dos canais com a infraestrutura além do impacto significativo em outros softwares internos. A modernização no monitoramento gerou um efeito cascata nos demais sistemas da companhia, sendo necessário realizar alterações robustas no aplicativo mobile, sistemas de pagamento e canais de pessoa jurídica e pessoa física.Durante a fase de implementação forma encontradas inúmeras dificuldade em relação ao novo código devido as diferenças entre eles. Foi necessário resolver os conflitos de softwares em todos os módulos por meio da reengenharia de sistemas para que fosse possível estruturar a aplicação e devido a nova tecnologia de Kubernetes houve inúmeros problemas em relação a parte técnica que demandou do time organização, estudo e teste acerca do desenvolvimento. Foi preciso construir novamente toda a web e conforme o desenvolvimento avançava eram feitos os testes, por se tratar de uma tecnologia nova não era possível prever onde seria impactado. Foi necessário congelar o aplicativo temporariamente para pode realizar a migração e desenvolver uma camada inteira de compatibilidade inteira para que fosse viável a migração. A migração ocorreu através de 11 métodos diferentes devido à alta complexidade e depois de inúmeros testes se fez possível concretizar o processo.Outro complicador superado está relacionado a unificação dos aplicativos, pois era preciso garantir que ambos os aplicativos funcionassem corretamente e, para isso, foi necessário realizar a alteração de mais de mil linhas de códigos. As alterações demandaram testes durante o desenvolvimento devido à complexidade de ambos os aplicativos e para isso se fez necessário dois times trabalhando simultaneamente. Por fim, a criptografia e verificações realizadas entre os meios de pagamentos digitais, se faz necessário por conta da exposição de dados a ambientes externos. Foi adicionada criptografia de ponta a ponta para garantir que as informações trafegadas entre os meios de pagamentos digitais e o aplicativo. METODOLOGIA: Com o objetivo de realizar o gerenciamento e entrega de qualidade de forma eficiente e ágil, as equipes de tecnologia do Banco do Brasil utilizam o framework Scrum em conjunto com a metodologia Kanban. O Scrum foi implementado com Sprints de quinze dias, permitindo ciclos curtos e iterativos de planejamento, desenvolvimento e revisão. No início de cada Sprint, a equipe realizava o Sprint Planning para definir o backlog e as prioridades. Reuniões diárias eram realizadas para discutir o progresso, identificar impedimentos e ajustar o trabalho conforme necessário. A cada final de Sprint, a equipe conduzia uma Sprint Review para apresentar as funcionalidades concluídas e coletar feedback das áreas envolvidas em cada uma das soluções desenvolvidas, seguida de uma Sprint Retrospective para refletir sobre o que funcionou bem, o que poderia ser melhorado e definir ações para aprimorar o processo nas próximas sprints.Paralelamente, o Kanban foi utilizado para gerenciar o fluxo de trabalho diário. Um quadro Kanban visualizava as etapas do processo (Por fazer, Em andamento, Em revisão, Concluído), com cartões representando cada tarefa ou item de trabalho. A limitação de trabalho em progresso (WIP) foi estabelecida para evitar sobrecarga e melhorar o foco da equipe. O Kanban permitiu a identificação e resolução rápida de gargalos, mantendo o fluxo de trabalho eficiente e contínuo.A integração do Scrum e Kanban no projeto proporcionou uma abordagem estruturada para o planejamento e execução das sprints, enquanto o Kanban gerenciava o fluxo de trabalho diário. Esta combinação trouxe um equilíbrio entre estrutura e flexibilidade, permitindo à equipe adaptar-se rapidamente às mudanças e manter um ritmo constante de entregas. Como forma de aplicar tal framework, pode-se observar as etapas realizadas para promover a modernização do aplicativo de investimentos do Banco, na qual a equipe realizou o levantamento de requisitos, pesquisa com usuários, desenvolvimento de protótipos e testes de usabilidade. Uma atividade essencial foi o desenvolvimento de fluxo de navegação do aplicativo, na qual foi desenvolvida pelo time de UX em conjunto com o PO (Product Owner) e desenvolvedores, buscando uma interface de usuário fluida e amigável. Essa atividade foi crucial para o novo Design System do Canal de Investimentos. Após isso, foram realizados os desenvolvimentos, testes e homologação das novas funcionalidades para o App Investimentos (Android e IOS). Tais atividades foram executadas pelo time de desenvolvedores, time QA (Quality Assurance) e PO do projeto, possibilitando aplicações estáveis e aderentes aos requisitos. Depois houve o piloto de implantação restrito: última fase de testes em que a nova aplicação mobile foi disponibilizada por meio do TestFlight e Firebase App Distribution para um público restrito de funcionários e clientes, resultando em maior abrangência de testes, dispositivos e integração dos diversos perfis de clientes. Essa etapa foi crucial para correção de problemas (principalmente bugs de interface e integração de APIs) não observados nas etapas anteriores e validação de requisitos não funcionais de performance. Por fim, foi realizada a implantação e monitoração de estabilidade na ferramenta Firebase Crashlytics, assim como avaliações e sugestões de clientes para realimentar a solução com novas melhorias.Essa abordagem híbrida entre Scrum e Kanban permitiu ao projeto alavancar os pontos fortes de ambas as metodologias, resultando em uma entrega mais eficiente e ágil, atendendo às exigências do Banco do Brasil para a modernização de seus processos e aplicativos.  INFORMAÇÃO COMPLEMENTAR: Complementarmente à resolução dos problemas técnico-científicos, a evolução da versão do React trouxe consigo a viabilidade de novos projetos devido às atualizações provenientes de uma nova versão. A atualização também não somente permitiu reduzir o tamanho do aplicativo, mas como também otimizá-lo com maior velocidade, evitando que as trocas de telas no aplicativo ocorram de maneira fluída e sem apresentação de mensagem de carregamento. Após esta evolução de software, constatou-se que tornou-se viável a utilização de novas funcionalidades em novos dispositivos. Esta atividade de reengenharia de parte do sistema operacional do aplicativo permitiu a reformulação das 18 bibliotecas e 76 módulos melhorando significativamente com a evolução de partes do aplicativo, como componentes de textos, imagem, entre outros. Realizamos também a alteração do motor em JavaScript que faz conversar com a tecnologia React Native provendo ao aplicativo ganhos em performance na velocidade de processamento. Adicionalmente, o projeto do App Ourocard React Native também utiliza-se dos conhecimentos obtidos na evolução do React para a versão 0.71, com novas funções e permitindo a integração dos bundles do React entre o aplicativo do BB e do Ourocard.  Com a unificação de ambos os motores de software a jornada do usuário passou a ser a mesma e garantiu maior segurança de ambos os aplicativos, uma vez que suas bases foram unificadas gerando maior conforto em relação a segurança. Outro avanço importante se deve a carteira digital, na qual realizamos análises, pesquisas e foi identificado o problema técnico-científico referente a inúmeras tentativas de fraudes, e para sanar esses problemas realizamos incrementações de segurança importante. Com os avanços dos métodos de pagamentos, desenvolveu-se novas técncas de encriptação com a unificação e criptografia dos dados das carteiras digitais (Apple Pay, Samsung Pay, Google Pay e WhatsApp Pay). Porém, como este é um desenvolvimento sensível ao Banco, não é possível exemplificar com dados técnicos, para preservar o Banco e seus usuários. RESULTADO ECONÔMICO: Espera-se que a otimização dos processos internos a partir da criação e atualização das estruturas de software consiga reduzir os custos com procedimentos manuais e aumentar os ganhos, tal como, atrair novos investidores ao Banco. RESULTADO INOVAÇÃO: O presente programa resulta na criação de novas estruturas sistêmicas ao ecossistema do Banco, fornecendo soluções automatizadas em seus processos, tal qual evolução em suas aplicações para um atendimento ágil, robusto e completo. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :Ao longo dos anos, observa-se que o BB procura cada vez mais solucionar problemas técnico-científicos na área de software para ter como resultado aplicações robustas, com o desenvolvimento de evolução de software a partir do processo de reengenharia, automações de metodologias digitais e entre outras. Em continuidade a este plano de digitalização, o ano base de 2023, tem direcionado parte de seus esforços para a experimentação de novas tecnologias digitais. Em resumo, as atividades de P&D executadas no ano-base inspiraram-se principalmente nas práticas do Kanban, sendo feitas adaptações de acordo com a inclusão de técnicas do método Scrum. No estado da arte do desenvolvimento de software é impossível a desvinculação desta metodologia. Porém, o fator único deste projeto é que a integração entre metodologias proporcionou uma abordagem estruturada para as atividades de planejamento e execução das sprints, enquanto o Kanban gerenciava o fluxo de trabalho diário. Esta combinação trouxe uma alta adaptabilidade para rapidamente as mudanças fossem desenvolvidas, mantendo mesmo um ritmo constante de entregas. Como parte integrante do projeto estão as atividades de pesquisa sobre fenômenos eventuais de usabilidade como o levantamento de requisitos, pesquisa com usuários, e desenvolvimento de mockups. Neste ponto do projeto, uma atividade imprescindível foi o desenvolvimento de fluxo de navegação do aplicativo, na qual foi desenvolvida pelo time de UX para desenhar uma melhor experiência do usuário. Após isso, foram executadas as atividades de desenvolvimentos, testes e homologação das novas funcionalidades para o App Investimentos (Android e IOS). Também neste ano-base desenvolveram-se as atividades para a última fase de testes em que a nova aplicação mobile foi disponibilizada por meio do TestFlight e Firebase App Distribution para um público restrito de usuários, resultando em maior abrangência de testes, dispositivos e integração dos diversos perfis de clientes. Por fim, foi realizada a implantação e monitoração de estabilidade, assim como avaliações e sugestões de clientes para realimentar a solução com novas melhorias.
27262;2023;CNPJ: 00000000000191 RAZÃO SOCIAL :BANCO DO BRASIL SA ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27262;NÚMERO: 2 ID ÚNICO: 107478 NOME: Desenvolvimento de soluções de atendimento integradas DESCRIÇÃO: Cada vez mais o mundo se vê conectado, principalmente quando falamos de tecnologias digitais, passo a passo mais ágeis. Nesse sentido o Banco do Brasil (BB) surge como um dos principais vetores de transformação digital no país. A estratégia digital do BB é amparada por um pilar conjunto de iniciativas que se desdobram em otimização e transformação digitais. Voltadas tanto para experimentação como laboratórios de experimentação novas tecnologias – por exemplo, Real Digital - quanto para acelerar e escalar, como no caso da expansão da Loja BB e o Open Finance.Em outra lente, o BB apresenta iniciativas com foco em se expandir e diversificar, com as plataformas tecnológicas e os novos modelos de negócio de banco como plataforma. Sendo assim, esse programa de pesquisa e desenvolvimento (P&D) reflete de modo exemplificado como o BB se propõe a atingir seu objetivo estratégico digital. Iniciando com o Minhas Finanças, que é um gerenciador financeiro ao cliente BB para uma experiência digital de gestão e assessoria financeira ao app BB (aplicativo mobile do Banco do Brasil). Ao longo de 2023, a resolução do problema técnico-científico se concentrou em transformar essa aplicação em uma fonte de gerenciamento multibanco (recomendações e análises) da vida financeira integrado às vantagens e possibilidades do Open Finance. Ressalta-se que esse desenvolvimento foi de suma importância para aumentar a performance digital do BB, garantindo o Prêmio ABT 2023. Com o case “Minhas Finanças no WhatsApp BB: Sobre Educação Financeira com IA”, o BB foi premiado com o troféu de prata no prêmio por oferecer aos clientes uma forma prática e moderna de gerenciar suas finanças, independentemente do Banco que estejam seus gastos e investimentos.Além disso, o BB vem consideravelmente fortalecendo sua estrutura analítica para gerar inteligência de negócios. Nesse contexto se apresenta a plataforma de Analytics e Inteligência Artificial do BB. A plataforma conduz o processo para que o analista consiga acessar os modelos de dados do banco de forma segura e fácil. Durante 2023, as atividades buscaram a evolução do ecossistema de big data com melhoria da esteira de ingestão de dados, segurança dos ativos de dados, como melhoria e automação no processo de gestão do ambiente de Big data para melhorar a segurança e a eficiência do processamento de dados complexos e a integração de análises avançadas.Como pudemos observar, o BB busca evoluir a cultura digital no ecossistema bancário. Essa nova filosofia prevê que seus times multidisciplinares atuem com objetivos únicos, orientados ao valor gerado aos clientes. Além de especialistas em tecnologia e negócios, os times são compostos por disciplinas como analytics, cyber segurança, atendimento, operações entre outras, dentro do conceito de agilidade em negócios. Primeiramente, para essa estratégia foram priorizadas linhas de importância estratégica para o resultado do BB, como o CDC e Crédito Consignado. Logo, em 2023 trabalhou-se em evoluções no sistema de CDC (crédito direto ao consumidor), o qual trata de empréstimos financeiros a pessoas físicas. Foram desenvolvidas e implementadas uma gama de novas funcionalidades para incrementar a experiência do usuário no processo de contratação. Destaca-se que a linha CDC e Crédito Consignado é composta pelos produtos de crédito consignado e não consignado, com mais de R$ 167 bilhões em carteira. A ambição desse time é tornar o BB a principal opção em solução de crédito, para atender as necessidades das pessoas, de forma rentável e sustentável. Portanto, concluímos que esse projeto traz em seu escopo a busca em desenvolver soluções para atendimento de seus clientes e alcançar novos públicos a partir de soluções de software ágeis, interconectas e aberta a esse novo ecossistema financeiro. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: Software PALAVRAS CHAVE: Integração, Modernização, Segurança, Dados, Performance NATUREZA: Serviço ELEMENTO TECNOLÓGICO: O BB vem cada vez mais expandindo seus horizontes para soluções de viés não tão bancários, ou como o mercado atual tende a chamar de iniciativas “beyond the bank”. São práticas nas quais os bancos criam soluções tecnológicas para o mercado, disponibilizando cada vez mais produtos das mais variadas áreas aos usuários. Com isso o BB busca que a experiência do cliente e o aplicativo do banco tenham mais valor cotidiano para o cliente. Visto que transações bancárias tradicionais atualmente são tratadas como comodities, ou seja, serviço primário para o setor. A ideia do projeto é pesquisar métodos de aprimoramento para a aplicação Minhas Finanças, ao ponto de se tornar uma das novas soluções digitais estratégicas do BB. Até então, o Minha Finanças apresentava a gestão de finanças de forma simplificada, demonstrando visões simples dos gastos dos clientes apenas no próprio BB. Como resolução do problema técnico-científico foi desenvolvida a integração de software entre processamentos de conta e dados do Open Finance.  Contraponto a isto para a disponibilização da nova versão do Minha Finanças com suporte ao Multibanco, foi realizada automatização da sincronização dos dados e a definição de categorização dos gastos com a aplicação de modelos de inteligência artificial (IA), a partir da monitoração de dados na jornada do cliente pelos canais digitais. Essa nova feature trouxe para o Minha Finanças uma experiência única ao cliente do BB, pois passou a oferecer ao usuário a vantagem de pré-processamento dos dados bancários, possibilitando escolher a Instituição Financeira, como também ter em tempo real os seus dados bancários a partir de seu smartphone.Destaca-se que o Minha Finanças “antigo” era todo desenvolvido em linguagem Cobol, porém, para a sua evolução foi necessária uma nova codificação para arquitetura de microsserviços. Com essa evolução de arquitetura, foi disponibilizado o Minha Finanças por serviços no Whatsapp. Dentre as novas funcionalidades disponibilizadas para o novo canal, ressalta-se o saldo multicontas - para as contas dos bancos tem consentimento pelo Open Finance - e o resumo financeiro do cliente (orgânico e ativo). Desta forma, o Minha Finanças passou a oferecer aos clientes uma forma prática de consultar saldos e gastos organizados em categorias, visualizar seu planejamento financeiro, suas metas de gastos e até mesmo monitorar todas as movimentações, não apenas as do BB, mas também de outras instituições financeiras. A implementação do Open Finance e a integração com APIs internas são mudanças significativas que melhoram a usabilidade e a performance da aplicação. Logo, houve uma enorme contribuição para aumentar a atuação digital do BB, o volume de negócios digitais e a retenção e engajamento dos clientes no canal.Para que soluções de problemas como essas sejam possíveis, existe um ecossistema de Analytics do BB. Como parte desta evolução do ecossistema de big data com evolução da esteira de ingestão de dados, melhoria e automação no processo de gestão do ambiente de big data. Para tal, o esforço técnico esteve na criação e na vinculação de operação IIB (barramento interno para troca de informação) à modelos analíticos permitindo a escoragem em ambiente transacional. Foi possível assim, desenvolver a customização dos serviços do ecossistema big data para comunicar com sistemas legados. Esta evolução permite uma plataforma analítica mais robusta, com clusters que monitoram de forma evolutiva os resultados e a resposta do modelo em tempo real. Esse desenvolvimento melhora a segurança e a eficiência do processamento de dados, possibilitando a automação de processos complexos e a integração de análises avançadas. DESAFIO TECNOLÓGICO: Novas integrações ou desenvolvimentos desconhecidos proporcionam uma série de desafios a serem superados. Inicialmente no Minhas Finanças, a equipe multidisciplinar do BB identificou débitos técnicos. Como resolução buscou-se utilizar do refinamento de ideias para questões conhecidas, mas que pudessem ter aplicabilidade os novos requisitos. Apesar das informações estarem integradas, ainda não era organizada a nova jornada do cliente. Portanto, foi necessário atualizar todas as visões do app, permitindo que o usuário atualmente tenha dinamismo na visualização de suas diferentes contas.Esse foi de longe o menor dos desafios desse projeto, que durante a integração do Open Finance observou-se que as APIs das demais instituições não necessariamente apresentavam os dados de forma padronizada. A solução foi o entendimento das informações pelas equipes do BB, já que o sistema Open Finance é autorregulado. Ou seja, é um ecossistema formado por entidades que estabelecem em conjunto suas próprias regras de funcionamento. Logo, com relação aos dados não existe padronização e consistência no ambiente para consolidação deles e sua apresentação na aplicação. Com a adesão diária de mais entidades ao programa é sempre necessário revisar as APIs e as estruturas de integração para o entendimento dos dados e das informações quando o cliente fizer uma transação em alguma instituição. Assim os dados são retornados corretamente para a nova arquitetura de micros serviços.A infraestrutura também foi um ponto de dificuldade para o desenvolvimento deste projeto, dada a imaturidade da equipe quanto a microsserviços e clusterização de sistema. Porém, com o auemto gradativo de conhecimento sobre toda a estrutura, os estudos sobre a camada Cloud (Nuvem) e Grafeno foram aproveitados para a implementação do Minha Finanças no Whatsapp, no qual acabou entrando nessa nova arquitetura como um novo canal de autoatendimento, a partir da criação de uma outra camada de interlocução para entregar esse serviço. Cabe ressaltar que as questões de performance dos microsserviços foram melhoradas conforme o desenvolvimento experimental da nova versão do Minha Finanças.Processamento também foi um dos problemas técnico-científicos constatados no projeto, no qual a equipe de pesquisadores e desenvolvedores executaram atividades de evolução do ecossistema de analytics, pois essa nova solução idealizada possibilitou que os logs de big data fossem enviados para outro sistema (IBM QRadar) que não está integralizado no ambiente de big data, e agrupa informações de log de distintas ferramentas do BB. Porém, não é fácil realizar essa integração devido à volumetria de dados coletados. A grande quantidade de informações, ocasionava queda do sistema do QRadar. Para solucionar a questão, fez-se um tratamento com base nos protocolos de sistemas, para compactar os dados de maneira que fosse mantida a quantidade de dados, mas alterando o volume de transação. A atividade do Minha Finanças, para melhoria do ecossistema analítico enfrentaram-se desafios na integração dos micros serviços de big data com o main frame, principalmente devido à incompatibilidade tecnológica, exigindo um intenso trabalho de refatoração e adaptação de código legado para garantir uma operação harmoniosa com as novas implementações. Por fim, para as equipes do BB, a evolução desses serviços e das ferramentas de analytics é um desafio diário, pois ao mesmo tempo que desenvolvem as melhorias, mantêm-se o ecossistema em pleno funcionamento. Assim, o aprendizado cresce paralelamente para atender os requisitos de segurança. Concluímos que as maiores barreiras enfrentadas nas atividades de 2023 estão relacionadas, principalmente, ao aprendizado de novas ferramentas tecnológicas pelas equipes técnicas, visando estar em adequação das mais atualizadas linguagens, metodologias e tecnologias em geral. METODOLOGIA: No início das atividades do Minha Finanças, principalmente na fase de alinhamento, decidiu-se que as grandes entregas utilizariam a metodologia tradicional de projetos, mas que de forma híbrida mantivessem os ritos da metodologia ágil para controlar as atividades e acelerar a entrega de valor ao usuário. Porém, na evolução da solução, a busca por excelência na gestão de projetos trouxe apenas a metodologia ágil como alternativa eficaz para resolução de problemas técnicos e adaptabilidade de cronograma, proporcionando a otimização dos fluxos de trabalho, aumento da produtividade das histórias de usuário e elevação das perspectivas de sucesso.Para as atividades mencionadas, as equipes de desenvolvimento adotaram uma organização baseada em sprints quinzenais. Estas sprints têm como objetivo sincronizar as atividades e encontrar a melhor forma de planejar a jornada de trabalho. A transparência resultante desse processo permite que todos os membros da equipe ou mesmo da organização acompanhem o progresso do projeto. Essa dinâmica facilita a reorganização de prioridades, garantindo que as sprints em andamento recebam a devida priorização até a sua conclusão.Além do Scrum, foi utilizado o método Kanban para complementar a gestão dos projetos. O Kanban proporcionou uma visualização clara do fluxo de trabalho, permitindo que as equipes identificassem gargalos e áreas que necessitavam de melhorias em tempo real. Com o uso de cartões Kanban, as tarefas foram organizadas e priorizadas de maneira eficiente, facilitando a adaptação rápida às mudanças e melhorando a comunicação entre os membros da equipe. Essa abordagem visual flexível ajudou a manter o foco nas entregas mais importantes e a garantir que os projetos avançassem de forma contínua.Foram implementados testes contínuos para garantir a qualidade e a performance dos produtos desenvolvidos, etapa crucial para a identificação, correção de falhas, impedimento de custos adicionais e atrasos nos lançamentos. Entre os testes realizados, destacam-se os testes de usabilidade, que verificam a experiência do usuário para assegurar que as plataformas sejam eficientes no uso. Além disso, testes automatizados e de regressão foram conduzidos com o objetivo de descobrir possíveis problemas e garantir que novas funcionalidades não introduzissem defeitos e nem afetassem negativamente as funcionalidades anteriores. Os novos sistemas foram também sujeitos a testes de implementação e integração, sendo adaptados para se harmonizarem com as plataformas existentes, exigindo elevado nível de competência técnica para assegurar uma integração coesa e segura.Adicionalmente, foram realizados testes de integração de componentes para garantir que todos os módulos do sistema funcionassem corretamente em conjunto. Ferramentas como Selenium, Postman e T-Rexx foram empregadas para facilitar a automação e a validação dos processos de testes, assegurando que as interações entre os diversos componentes fossem adequadas.Para a finalização do projeto, foi realizada a homologação das soluções, garantindo que todos os requisitos funcionais e de negócio estivessem em correto funcionamento. Essa etapa permitiu avaliar se as soluções se comportavam conforme o esperado por meio da execução de testes realizados de forma controlada. INFORMAÇÃO COMPLEMENTAR: Complementarmente às ferramentas do ecossistema analítico, destaca-se que o seu escopo de atuação foi pulverizado no ambiente de analytics com mais de vinte serviços. No entanto, essa plataforma pode ser um ambiente “hostil” para aquelas pessoas que não possuem tanto conhecimento no mundo da ciência de dados. Portanto, como forma de expandir o conhecimento sobre dados, analytics e big data, a equipe apresentou um projeto de gamificação da jornada de aprendizado a partir de uma série de iniciativas como treinamentos direcionado ao ambiente analytics. Portanto, foi desenvolvido um MVP dessa solução denominada por “Dangeous & Data”. Com tal solução, espera-se que haja uma melhoria na oferta e na qualidade dos serviços aos usuários da Plataforma Analítica, como também aumento do engajamento dos usuários no ambiente analítico do BB. Apesar de recente, podemos observar que o novo aculturamento digital já gera resultados, pois em 2023, os projetos de IA e analytics foram utilizados por mais de 4 mil colaboradores.Os resultados podem ser vistos até em serviços do próprio banco, como a solução do Minhas Finanças, no qual possui motor de categorização em IA. Inclusive, no ano de 2023, essa solução acumulou mais de 10 milhões de usuários únicos, aproximadamente 1,8 milhão de planejamentos financeiros realizados e fechou o ano com 4,6 milhões de usuários ativos no último trimestre. No mês de dezembro, foram realizados mais de 10 milhões de acessos por 3,2 milhões de usuários únicos. Ademais, quase 10% dos consentimentos de dados via Open Finance recebidos de outros bancos foram iniciados no Minhas Finanças.Esses dados gigantescos apenas reafirmam os prêmios ganhos por essa solução como: o Prêmio Open Summit Awards 2023, no qual o BB foi a instituição mais premiada pelo segundo ano consecutivo, neste que é considerado o maior evento de Open Finance do Brasil. Na edição, o BB foi vencedor na categoria “Caso de uso para Pessoa Física”, com o case Minhas Finanças no WhatsApp. Foi também premiado com o troféu de prata no prêmio ABT 2023 - premiação em relacionamento com o cliente no Brasil, a fim de reconhecer e transformar os cases vencedores em referência para o mercado. Sobre Educação Financeira com IA”, foi vencedor por oferecer aos seus clientes uma forma prática de gerenciar suas finanças, por meio de uma experiência na qual seus clientes podem consultar saldos e gastos organizados em categorias, visualizar seu planejamento financeiro, suas metas de gastos e até mesmo monitorar todas as movimentações, não apenas as do Banco do Brasil, mas também de outras instituições financeiras. Esses dados demonstram o resultado dos esforços do BB para evoluir seus ecossistemas de atendimento, trazendo inovação, qualidade e segurança para garantir uma boa experiência de uso aos clientes.Por fim, ao longo de 2023, como estratégia da digitalização foram priorizadas linhas de importância estratégica para o resultado do BB, como o CDC e Crédito Consignado, que apresentam bilhões em carteira. Sobre as melhorias, o elemento tecnologicamente novo é a evolução de software para o programa Desenrola Brasil. Nesse desenvolvimento, foi necessário classificar as operações em um fluxo diferenciado de modo que essas operações tivessem reflexo no documento do Bacen. Portanto, para essa apuração, desenvolveu-se uma nova metodologia de cálculo, visto que o CDC tem sua própria metodologia. Caso isso não tivesse sido feito, o risco seria apresentar um saldo errado para o cliente, colocando em risco a reputação de uma das maiores instituições bancárias do País. Como solução, foram desenvolvidos novos cálculos no sistema legado CLC e a integração entre ambas as aplicações. Nesse ponto do projeto, cabe ressaltar a complexidade de desenvolver uma nova metodologia em tecnologias de ampla complexidade de processamento e codificação como é o Cobol. RESULTADO ECONÔMICO: O projeto evoluiu diversas das aplicações que fazem contato direto com os clientes, gerando maior aderência e fidelização deles. Assim, houve um expressivo ganho de competitividade no mercado bancário. RESULTADO INOVAÇÃO: As iniciativas do BB trouxeram um aumento da eficiência e segurança para os usuários, melhorou técnicas e metodologias de trabalho, proveu soluções tecnológicas mais modernas e conseguiu gerar um ecossistema de aplicações mais integrado. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :Em anos anteriores, este programa apresentava atividades de concepção de novas soluções que agreguem clareza nos processos e serviços dispostos ao cliente do BB para assim assegurar que o relacionamento entre o cliente e o Banco seja efetivamente sólido. Em continuidade a este plano, em conjunto com a estratégia de digitalização do Banco do Brasil. Em 2023, o BB iniciou a segunda fase da Aceleração da Transformação Digital com o objetivo de implementar o novo modelo de trabalho no conceito de comunidades. Essa nova filosofia prevê times multidisciplinares, nos quais foram imprescindíveis para a execução de atividades de estudo para o levantamento dos problemas técnico-científicos relacionados com o Minha Finanças. Para as atividades mencionadas, as equipes de desenvolvimento adotaram uma organização baseada em sprints quinzenais. Estas sprints têm como objetivo sincronizar as atividades e encontrar a melhor forma de planejar a jornada de trabalho. Portanto, as atividades de P&D executadas no ano-base inspiraram-se na mescla das práticas do Kanban e Scrum. As atividades iniciais do projeto consistem na pesquisa sobre fenômenos sociais e de usabilidade sobre as finanças, ou seja, entendimento em como os usuários utilizam este tipo de serviços, para então chega ao levantamento de requisitos, pesquisa com usuários etc. Após o planejamento, foram executadas as atividades de codificação e testes. A abrangência de testes para o projeto é grande, iniciando com testes contínuos para garantir a qualidade e a performance dos serviços do novo Minhas finanças. Adicionalmente, tivemos as atividades de testes automatizados e de regressão, conduzidos com o objetivo de descobrir possíveis problemas. Por fim, foram executados os testes de integração de componentes para garantir que todos os módulos do sistema funcionassem corretamente em conjunto. Ferramentas como Selenium, Postman e T-Rexx foram utilizadas para facilitar a automação e a validação dos processos de testes, assegurando que as interações entre os diversos componentes fossem adequadas. Por fim, foi realizada a homologação das soluções, garantindo que todos os requisitos funcionais e de negócio estivessem em correto funcionamento. Desta forma, fecha-se o ciclo de desenvolvimento contínuo previsto na metodologia deste programa de projetos.
27262;2023;CNPJ: 00000000000191 RAZÃO SOCIAL :BANCO DO BRASIL SA ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27262;NÚMERO: 3 ID ÚNICO: 107479 NOME: Desenvolvimento de soluções para atendimento de clientes pessoa física DESCRIÇÃO: A ascensão da interação humano-computador (IHC) sempre foi marcada por desenvolvimentos como interfaces gráficas amigáveis, assistentes virtuais inteligentes, automação e personalização de serviços, todos projetados para tornar a interação com sistemas digitais mais fluida, eficiente e satisfatória. Esses avanços não apenas transformaram a maneira como as pessoas interagem com a tecnologia, mas também redefiniram os conceitos de experiência e expectativa dos usuários em relação à um serviço digital. Nesse sentido, o Banco do Brasil (BB) tem mantido sua presença digital em complementaridade à rede física com sua estratégia que decide avançar ainda mais na evolução e digitalização do ecossistema tecnológico, fortalecendo a oferta de valor ao cliente para além das fontes tradicionais de receita, conhecido pelo termo beyond banking. Essa é uma abordagem que evita a comoditização das ofertas financeiras e possibilita que instituições tradicionais, como o BB, façam frente às fintechs e novos bancos que já nascem no mundo digital. Diante desse cenário, esse projeto de pesquisa e desenvolvimento (P&D) apresenta atividades para construção de uma nova solução para cashback automático. Na qual, em seu escopo, inclui a atribuição de pontos automatizados para crédito em conta corrente, fundos de investimento ou fatura do cartão de crédito. Percebeu-se que existia uma grande recorrência de solicitações para transformar pontos em dinheiro, por exemplo. Visto isso, criou-se esse serviço digital que automatiza a conversão de pontos em dinheiro, e permite que os clientes possam continuar acumulando pontos na Livelo ou Dotz, se assim preferirem. Em pouco mais de dois meses, a conversão automática de benefícios obteve cerca de 500 mil adesões, movimentando cerca de R$ 4,3 milhões no período. Com esta e outras soluções, o clube de benefícios BB apresentou crescimento de 4% em receita bruta em relação ao ano anterior. Diretamente nesses resultados estão os esforços do BB em proporcionar conveniência no atendimento aos clientes, em qualquer lugar e a qualquer momento. Reconhecendo essa importância, o Banco tem investido em soluções que vão desde a modernização de suas interfaces de usuário até a automação de processos de atendimento. Logo, o projeto demonstra como foram desenvolvidas as soluções para otimizar o tempo de atendimento aos clientes do Banco, com o objetivo de melhorar a jornada dos usuários em relação ao suporte técnico. As melhorias abrangem desde o autoatendimento até a aplicação de robôs, entre as soluções está o Meu Suporte, feito para abertura de solicitações e requisições dos clientes. Enquanto orquestrando os diversos robôs de atendimento, está o projeto Andrew, do qual é uma plataforma para solicitação e automatização de processos de atendimento.Ainda sobre melhorar experiência dos usuários nos canais digitais, destaca-se a evolução do ecossistema do aplicativo do Banco do Brasil (App BB). Tecnicamente, esse projeto demonstra as atividades de evolução de estruturas do legado para novas tecnologias, como React e Grafeno, envolvendo a implementação de telas, funcionalidades e transações dentro do aplicativo. Adicionalmente, o BB apresentou crescimento de 9,8% na carteira de crédito consignado, que alcançou R$ 126,4 bilhões, o que reflete o atendimento próximo e especializado aos mais de dez milhões de clientes proventistas. Como parte do escopo, foram desenvolvidos os fluxos de contratação de linhas de crédito para pessoa física (PF) com processamento em computação em nuvem. A abordagem permitiu que as operações de crédito fossem gerenciadas de maneira ágil e eficiente, melhorando a resposta às demandas dos clientes. Em suma, as atividades do projeto se complementam à medida que os resultados vão sendo alcançados, de forma a agregar valor no atendimento de clientes PF com maior eficiência, segurança e comodidade para acesso dos serviços do BB. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: Software PALAVRAS CHAVE: Atendimento, Migração, Experiência de Usuário, Nuvem, Modernização, Automação, Eficiência. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: O BB tem realizado avanços significativos em diversos âmbitos de negócio, principalmente no que se diz respeito a novos elementos para expandir os horizontes de seus serviços financeiros. A plataforma de Benefícios BB continuou evoluindo, principalmente na personalização das ofertas aos clientes. Dentre uma das evoluções apresenta-se a solução de Cashback Automático. Inicialmente foram executados estudos no comportamento e experiência de consumidores PF, tendo por objetivo a criação de novos tipos de serviços bancários. Como resultado da pesquisa entendeu-se que existia uma grande recorrência de solicitação de transformação de pontos em dinheiro. Visto que tradicionalmente, os clientes precisavam converter seus pontos manualmente. Com esse conhecimento, foi desenvolvido uma estrutura de software nos aplicativos digitais do BB, na qual quando o cliente opta por receber cashback, sempre que houver pagamento da fatura do cartão de crédito participante, será aplicado um percentual de conversão sobre as transações pontuáveis e poderá ser transformado automaticamente em dinheiro. Na nova modalidade de Cashback Automático, os clientes podem escolher receber o benefício de cashback diretamente em conta corrente, fundos de investimento ou na fatura do cartão de crédito, sem a necessidade de intervenções manuais. Apesar de serem disponibilizadas novas formas de benefício, os clientes ainda podem continuar acumulando pontos na Livelo ou Dotz. Entende-se que essa nova solução atende às expectativas dos clientes modernos, que buscam conveniência e eficiência em um serviço financeiro. Para além disso, é uma solução relevante que posiciona o BB no mercado, aumentando o engajamento dos clientes que buscam variedade em recompensas na hora de escolher seus benefícios. Destaca-se que o uso de APIs robustas, juntamente com a versão mobile desenvolvidas em React Native foram componentes chave que tornaram esse novo serviço altamente eficiente e escalável.A linha de Canais Digitais visa potencializar negócios com a disponibilização de novas soluções e oferecer experiência digital de excelência nos canais App BB. Nesse sentido ressalta-se a modernização do aplicativo para React e Grafeno como claros exemplos do foco estratégico do Banco. As atividades tiveram como objetivo melhorar a experiência do usuário por meio de uma interface mais responsiva e eficiente, assim o uso de React como tecnologia de front-end ofereceu uma melhor gestão da interface, permitindo uma navegação mais fluída e rápida aos usuários. O Grafeno, por sua vez, trouxe uma plataforma robusta para operações críticas, garantindo a segurança e a escalabilidade das transações. Ressalta-se nessas atividades que transações modernas, como o PIX, nunca haviam sido vinculadas antes com o centralizador – estrutura monolítica de software responsável por chamar as transações no main frame - pois estavam com tecnologias mais abruptas e escaláveis, impossibilitando a integração das transações com versões antigas do aplicativo. Logo, com o conhecimento obtido no desenvolvimento com React e Grafeno, houve ganho de recursos de software, principalmente para o cadastro de pessoas físicas. Essas tecnologias escaláveis, permitem cadastrar uma “Chave PIX” com alguma das informações pessoais do cliente como chave, por exemplo: e-mail, telefone ou CPF. Em resumo, a modernização das estruturas do App BB e novos serviços, além do escopo tradicional, são elementos que elevam a experiência dos clientes BB a novos patamares. Como resultado, o BB apresentou o número recorde de 22,9 milhões de usuários no App em 2023. Os números reforçam que o Banco está no ritmo certo de sua estratégia digital e se mantém como um dos maiores bancos no atendimento de pessoas físicas do Brasil.  DESAFIO TECNOLÓGICO: Novos desenvolvimentos experimentais são desafiadores, principalmente por não entender se a solução desenvolvida terá ou não viabilidade técnica. Logo, foi necessário um estudo para verificar quais concorrentes possuíam cashback automático, como também validar com o cliente para determinar a parametrização do novo serviço e propor requisitos de negócio diferentes. Tecnicamente a nova solução de cashback foi desafiadora para o time de desenvolvimento, pois com os requisitos identificados nos estudos, ficou evidente que era uma mudança de paradigma, não apenas para a programação como também para o processamento de informações. Uma das principais dificuldades foi desenvolver um novo roteiro contábil que pudesse integrar de forma eficiente com o sistema de pontos proposto a nova modalidade de cashback. A complexidade do sistema aumentou devido à necessidade de atender a diferentes modalidades de clientes e contas. Portanto, a metodologia nesse caso é um facilitador para solucionar as dificuldades, onde a cada sprint com as pequenas entregas foram se adaptando e ajustando os pontos necessários em tecnologia e que pudessem atender o escopo do novo serviço. Como exemplo, cita-se o enfrentamento de dificuldades em criar uma boa experiência de usuário, ao mesmo tempo em que assegurava a robustez e a confiabilidade do sistema back-end. Pois, essa solução abriga a necessidade de garantir a segurança e a integridade dos dados durante a conversão de pontos para dinheiro.Adicionalmente, para realizar a atualização tecnológica a principal barreira foi a descentralização e a melhoria do fluxo de chamadas e transações, dado o fato que esta migração lida com cerca de mais de 600 telas do App BB. Assim, o desafio está relacionado aos requisitos necessários para se manter as transações, visto que os mantenedores são de equipes diferentes e o conhecimento técnico está descentralizado. Seguindo a trilha digital do BB, esse é um projeto que trabalha os aspectos de disseminação de conhecimento. Com a metodologia de equipes multidisciplinares, os desenvolvedores ao trabalharem, retornam as suas equipes e repassam o conhecimento tecnológico com relação a nova estrutura desenvolvida no App BB. Para que assim continue evoluindo os serviços digitais do banco.Tecnicamente enfrentaram-se desafios na migração de funcionalidades e na adaptação do sistema para novas tecnologias sem comprometer a segurança e a integridade dos dados. Houve a necessidade de reestruturar a arquitetura do sistema para permitir a implementação eficiente de React e Grafeno, garantindo a responsividade e a segurança do App BB. Em relação ao conhecimento de infraestrutura de software, observa-se neste programa de projetos que nas melhorias incrementais CDC, o Banco do Brasil se dedicou a migrar o fluxo de contratação da linha “BB Crédito 13° Salário” para a computação em nuvem. Tal mudança é notável pela complexidade devido aos sistemas legados, que envolvem a integração do front-end do aplicativo ao main frame. Para isso foi necessário o time aprender tecnicamente a lidar com um número enorme de contratações e requisições dentro de um ambiente de produção. Este aprendizado está para além da leitura teórica, mas para o experimento da reengenharia de software. Com isso foi possível garantir a antecipação e a otimização dos cálculos necessários para evitar sobrecargas no sistema. Dessa forma, se houver indisponibilidade de micro serviço o sistema está preparado para se encerrar e ir para o processamento de main frame novamente. Portanto, com a superação técnica dos desafios tecnológicos cada vez mais o App BB, se torna uma estrutura robusta para que os clientes possam ter acesso aos serviços do Banco pela internet, onde e quando precisarem. METODOLOGIA: O projeto do Banco do Brasil engloba uma equipe multidisciplinar que intercambia seu conhecimento para desenvolver as soluções apresentadas. Por natureza, a metodologia de todas as etapas desse programa de atividades adotou métodos avançados e estruturados para garantir eficiência, segurança e eficácia no desenvolvimento e implementação das soluções tecnológicas. A abordagem metodológica comum a todas as atividades foi a utilização de metodologias ágeis, que permitiram uma gestão flexível e adaptativa das atividades, facilitando a entrega contínua de melhorias e a rápida adaptação às mudanças e necessidades emergentes.A metodologia ágil, especialmente a aplicação de Scrum, foi um ponto central. O Scrum permitiu a divisão dos projetos em sprints, ciclos de desenvolvimentos curtos e interativos, onde a equipe podia se concentrar em entregas incrementais e obter feedback constante dos usuários. Esse feedback era fundamental para ajustar e refinar as funcionalidades ao longo do desenvolvimento. A utilização de sprints garantiu que problemas fossem identificados e solucionados rapidamente, e que melhorias fossem implementadas de forma contínua e consistente.Uma característica comum entre os projetos foi a integração e entrega contínuas (CI/CD). Essa prática permitiu que o código fosse integrado regularmente ao repositório central, seguido por testes automatizados para garantir que novas alterações não ocasionassem bugs ou problemas de desempenho. Ferramentas como o GIT foram amplamente utilizadas para o versionamento de código, enquanto plataformas como Kubernetes orquestraram micros serviços em nuvem, garantindo escalabilidade e flexibilidade.Adicionalmente nessa metodologia, a adoção de práticas de DevOps foi fundamental para integrar o desenvolvimento (Dev) e as operações (Ops) em um fluxo contínuo e colaborativo. A abordagem permitiu a automação de tarefas rotineiras, como a implantação de software e a gestão de infraestrutura, reduzindo o tempo de entrega e aumentando a confiabilidade das aplicações. A integração contínua (CI) e a entrega contínua (CD) são pilares do DevOps que foram amplamente utilizados nos projetos. Essas práticas garantiram que novas funcionalidades e melhorias fossem rapidamente implementadas e testadas.Os testes foram parte essencial da metodologia utilizada. Foi fundamental para garantir a qualidade, segurança e performance das soluções desenvolvidas. Foram realizados testes automatizados, de desempenho, de integração, de usabilidade e de segurança, garantindo que todas as soluções fossem eficientes e seguras. Essa prática permitiu identificar e resolver problemas rapidamente, mantendo uma alta qualidade. INFORMAÇÃO COMPLEMENTAR: Além dos aspectos de inovação, metodologias, e superação de barreiras tecnológicas, os projetivos do Banco do Brasil incorporaram várias outras práticas e elementos para proporcionar o atendimento ao cliente de forma completa. Com isso, nesse projeto está a criação e a evolução de soluções que melhorem a jornada dos clientes com o suporte desde o autoatendimento. Primeiramente pode-se citar a criação de uma nova plataforma chamada “Meu Suporte”, que integra diversos sistemas de atendimento e informações de clientes. O sistema permite que os clientes recebam suporte de maneira mais ágil e eficiente, consolidando informações de múltiplas fontes e automatizando processos de atendimento.Além dos ganhos para o cliente, observa-se o aumento da maturidade do time, com relação a utilização da computação em nuvem e outras novas tecnologias, sendo um dos pontos de maior desafio do projeto. Foi crucial integrar e consolidar informações de diversos sistemas para permitir um atendimento mais ágil e eficiente. Além disso, a equipe enfrentou dificuldades na manutenção da consistência dos dados e na resposta rápida às solicitações de suporte. Adicionalmente, existiram evoluções na orquestração de robôs para automatizar tarefas repetitivas de atendimento ao cliente como estratégia de aumento da resolução de problemas e eficiência operacional. Na criação e evolução de soluções para otimizar o tempo de atendimento, implementar a orquestração de robôs para automatizar processos exigiu um planejamento detalhado e a resolução de problemas relacionadas à interoperabilidade de sistemas. Por fim, podemos concluir que esse conjunto de atividades se tornam um projeto maior à medida que seus escopos se encontram como ativos estratégicos para aprimorar o atendimento de pessoas físicas, servindo-se essencialmente de novas tecnologias para o Banco e aumento de conhecimento para a criação de novos negócios, melhoria e ampliação dos serviços existentes.Todo o esforço e investimento aplicado no ano base, priorizando as iniciativas de pesquisa, desenvolvimento, evoluções e novas tecnologias, reflete nos números apresentados no início do documento e que são reforçados aqui, com o crescimento de 4% no clube de benefício, que é um resultado dos programas de pontos e do cashback (projeto descrito neste relatório), e um número recorde de 22,9 milhões de usuários no App em 2023, que também é reflexo das evoluções apresentadas anteriormente. RESULTADO ECONÔMICO: Os projetos do Banco do Brasil em 2023 resultaram em maior eficiência operacional, redução de custos em manutenção, aumentou a satisfação de clientes e o potencial crescimento de receita através de soluções automatizadas e inovadoras. RESULTADO INOVAÇÃO: Os projetos de 2023 inovaram ao modernizar a infraestrutura tecnológica, implementar automatizações, otimizar processos e melhorar a experiência dos usuários, resultando em maior eficiência, flexibilidade e segurança aos clientes. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27262;2023;CNPJ: 00000000000191 RAZÃO SOCIAL :BANCO DO BRASIL SA ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27262;NÚMERO: 4 ID ÚNICO: 107480 NOME: Desenvolvimento de novas soluções voltadas ao atendimento de clientes PJ DESCRIÇÃO: Ofertar qualidade no atendimento às pessoas jurídicas (PJ), transformando o Banco do Brasil (BB) em “Um Banco para cada Cliente”, é um objetivo fundamental do Banco, o qual exige um intenso trabalho de melhoria contínua e inovação tecnológica aplicada à concepção de novas soluções. Para tal, é primordial a solução de problemas técnico-científicos no âmbito do estudo e desenvolvimento experimental de um software para operações bancárias via internet. Esse programa de P&D propõe o desenvolvimento de novas soluções voltadas ao atendimento de clientes PJ com atividades que se complementam tecnicamente e trazem um aumento no corpo de conhecimentos do Banco.Neste contexto, como parte do problema técnico científico está a defasagem do aplicativo BB PJ em relação ao aplicativo voltado para pessoas físicas (PF). Portanto, neste projeto será apresentada as atividades principais que descrevem em linhas gerais parte dos esforços para resolver conflitos dentro do software, baseados no processo de reengenharia de sistemas com o desenvolvimento do novo aplicativo BB PJ, no qual expõe a experimentação técnica da nova tecnologia Kodiak – tecnologia desenvolvida internamente pelo BB e em processo de patente -, trazendo transformação digital de experiência ao usuário.As atividades deste projeto incluem o desenvolvimento de algoritmos mais eficientes baseados em novas técnicas de programação para incremento da experiência deste perfil de clientes (PJ). Como exemplo, citamos novos widgets para exibirem qual empresa e conta estão contextualizadas no momento de login, visto que a figura PJ pode administrar contas distintas. Outros exemplos seriam para demonstrar pendências de pagamento, informações sobre o cartão PJ, entre outras.Adicionalmente, encontra-se o desenvolvimento de técnicas de software a fim de pesquisar e explorar a criação de novos tipos de serviços bancários via internet, ou seja, levar ao ambiente digital serviços que ainda não existiam neste novo ecossistema de autoatendimento. Parte deste estudo está algoritmos que fazem a categorização desses serviços por necessidade e, conforme o usuário interage, os serviços vão sendo mostrados e ao chegar na última camada, ele é automaticamente direcionado para a aplicação desejada ou é instruído em como proceder. Dessa forma, o aplicativo deixa de ser um ponto unicamente transacional e passa a trazer ao usuário uma nova experiência, permitindo que ele utilize como uma plataforma de interação com os demais serviços que sejam compatíveis com ele. Isso amplia as possibilidades de negócio do Banco, gerando mais receita dentro de um mesmo cliente PJ.Com este olhar centrado ao cliente, foram desencadeadas as oportunidades de desenvolvimento experimental de novas técnicas a fim de pesquisar o comportamento de usuários tendo por objetivo a criação de novos tipos de serviços bancários para PJ. Neste contexto, foi elaborado um estudo para “tagueamento” de jornada dos clientes PJ dentro do aplicativo, permitindo melhor compreensão de como os clientes estão encontrando as informações que eles precisam, subsidiando de forma mais adequada as tomadas de decisões das futuras evoluções do canal de atendimento.Por fim, o BB traz ainda o desenvolvimento de novos fenômenos digitais com um impacto na gestão financeira de seus clientes voltada para o atendimento de multibancos, entregando um extrato multibanco e multicontas, permitindo que os usuários PJ tenham acesso a informações de todas as contas BB, bem como de contas de outros bancos dentro do próprio aplicativo BB, facilitando a interface com o sistema Open Finance. Além disso, neste contexto foi desenvolvida também novos componentes de software como a Central de Usuário e Central de Limites, diminuindo o tempo gasto com operações dentro das agências bancárias banco e provendo melhoria nos serviços de segurança para operações financeiras em canais de autoatendimento móvel. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: Software PALAVRAS CHAVE: Gestão Financeira, Pessoa Jurídica, Soluções Digitais, Aplicativo PJ NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Como solução do problema técnico-científico descrito no início deste projeto, o Banco do Brasil visa a entrega de novos softwares interconectados no ambiente digital que proporcionam comodidade, agilidade e mais autonomia aos seus clientes. Para viabilizar isso alguns dos destaques tecnológicos das atividades deste projeto podem ser apresentados no desenvolvimento do novo aplicativo BB PJ. Cabe ressaltar que anteriormente este aplicativo era construído em uma arquitetura monolítica, ou seja, todas as funções no código-fonte estão em um único pacote a ser distribuído e manutenção centralizada, não sendo possível a descentralização das informações de forma estruturada para outras áreas proverem algoritmos suportáveis para o aplicativo. Nesse contexto, foi desenvolvida uma nova estrutura lógica de software desenvolvida a partir do processo de reengenharia de sistemas, mas com a comparação técnica de novas tecnologias, como por exemplo, o React Native. Com este framework entendeu-se que era possível criar o código apenas uma vez e usá-lo para alimentar aplicativos multiplataforma (iOS e Android), e isso se traduz em uma enorme economia de tempo e recursos. Paralelamente, com o React foi possível investigar a nova tecnologia Kodiak. Esta é uma solução low code - abordagem de desenvolvimento de software e aplicações corporativas com o mínimo de necessidade de codificação - totalmente desenvolvida pelo Banco do Brasil (em processo de patente), na qual possui sua arquitetura baseada em templates de tela que podem ser parametrizados para organizar o fluxo de navegação e realizar o consumo de APIs. As novas tecnologias retiram a necessidade de atualizar o aplicativo diretamente em código-fonte ou implementar uma nova versão no canal de atendimento PJ para estabelecer melhorias, trazendo ganho de eficiência e agilidade. Além disso, possibilita que várias áreas do Banco possam prover soluções para pessoas jurídicas sem o conhecimento técnico em programação de programadores.Este projeto ainda foca no desenvolvimento de novas técnicas a fim de pesquisar o comportamento de seus usuários tendo por objetivo a criação de novos tipos de serviços bancários. Tecnicamente, foram programados algoritmos para o rastreio (método conhecido também como “tagueamento”) de jornada para PJ, em que é possível acompanhar todo movimento e jornada do cliente dentro do aplicativo, com o diferencial de que, por padrão, o software armazenava apenas o título das telas por onde o usuário passou, mas não quais widgets o usuário clicou. Com o desenvolvimento foi possível cadastrar “eventos” em cada um dos botões das aplicações desenvolvidas, ou seja, foram mapeados via algoritmo as ações ou ocorrências que acontecem no sistema para coletar as informações de:  quais telas o cliente acessou, quais tipos de usuário estão clicando em determinados botões, quantos clientes estão clicando em determinados produtos, entre outras atividades.Com isto o BB consegue desenvolver novos serviços bancários pela internet, como por exemplo o desenvolvimento da gestão financeira de cliente para multibancos. Com novos tipos de meio de pagamento e transações digitais, o BB apresenta-se como ativador tecnológico do extrato de todas as contas vinculadas a mesma instituição, bem como as de outras instituições, evitando que o cliente precise fazer alteração entre logins ou aplicativos. É também o primeiro produto a trazer os benefícios do Open Finance para o cliente, sistema que permite o compartilhamento de dados financeiros entre as instituições participantes. Outro elemento inovador dentro desse conceito é o ITP (Iniciador de Transações de Pagamento), um serviço viabilizado pelo Open Finance e Pix, que permite ao usuário realizar pagamentos e transferências digitais, usando saldos em outros bancos a partir de um único canal de forma segura e rápida. DESAFIO TECNOLÓGICO: Como parte da resolução dos problemas técnico-científicos ligados ao aplicativo BB PJ está a superação de uma série dificuldades técnicas e desafios digitais para alcançar os resultados supracitados nas Informações complementares ou no tópico anterior. Logo, é possível ressaltar que o desenvolvimento deste novo software para operações bancárias trouxe mudanças drásticas na estrutura lógica, back-end, front-end e inclusão de novas feature do aplicativo PJ e cujo desenvolvimento se deu a partir de uma nova tecnologia, Kodiak, em conjunto com o framework React Native, não sendo viável realizar atualizações bruscas no aplicativo para os clientes. Ainda, cada sistema operacional (do dispositivo do cliente) possui suas próprias políticas com diferentes mecânicas para disponibilização de um software piloto.A disponibilização das versões piloto das APIs enfrentou desafios como a interrupções de processamento da aplicação, ou seja, o travamento e/ou encerramento da aplicação ao iniciar, bem como em funcionalidades dentro do aplicativo. Assim, foi necessário a interrupção dos experimentos em ambiente de produção para estudar e desenvolver novas soluções digitais, com o comparativo de tecnologias e métodos de codificação.Adicionalmente, a descentralização do novo aplicativo PJ a partir da implementação da nova tecnologia, Kodiak, demandou o aumento no corpo de conhecimento técnico para desenvolver com o time interno suas funcionalidades, como também gerar conhecimentos sobre a tecnologia React Native em plataformas low code para facilitar a concepção em operações que rodam em back-end. Foi necessário também a contextualização das necessidades dos clientes PJ para que pudessem ser desenvolvidas soluções específicas e alterações de forma mais ágil.No aspecto do desenvolvimento de uma nova gestão financeira para multibancos enfrentou-se dificuldades técnicas no contexto da segurança do aplicativo, especificamente, na autenticação de contas não logadas e na integração e comunicação de contas em outros bancos. Portanto, foi necessário soluções paralelas em computação em nuvem (cloud computing) para orquestração do Open Finance. Com o desenvolvimento de uma plataforma cloud chamada de Pacman, passou-se a investigar diariamente os dados de todos os clientes do Banco em outras instituições financeiras. Apesar da base Pacman, complementarmente foi desenvolvido o acionamento em tempo real, em que os dados são retirados assim que solicitados. Com estas duas técnicas constataram-se que o acionamento de dados em tempo real seria a melhor alternativa em termos de assertividade de dados, especialmente quando se refere a saldo, pois possui a última versão.Na esfera de performance de dados do Open Finance, foi necessário contornar complexidades técnicas relacionadas ao entendimento sobre os limites de requisição de dados que cada cliente pode fazer no ecossistema e realizar um controle de informações para que o cliente não extrapole os limites. A partir de um determinado número de consultas, o usuário é notificado de que não terá mais o acesso às informações em tempo real. Para isso, toda consulta é usada para internalizar os dados na computação em nuvem do Pacman e mostrá-la quando necessário para evitar a extrapolação de requisições de software na plataforma.Por fim, no aplicativo do BB, ao abrir uma conta para determinada empresa é gerada uma chave J, que é a credencial de acesso para aquela empresa nos canais de atendimento. Essa chave é gerada para cada usuário que precisa de acesso, como por exemplo, diferentes sócios. Adicionalmente, caso o usuário tenha mais de uma conta da empresa, é gerado uma chave J para cada um desses acessos, gerando dificuldades. A fim de solucionar esse desafio técnico-cientítico, houve um movimento do BB para unificar esse acesso através de uma única chave J e senha, em que o usuário pode escolher a conta e ter acesso ao canal de atendimento. METODOLOGIA: Os desenvolvimentos experimentais de novas soluções para clientes PJ foram realizados conforme a metodologia Kanban, uma metodologia de fluxo contínuo que visa organizar os processos que envolvem as equipes, ressaltando a priorização das tarefas tornando possível identificar e resolver problemas no fluxo dos desenvolvimentos. As métricas foram quinzenais, realizando o planejamento, bem como refinamento, em ciclos de 15 dias, de acordo com a demanda. Isso possibilitou a melhor repartição das entregas com diversas atividades a serem realizadas, entregando histórias mais detalhadas.Dessa forma, foi realizado o faseamento, coletando inicialmente quais eram as necessidades para PJ, com base em feedbacks fornecidos pelas unidades gestoras de clientes, complementarmente com os objetivos do Banco baseados no direcionamento estratégico interno. Nesse contexto, montou-se um backlog e após validação dele, foi possível avançar para a etapa de priorização, balanceando a complexidade do desenvolvimento e a entrega de valor, separando as necessidades de alto valor agregado. A partir disso, foi feito o desenvolvimento do modelo ágil, iniciada a escrita de histórias, refinamento, sprint, review retrospectiva, planejamento, entre outras etapas internas. Ainda, ressalta-se que todas as etapas, desde a prototipação até a implementação e testes finais, foram realizadas com o auxílio da equipe de experiência do usuário.Os testes com as APIs pilotos se deram a partir da implementação em grupos pequenos, expandindo gradualmente a distribuição para 1% dos clientes até que atingisse 100% da implementação, abrangendo todos os usuários PJ. Os pilotos passaram por testes como o Flight e Firebase a fim de testar a estabilidade do novo aplicativo. A fase inicial foi realizada com o pessoal interno do BB, a qual ocorria problemas como travamentos, encerramento e situações semelhantes. A partir do momento em que o aplicativo estava estável, foi testado API piloto mais abrangentes, avaliando a usabilidade e funcionalidade com cliente externo, sendo essa etapa realizada com o suporte da equipe de Cash Management, que auxiliam individualmente os clientes e a rede na definição de soluções digitais. INFORMAÇÃO COMPLEMENTAR: Complementarmente, o projeto conta com a criação de novas técnicas de segurança o para atendimento de clientes PJ com o desenvolvimento de ambientes de software no aplicativo PJ, conhecidos por Central de Usuário e Central de Limites. Este novo desenvolvimento experimental permite que os clientes tenham mais acesso e personalização aos serviços financeiros com opção de criar outros usuários, excluir usuários, senhas, permitir acessos, liberar dispositivos, entre outras atividades que demandavam anteriormente uma série de autorizações. Para tal foram parametrizados um agrupamento de pouco mais de oitocentas autorizações em cinco grupos, fazendo com que o administrador tenha mais controle sobre as autorizações que ele permite para outros usuários, consequentemente, aumentando o nível de segurança das operações financeiras.As soluções apresentadas representam facilidade, retenção do cliente, ampliação da permeabilidade nos negócios e transações dos clientes PJ, tornando assim o Banco do Brasil mais atrativo para esse perfil de cliente. Estes critérios e conceitos inovadores do BB são mundialmente reconhecidos, inclusive soluções desenvolvidas nesse programa. O Painel PJ, por exemplo, que integra a interface Open Finance, foi vencedor da premiação organizada pelo jornal inglês “Financial Times”, no quesito tecnologia em 2023. A solução integra informações de pagamentos e recebimentos das micro e pequenas empresas e permite conciliar as vendas de maquininhas de quaisquer credenciadoras. Estima-se que Painel PJ participe na gestão de R$ 125 bilhões em faturamento anual de cerca de 49 mil empresas.Por fim, destaca-se que as atividades realizadas em 2023 para os desenvolvimentos que não são continuados foram citadas nesse programa. O Banco do Brasil apresenta ainda desenvolvimentos experimentais iniciados no ano de 2023 que serão finalizados posteriormente devido à complexidade da operação, como a unificação do acesso ao aplicativo BB PJ e PF por meio de login único com CPF e senha. Esse projeto nasce a partir do desenvolvimento multicontas, em que o Banco do Brasil propõe desenvolvimentos experimentais para unificar o acesso de contas PJ por meio de CPF e senha. Sendo assim, espera-se que com os estudos e a pesquisa da viabilidade técnica, a partir do CPF, será possível unificar o acesso PF e PJ do usuário, apresentando a lista de contas as quais o usuário tem acesso. Esta é uma operação de alta complexidade, pois é necessário acoplar elementos como recuperação de credencial, credenciamento, liberação de dispositivo, dentre outros desafios. Dessa forma, esse desenvolvimento, apesar de ter se iniciado em 2023, terá a maior parte de suas atividades realizadas em 2024. RESULTADO ECONÔMICO: As novas soluções para atendimento de cliente PJ têm um impacto direto no crescimento da carteira ampliada PJ, o qual em 2023, por exemplo, cresceu em 9% em 12 meses, com destaque em operações de capital de giro e de investimentos. RESULTADO INOVAÇÃO: Os desenvolvimentos desse programa trouxeram inovações de impacto ao cliente, como o acesso a diferentes contas, inclusive de diferentes bancos, bem como descentralização do aplicativo e melhorias nos serviços de segurança para operações financeiras. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :Como pudemos observar ao longo do último ano o BB avançou na jornada de transformação digital, entre eles o lançamento do novo aplicativo PJ. Até o ano de 2022, o aplicativo PJ era muito mais operacional. Em comparação com o aplicativo Pessoa Física (PF), o aplicativo PJ ainda apresentava um código monolítico e com seu front-end relativamente antigo ao PF. Portanto, para 2023, a estratégia, orientada à experiência personalizada, tem o cliente pessoa jurídica como objeto central. Desta forma, os desenvolvimentos experimentais aqui apresentados caracterizam-se como continuados ao atendimento deste público, mas novos perante ao cenário de transformação digital do Banco.Em linhas gerais, as atividades de P&D desenvolvidas no ano-base seguiram os ritos previstos em metodologias ágeis, especialmente regidas pelo Kanban. Inicialmente neste projeto foram levantadas as reais necessidades para o público PJ, principalmente com base nos feedbacks acolhidos ao longo dos anos, complementarmente comparados ao direcionamento estratégico interno. Nesse contexto, montou-se um backlog, isto é, uma lista de priorização de requisitos para desenvolvimento. Com este método foi possível realizar a priorização, balanceando a complexidade do desenvolvimento e a entrega de valor ao cliente BB. É importante destacar que no método Kanban do BB sempre que há um fluxo estruturado de trabalho por ciclos de desenvolvimento. Isso garante o bom andamento das tarefas e a otimização do gerenciamento do fluxo de trabalho.  Portanto, as atividades das soluções aqui descritas seguem um fluxo similar de entrega e integração contínua. Tecnicamente, a entrega contínua (CD) descreve o processo de lançamento frequente novas releases de software, como por exemplo a nova gestão financeira de multibancos, com todos os benefícios do Open Finance. Enquanto, a integração contínua (CI) é a prática de criar e testar de imediato o código aos poucos. Então é neste ponto que as atividades se diferem entre si, por exemplo, com a esteira de CI/CD, as atividades de testes com as APIs pilotos se deram a partir da implementação em grupos pequenos, expandindo gradualmente a distribuição aos usuários PJ. Como exemplo de outra atividade de P&D desenvolvida no ano-base de 2023, apresentou-se a avaliação da usabilidade e funcionalidades com cliente externo após a estabilização do back-end do novo aplicativo, como forma de fechar a esteira DevOps neste ciclo vivo de atividades. 
27262;2023;CNPJ: 00000000000191 RAZÃO SOCIAL :BANCO DO BRASIL SA ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27262;NÚMERO: 5 ID ÚNICO: 107481 NOME: Novas soluções para o atendimento do agronegócio DESCRIÇÃO: O agronegócio está consolidado como um dos segmentos econômicos de maior valor no Brasil, mas ainda possui uma ampla margem para crescimento e geração de capital. Essa atividade cresceu 15,1% de 2022 para 2023, impactando positivamente o desempenho do PIB do país. Nesse cenário, o Banco do Brasil (BB) está como um dos principais líderes de investimentos em um setor tão estratégico para o Brasil. Conforme dados do Banco Central do Brasil, em dezembro de 2023, o BB detinha 49,6% de participação nos financiamentos destinados ao setor. Nos créditos direto ao produtor rural, a participação de mercado é de 56,3%. Por esses e outros motivos estratégicos, o BB está constantemente desenvolvendo novas soluções tecnológicas para se manter atualizado nas melhores práticas de mercado e propor tecnologias inovadoras, bem como continuar competitivo num setor cada vez mais concorrido, devido ao surgimento rápido de novas entidades financeiras digitais e bancos digitais.Dessa forma, o programa de pesquisa e desenvolvimento (P&D) tem como objetivo demonstrar minuciosamente o esforço investido pelas equipes multidisciplinares do BB, em conjunto com o time de pesquisadores de tecnologia da informação, superar problemas técnico-científicos e realçar o aspecto transformador do Banco perante o mercado do agronegócio brasileiro. Para tal, destaca a pesquisa e estudo para “tokenização” de ativos do agronegócio, proporcionando desenvolvimento experimental da estruturação de uma nova solução em blockchain, capaz de automatizar e orquestrar uma cadeia completa de negócio, agregando eficiência, transparência e escalabilidade. Com foco na elevação do nível tecnológico do futuro do mercado financeiro brasileiro. Tecnologicamente, foi explorada a viabilidade técnica do desenvolvimento as bases para a rede de blockchain do BB, denominada “Plataforma de Ativos Agro”, bem como sua Plataforma de Tokenização (TAF) e a Plataforma de Negociação de Ativos Tokenizados (NTD), dos quais estarão mais bem aprofundados com detalhes nas próximas seções deste relatório. Como demonstrado no início desse projeto, o BB possui uma grande fatia de atuação no cenário de créditos financeiros desse estratégico setor brasileiro. Assim, complementarmente a esta atividade disruptiva, o projeto evidencia a continuação de atividades para a evolução incremental dos softwares do BB,  responsáveis por atender um grande público, elevando o nível da transformação digital e de experiência do usuário do Agro. Em especial, cita-se a expansão das fronteiras do Portal de Crédito, aplicação que foca o Agronegócio, sendo acessada pelos correspondentes bancários para o acolhimento de propostas de crédito, incrementando a prospecção de clientes de forma mais efetiva e próxima ao mercado.O Portal de Crédito é formado por uma forte estrutura de APIs, assim em 2023 iniciaram-se os trabalhos de disponibilização de APIs de terceiros com o BB. O processo de “externalização” requereu alto investimento em segurança da informação, onde encontraram pontos de fragilidade sistêmica e evoluíram a linguagem como componentes de segurança. Além desse escopo, os conhecimentos obtidos nas atividades foram essenciais para expandir as formas de pagamento voltadas para o Agronegócio, permitindo desenvolver novas oportunidades para o cliente BB efetuar pagamentos de compras de insumos agropecuários com o saldo disponível das operações contratadas. Conclui-se que esse conjunto de atividades quando juntas tornam-se um único projeto à medida que as estruturas tecnológicas desenvolvidas demonstram ser ativos importantes e estratégicos para o Banco nesse setor da economia, servindo essencialmente para incremento do conhecimento, criação de novos negócios, ampliação dos produtos e serviços voltados ao Agronegócio brasileiro. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: Software PALAVRAS CHAVE: Blockchain, Automatização, Escalabilidade, Performance, Operações. Investimentos, Crédito. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: A atividade de pesquisa da Plataforma de Ativos Agro é composta inteiramente por elementos disruptivos tanto no Banco do Brasil quanto no mercado. Isso porque a solução deste problema técnico-científico envolve o entendimento de negócios do Banco e do estado da arte de sua posição relacionada ao blockchain. Especialmente, o DREX - moeda brasileira em formato digital - emitido em plataforma operada pelo Banco Central (BC). Sendo este o fato gerador de conhecimentos das possibilidades que o BB pode atuar no mercado do ponto de vista tecnológico. Observou-se que com as movimentações do DREX, abriam oportunidades para a pesquisa de “tokenização de ativos” no Banco. A partir do levantamento da problemática vislumbrou-se a estruturação de solução em blockchain capaz de automatizar e orquestrar toda a cadeia financeira, agregando eficiência, transparência e escalabilidade em novos negócios digitais. Contudo, no Brasil não existem casos práticos de estudo e aplicação do blockchain nas estruturas de negócio de Instituições Financeiras, tal constatação exigiu da equipe técnica do Banco a evolução da geração do conhecimento para alcançar no desenvolvimento experimental algumas hipóteses para este problema técnico-científico. Dentre os tópicos de pesquisa, ressalta-se o estudo de novas técnicas para o BB executar criptografia, estruturar local físico de armazenamento, definir fluxo de armazenamento digital das chaves dos clientes, entre outros aspectos. Os primeiros passos do projeto deram-se com a experimentação de uma nova rede blockchain própria do Banco do Brasil, similar ao conceito do DREX. Para o desenvolvimento houve um amplo aprendizado sobre o framework BESU - estrutura blockchain descentralizada -, sendo uma tecnologia nova para o escopo tecnológico brasileiro, como também ao BB. Com esta tecnologia foi possível o desenvolvimento da infraestrutura dessa nova rede blockchain. O framework foi projetado para casos de uso de redes autorizadas públicas e privadas, sendo expansível para redes de testes. Os próximos passos para os desenvolvimentos experimentais do projeto foram relativos à Plataforma de Tokenização (TAF). Para a estruturação foi crucial o aprendizado sobre as tecnologias de smart contract (contratos inteligentes) e linguagem de programação Solidity. Por sua vez, esta linguagem única possibilitou a experimentação dos smart contracts nesta rede codificada com o BESU. Neste momento do projeto, houve a implementação do Smart Contract Token BB. Com esta tecnologia constatou-se a facilitação na emissão de Tokens, como também a habilitação em tecnologias de transações descentralizadas. Após o entendimento dos conceitos técnicos aprendidos na TAF, partiu-se para o desenvolvimento experimental da Plataforma de Negociação de Ativos Tokenizados (NTD), no qual contou com o front-end implementado no aplicativo APJ, e a disponibilização de novas funcionalidades, a exemplo da integração com a carteira de ativos digitais do BB. Em relação às outras necessidades dessas atividades, os pesquisadores e desenvolvedores se preocuparam em garantir a cibersegurança das soluções, utilizando novas técnicas de verificação da legitimidade de transações, acesso a dados sensíveis e bloqueio de acessos maliciosos. Por fim, salientamos que estes desenvolvimentos ainda se encontram em fase de pesquisa e experimentação, justamente porque esse tipo de tecnologia enfrenta desafios, tanto técnicos como relativos à regulamentação brasileira. Inclusive, nos primeiros testes e estudos do projeto, constatou-se que as estruturas de tecnologia da B3 (bolsa de valores brasileira) demonstravam não estar preparadas para se conectar às tecnologias de blockchain. Esse fator é uma demonstração das limitações tecnológicas do escopo brasileiro no assunto. Destaca-se o pioneirismo do BB para o estudo de novas soluções que podem transformar o setor financeiro brasileiro, principalmente ao setor de Agronegócio. DESAFIO TECNOLÓGICO: No que tange a pesquisa da Plataforma de Ativos Agro, o maior desafio foi o caráter pioneiro do projeto, pois assim como supracitado ainda não existe um caso prático e público de experimento com blockchain para “tokenização” de ativos. Logo, esse fator exigiu a evolução do corpo de conhecimento para alcançar o desenvolvimento experimental. Houve um grande esforço da equipe de pesquisa para aprender novas linguagens de programação e tecnologias, a exemplo o entendimento da estrutura de smart contract e as hipóteses de aplicabilidade ao BB, linguagens de programação como a Solidity, a qual é voltada para implementação de contratos inteligentes em várias plataformas blockchain, principalmente Ethereum. Por ser uma área de conhecimento completamente nova, exige-se que toda a pesquisa e o desenvolvimento experimental sejam baseados sob a perspectiva de incertezas, visto que não há uma regulamentação completa sobre o tema, bem como não há bases teóricas ou científicas sobre a sua aplicabilidade em ambientes financeiros tradicionais. Nesse cenário, para a geração de conhecimentos, características específicas  foram determinantes para a viabilização técnica da pesquisa, como por exemplo, utilizar-se da criatividade de transformar os conceitos de ativos reais de forma funcional para superar a complexidade que o blockchain propõe. Adicionalmente, essa atividade do projeto foi um desafio de negócio, pois as áreas ainda não possuem uma visão clara sobre o limite do possível nessas novas tecnologias. Os desafios ainda são superados diariamente no estudo pela busca de soluções práticas para o conhecimento a ser desenvolvido e aplicado.Outro desafio técnico constatado ao longo da pesquisa foi a integração de vários módulos com funcionalidades diferentes. No que tange ao funcionamento do blockchain, várias funcionalidades de software precisam atuar em conjunto, demandando uma dificuldade adicional para que todas as fases sejam corretamente codificadas. O que nos leva ao ponto da segurança da informação, no qual é um fator crucial para o BB, tanto no que tange a dados quanto às transações financeiras, já que existem várias camadas de segurança envolvidas para a rede blockchain. Devido à grande estrutura do BB, em conjunto com a dificuldade de emissão de tokens, a estruturação de uma rede interna para descentralização de transações e a garantia que haja segurança, adiciona um componente de complexidade a mais, exigindo uma excelente organização da metodologia de trabalho.Complementarmente ao projeto da Plataforma de Ativos Agro, outros problemas técnico-científicos  devem ser evidenciados. Para alavancar ainda mais a presença do BB no cenário de crédito financeiro do Agronegócio, foram propostas atividades de expansão das fronteiras do Portal de Crédito. Deu-se início ao trabalho exposição de APIs. Porém, devido ao tamanho das estruturas sistêmicas do BB, essa atividade exigiu um investimento alto em segurança, visto que durante a fase de estudos houve algumas hipótestes de fragilidade no Portal. Constatou-se que a falha apresentada era causada por falta de mecanismos de controle de recursos que permitiam que usuários consumissem as APIs de forma livre. Esse acesso abre espaço para que usuários mal-intencionados buscassem informações nas APIs, acarretando problemas ao BB. Dessa forma, precisaram da implementação de novas técnicas de cibersegurança tanto no âmbito de linguagem de programação quanto nos componentes de segurança, como a inclusão de Captcha em um viés mais simples, até soluções de validações mais complexas, como a exigência de certificação (client credentials) nos end points das APIs para acesso dos agentes bancários externos e validação de tais certificações. METODOLOGIA: Com o objetivo de se alinhar aos novos paradigmas tecnológicos, o Banco do Brasil tem tomado esforços para modernizar sua metodologia de trabalho. Os meios tradicionais de projeto foram questionados nas atividades da Plataforma de Ativos para o Agro porque as etapas processuais foram substituídas pela abordagem ágil, dividindo o projeto em fases, com ênfase na colaboração e na melhoria contínua, onde as equipes seguem um ciclo de planejamento, execução e avaliação. O conceito de adaptabilidade do ágil é muito importante para as atividades do projeto, sendo necessário o teste empírico, a exemplo dos mapeamentos iniciais do que é ou não possível de ser desenvolvido. Especificamente nas fases iniciais, foi utilizado a prática de design thinking para estimular a ideação de soluções ao abordar problemas relacionados ao blockchain. Nessa fase da metodologia, o foco esteve nas limitações dos usuários em relação ao produto. Complementarmente, junto ao time de UX, são feitas todas as prospecções e visões do mercado sobre o assunto de blockchain, contratos inteligentes etc., para entender quais são as novidades, as principais dificuldades e assim com a aplicação de técnicas como brain stormming, gerar novas ideias e soluções. Em seguida, a equipe faz a prototipação da ideia selecionada, aplicando os conceitos aprendidos nas pesquisas e nas fases iniciais do projeto em forma de MVP, ou seja, produto mínimo viável. Essa estratégia fornece valor por meio de entregas regulares e dissemina o conteúdo para as áreas de negócio de forma concreta. Esse método personalizado de desenvolvimento é extremamente importante, pois explora novos territórios. O desafio é em parte mitigado pelas analogias técnicas, como transpor conceitos de ativo imobiliário para o mundo digital. As incertezas se estendem para a metodologia, pois há falta de um ambiente para testes da tecnologia, desta maneira todos os testes e avaliações dos MVPs são feitos por especialistas dentro do BB. Tais fatores somados à falta de regulamentação, são os fatores chaves de atraso da pesquisa e aprofundamento desse universo de blockchain e ativos digitais. INFORMAÇÃO COMPLEMENTAR: De forma semelhante a outras dificuldades apresentadas, uma das APIs que contemplam o Ecossistema do Portal de Crédito: a API para Simulação de Crédito Rural Também apresentou dificuldades na conformidade com as camadas de segurança e juntamente com as outras soluções vêm compondo a curva de aprendizado da equipe. O conhecimento obtido foi essencial para desenvolver novas estruturas de software para os clientes BB do Agronegócio. Como novas formas de negócio, o Simulador de Crédito Rural passou a possibilitar em suas funcionalidades a variação de taxas (taxas pós fixadas) para os produtores que queiram dividir seu crédito em inúmeros itens como máquinas, equipamentos, implementos agrícolas. Para melhorar ainda mais a experiência do cliente com o simulador cita-se a expansão para diversos canais do Banco, como por exemplo o Whatsapp. Foram necessárias novas parametrizações de dados para integração das APIs, mas principalmente o desenho e desenvolvimento de uma nova jornada do usuário com foco em possibilitar o atendimento pela plataforma de Whatsapp. Isso porque no Whatsapp a jornada é um pouco mais reduzida, logo os estudos e pesquisas para entender o seu usuário é de extrema importância para ter uma solução competitiva no mercado e incluir cada vez mais novos métodos digitais para aquisição de crédito Rural. Ressalta-se que esta é a primeira vez que o BB disponibilizou o simulador de crédito em canais digitais como Whatsapp. Porém, esta não foi a única novidade neste projeto, com destaque para estruturação de uma nova solução de API que permita ao cliente efetuar pagamentos de compras de insumos agropecuários com o saldo disponível das operações contratadas. Neste desenvolvimento de software novo para operações bancárias, o cliente pode acessar uma plataforma de compra, como marketplace ou até mesmo presencial, realizar a compra de produtos agrícolas (sementes, materiais, insumos etc.) e o pagamento da compra pode ser feita mediante crédito ou débito a liberar automaticamente da conta. Esse novo serviço digital além de diminuir a mão de obra do BB, também age como facilitador para o cliente, pois permite que seja realizado um autoatendimento. Visto que anteriormente era necessário o cartão físico e um fluxo burocrático. Nesta nova metodologia de serviço, o cliente pode realizar transações por cartão virtual, PIX e TED com a liberação praticamente instantânea dos recursos que precisa. Para a solução foram mais de dois meses de testes, visto que estamos falando de um banco de grande magnitude, com a estrutura de segurança complexa. Logo, foi necessário transpor uma estrutura multifacetada de requisitos, pensando em esteiras de “chargeback”, caso o usuário faça uma solicitação de cancelamento sem prejudicar a experiência, como também ao financeiro.Por fim, em 2023 o BB manteve a posição histórica como o principal agente financeiro no país, contribuindo de forma expressiva para o atendimento da demanda de crédito do segmento e da cadeia de agronegócio com crescimento de 12% no mesmo período. Portanto, o BB apresenta novos métodos de se utilizar a tecnologia para fornecer soluções digitais que vão aprimorar a experiência de seu cliente e incrementar anda mais a relação do Banco do Brasil com o Agronegócio brasileiro com as atividades desse projeto. Conclui-se que seria impossível ter resultados tão expressivos no ambiente de negócio sem investimentos em estudos e pesquisas de novas tecnologias como blockchain ou mesmo aplicação experimental de novas técnicas e tecnologias para a inclusão digital do atendimento. O engajamento digital do Banco já contava com 29,6 milhões de clientes ativos nas plataformas digitais do BB ao final de 2023 e as transações realizadas nesses canais representaram 93% das operações. RESULTADO ECONÔMICO: Estima-se que o resultado econômico será visto no longo prazo, na consolidação do sistema de blockchain, garantindo um potencial competitivo no mercado. As soluções digitais irão garantir maior segurança, confiança e aderência aos anseios dos cliente RESULTADO INOVAÇÃO:  A solução em blockchain, capaz de automatizar e orquestrar uma cadeia inteira, gera novas oportunidades de negócio, agregando eficiência, transparência e escalabilidade. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :No ano anterior, com este projeto o BB executou atividades transformadoras na arquitetura de software do Portal de Crédito, que proporcionou agilidade, flexibilidade e maior escalabilidade de processos. Em continuidade ao processo de transformação digital para o setor do Agronegócio, no ano de 2023, as atividades de P&D voltadas ao Portal de Crédito se guiaram principalmente nos métodos científicos previsto na metodologia Scrum, sendo feitas adaptações de acordo com as especificidades não previstas na etapa de planejamento. As atividades iniciais deste projeto foram para levantar as necessidades do público de agronegócio, principalmente com base nos comportamentos, serviços mais utilizados e canais mais procurados. Nesse contexto, montou-se um backlog, para priorização de requisitos de desenvolvimento. No estado da arte do desenvolvimento de software, para o desenvolvimento da API para Simulação de Crédito Rural, a partir do levantamento de requisitos e das novas formas de negócio, foi realizada a codificação em sprints para o Simulador de Crédito Rural, no qual passou a possibilitar em suas funcionalidades a variação de taxas (taxas pós fixadas), por exemplo. Como parte do método de trabalho, as atividades de integração contínua reforçaram para o time técnico a prática de criar e testar de imediato o código aos poucos, principalmente para integração das APIs junto à plataforma de software do Whatsapp, já que este era um novo canal de atendimento.Diante dos novos paradigmas tecnológicos, o planejamento sendo parte integrante das atividades da pesquisa na Plataforma de Ativos para o Agronegócio, tiveram um escopo inicial diferente daquelas sobreditas. A prática de design thinking foi um dos focos, principalmente para estimular a ideação de soluções ao abordar problemas relacionados ao blockchain. Como complemento as atividades junto ao time de UX, foram executadas para entender e captar as prospecções do mercado financeiro sobre o tema de blockchain, contratos inteligentes etc., finalizando com um brain stormming para a geração de novas ideias e soluções. Com a idealização do problema técnico-científico, os estudos teóricos foram mesclados junto com a parte prática. Portanto, ao longo do desenvolvimento a equipe técnica esteve aplicando os conceitos aprendidos nas pesquisas em um produto mínimo viável (MVP). Neste momento são realizados também os ensaios e avaliações dos MVPs junto aos especialistas do tema de ativos do BB. A partir destas validações os pesquisadores e desenvolvedores retornaram para os estudos, com todas as lições aprendidas. Desta forma, fecha-se o ciclo de desenvolvimento contínuo previsto na metodologia deste programa de projetos.
27262;2023;CNPJ: 00000000000191 RAZÃO SOCIAL :BANCO DO BRASIL SA ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27262;NÚMERO: 6 ID ÚNICO: 107482 NOME: Processos Internos DESCRIÇÃO: Com o objetivo de ser cada vez mais tecnológico , o Banco do Brasil (BB) tem direcionado parte de seus esforços e recursos para a solução de problemas técnico científicos no âmbito do desenvolvimento de novos processos e conflitos de software internos, os quais, no ano base de 2023, envolveram a exploração e experimentação de novas tecnologias em software, integrando-as em suas soluções, refletindo principalmente nos serviços bancários. Nesse contexto, foram concebidas melhorias incrementais em diversos métodos digitais e processos, como por exemplo, serviços de agendamento batch, envolvendo o fechamento de balanço, cartões, pix e outros, visando maior agilidade, segurança e uniformização nas atividades internas.Investiu-se nos esforços para resolver conflitos dentro do aplicativo BB (App BB), bem como nos métodos de incidentes e segurança, com o objetivo de garantir aos clientes uma melhor experiência na utilização do aplicativo no que tange à segurança e à praticidade. Dentre as técnicas aprimoradas, cita-se a biometria, na qual adicionou-se novas camadas de encriptação de dados e maior precisão a partir de informações de geolocalização.Ainda no contexto de estruturação de métodos mais eficazes de colocar serviços no mercado, foram realizadas novas técnicas de reengenharia dos dados de atendimento e da jornada dos usuários para aumentar a eficiência operacional no serviço de suporte técnico. Para isso, foi realizada a experimentação técnica de um novo algoritmo utilizando inteligência artificial (IA) para gerar prompts de dados a fim de contribuir no entendimento de pontos frequentemente solicitados por clientes. Além disso, foram mapeadas palavras-chave comumente usadas, bem como termos, expressões e captura de sentimentos, permitindo que o processo de tratamento de dados seja mais eficiente e preciso.Foi também estudado e desenvolvido no ano, uma nova estrutura de software para intranet coorporativa, a qual experimentou-se novas soluções relacionadas a ferramentas internas de comunicação, permitindo o compartilhamento interno de notícias, comentários e compartilhamento de informações corporativas. Entre as melhorias adquiridas a partir do desenvolvimento, está o estudo para a redução da quantidade de recursos (CPU, memória), reduzindo de forma significativa o tempo de resposta e latência, otimizando a performance.A partir do contexto geral desse programa e como resultado das iniciativas desenvolvidas nesse ciclo, o Banco do Brasil agrega novas oportunidades de negócio, solucionando problemáticas técnico-científicas financeiras e tecnológicas, garantindo estruturas e processos internos mais robustos e inovadores. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: Software PALAVRAS CHAVE: Automatização, Monitoramento, Segurança, Eficiência, Suporte, Dados. NATUREZA: Processo ELEMENTO TECNOLÓGICO: No contexto do desenvolvimento experimental de automatizações de serviços de batch - lote de pontos de dados que foram agrupados em um intervalo de tempo específico -, houve elementos resolutivos para o problema técnico-científico, no qual demonstra que o estado da arte do processo anterior, a documentação ocorria manualmente, com necessidade de interpretação humana do documento gerado. Logo, a fim de alcançar maior agilidade com a digitalização do método de trabalho, a solução proposta foi a codificação de um arquivo estruturado de configuração para que esta possa ser feita de forma automática, ao invés da documentação manual. Com este novo algoritmo se permite buscar automaticamente modelos de configuração e codificação pré-existentes no ambiente de desenvolvimento, de maneira a permitir que o desenvolvedor apenas complemente suas necessidades, sem ser preciso que todo o trabalho seja feito manual. Dessa forma, é possível realizar a interpretação automatizada pelo novo algoritmo desenvolvido, reduzindo e otimizando o trabalho realizado.Além da otimização de metodologias de desenvolvimento por tecnologias digitais, é também preocupação do Banco garantir estruturas que gerem segurança da informação. No âmbito dos processos relacionados ao App BB, destaca-se a criação de técnicas novas e originais de encriptação e segurança. Especificamente, foram desenvolvidas novas técnicas envolvendo a utilização de biometria facial, bem como coleta de dados baseados em geolocalização, com o objetivo de agregar acurácia à autenticação e login dos clientes. Para que viabilização técnica desta nova solução de software, foi necessário desenhar novos fluxos, contendo componentes diferenciados como novas telas, liberações e mensagens para os usuários. Adicionalmente, foi estudado um novo fluxo de liberação dos dispositivos – processo de realizar movimentações financeiras pelo computador, smartphone ou tablet - para que pudesse ocorrer de forma 100% (cem por cento) digital, reduzindo assim a necessidade da ida do usuário para uma agência física, agregando conveniência nos serviços prestados.Uma das novas tecnologias que compõe a gama de experimentações técnicas deste programa de projetos foi o estudo e aplicação de IA generativa no contexto de aumento da eficiência operacional do serviço de suporte técnico, a qual foi utilizada para compor ideias de prompts que os usuários poderiam realizar em determinados atendimentos. Tecnicamente para este desenvolvimento experimental, foi utilizada a técnica de machine learning voltada para processamento de linguagem natural (também conhecido como NLP). Com esta técnica abrangente, buscou-se superar barreiras de entendimento e processamento de dados não estruturados, contribuindo ativamente para redução  de processamento manual para execução de processos mais otimizados. Isto porque o NLP é um ótimo conjunto de técnicas para processamento de textos, a fim de decodificar o seu significado e seu contexto.Por fim, ainda neste programa, destaca-se o desenvolvimento de um novo sistema operacional corporativo (intranet), como uma solução codificada e tecnicamente estruturada inteiramente pelo corpo técnico do BB, buscando extinguir as limitações geradas por fornecedores. Para esta experimentação de software foi necessário a geração de conhecimentos técnicos e propostas de ideias criativas para unificar e integrar os canais de comunicação e melhorar a comunicação interna. Dentre as atividades deste projeto, foram desenvolvidas soluções para permitir que todos os softwares possam ser tornados como aplicativos, além de estruturação da arquitetura para aprimorar a experiência do usuário interno na utilização da intranet. DESAFIO TECNOLÓGICO: Os projetos sobreditos envolvem solucionar questões técnicas com o uso de novas tecnologias, experiência técnica e criatividade. Para essas questões, é de extrema importância compreender que o BB dispõe de uma estrutura organizacional particularmente complexa, com inúmeras regras de negócio e demandas regulatórias obrigatórias. No desenvolvimento de algoritmos e automatizações de serviços de batch, essa complexidade foi evidente, visto que há a necessidade de a solução manter a compatibilidade com os processamentos de Mainframe (sistema legado), em conjunto com o processamento de lote, tendo em vista as diversas exceções que ocorrem neste processo. Esse aspecto, além de ser tecnologicamente desafiador, é potencializado por expor o Banco a questões legais, devido o envolvimento de rotinas classificadas como críticas na metodologia operacional rotineira.Devido à grande estrutura do BB, cada projeto e atividade neste programa possui uma especificidade, como integrações, superação de desafios técnico-científicos e regulatórios. Para cumprir todas as autenticações do Banco, nessas circunstâncias, são trocadas informações com as equipes externas e de outras áreas do BB para validar se as soluções criadas atendem as gerências pertinentes e cumprem as normas do BACEN. Para a realização de melhorias voltadas à segurança, as barreiras tecnológicas enfrentadas estão principalmente relacionadas com a elevada demanda regulatória do Banco Central, como por exemplo, a exigência que o usuário só possa ter acesso ao aplicativo nos dispositivos permitidos e esse fluxo de segurança envolve fatores complexos de acesso.Quanto a experimentação de IA no atendimento e experiência do suporte técnico, a grande dificuldade esteve em compreender, viabilizar e catalogar as necessidades dos clientes em uma base de dados, bem como melhorar o método de coleta das informações, tendo vista a dimensão dos serviços prestados pelo Banco. Existe uma alta complexidade no tratamento de dados, uma vez que são submetidos dados não estruturados que fogem do padrão, podendo ser obtidos por diversas plataformas de atendimento, assim como telefone, e-mail, pessoalmente em agências etc. Dessa forma, a concepção de soluções envolveu primeiramente o ganho de conhecimento relativo às novas tecnologias como NLP (natural language processing) e LLM (large language model), envolvendo testes, entendimento de modelos e verificação de assertividade dos modelos gerados.Por fim, em relação ao desenvolvimento do sistema operacional interno (intranet) do Banco, os maiores desafios estavam vinculados à solução de conflitos em software, além de que o amplo uso da ferramenta fazia com que o uso da memória cache fosse sobrecarregado. Para que a solução fosse viável tecnicamente, houve a aplicação de técnicas de reengenharia do software com a codificação de melhorias nas regras de cache, refatoração mais performática e melhorias nos bancos de dados. Como exemplo de solução proposta, a refatoração de códigos alterou o que anteriormente era síncrono para assíncrono. Desta forma, o software passou a ter a capacidade de executar várias tarefas ao mesmo tempo, sem bloquear a execução do programa. METODOLOGIA: Os projetos foram desenvolvidos utilizando conceitos de metodologias ágeis, inspirando-se principalmente no Scrum e Kanban, sendo feitas adaptações de acordo com a necessidade das atividades envolvidas, incluindo o uso de DevOps. Em termos gerais, a seguinte metodologia foi utilizada:•	Planejamento e Design: Nessa fase inicial, foram definidos os objetivos do projeto e as necessidades dos usuários. Também são apresentados requisitos funcionais e tipos de dados a serem usados durante o projeto da solução, bem como padrões recém-desenvolvidos para acomodar regulamentações bancárias e financeiras. A importância dessa etapa reside na criação de uma fundação sólida para o desenvolvimento, garantindo que o produto atenda às expectativas dos stakeholders,•	Desenvolvimento: Foram utilizadas metodologias ágeis (Scrum e Kanban), para promover uma abordagem iterativa e incremental. Essa prática permite flexibilidade e adaptabilidade durante o desenvolvimento, assegurando que as funcionalidades mais críticas sejam entregues primeiro. Por meio de sprints quinzenais, foram definidas quais ferramentas utilizar, bem como as linguagens de programação adequadas,•	Testes: Foram implementados testes contínuos para garantir a qualidade e a performance dos produtos desenvolvidos. Essa etapa foi crucial para a identificação e a correção de falhas, evitando custos adicionais e atrasos nos lançamentos. Dentre os testes feitos, podem ser citados:I.	Teste de usabilidade – O objetivo deste teste foi a experiência do usuário, a fim de garantir que as plataformas sejam eficientes, agradáveis para o público.II.	Teste unitário – Incluiu a verificação do comportamento da menor unidade no sistema. Tecnicamente, isso seria uma classe ou mesmo um método de classe em linguagens orientadas a objetos, e seria um procedimento ou função em linguagens procedurais e funcionais.III.	Testes automatizados/regressão – Realizados com o objetivo de descobrir possíveis problemas, além de assegurar que novas funcionalidades não apresentariam novos defeitos e nem afetariam negativamente as outras funcionalidades existentes.•	Implementação e Integração: Os novos sistemas foram adaptados com as plataformas presentes. A competência técnica é vital para que seja garantida uma integração fluida e funcionalidade completa,•	Lançamento e Manutenção: Após o lançamento, é monitorado o desempenho dos produtos e coletados os feedbacks dos usuários. •	Para a finalização dos projetos, realiza-se a homologação das soluções garantindo que todos os requisitos funcionais e de negócio estejam em correto funcionamento. Por meio da execução exaustiva de testes, realizado de maneira controlada, foi possível avaliar se os sistemas se comportavam, ou não, conforme o especificado e se apresentou falhas. INFORMAÇÃO COMPLEMENTAR: Como informações complementares, das quais estão relacionadas aos impactos dos resultados dos projetos, podem ser citadas que em decorrência das ações voltadas ao atendimento dos usuários. Os usuários do App BB Pessoa Física manifestaram-se como “muito satisfeitos”, segundo a pesquisa do indicador Customer Satisfaction Score (CSAT), da qual apresentou nota 89,21 em uma escala de 1 a 100. Além disso, dentre os 29,5 milhões de clientes digitais do BB, cerca de 12 milhões atingiram o nível de maturidade digital considerado avançado, segundo critérios de indicadores internos. O App do BB permaneceu entre os mais bem avaliados do mercado financeiro nas plataformas de download.Além disso, no ranking trimestral do BACEN sobre reclamações das instituições financeiras, quanto maior a posição, melhor. Em 2023, o Banco do Brasil manteve a melhor posição entre as instituições tradicionais e alcançou um resultado histórico, atingindo menos de 500  reclamações consideradas procedentes no quarto trimestre de 2023.Com a continuidade de investimentos em frentes de IA e os esforços no aumento da capacidade de processamento em nuvem, contemplando soluções como o PIX e o Open Finance, o engajamento digital ao final de 2023 contava com 29,6 milhões de clientes ativos nas plataformas digitais do BB. Dessa forma, as transações realizadas nesses canais, representaram 93% das operações. RESULTADO ECONÔMICO: Houve uma melhora significativa no tratamento de clientes, bem como implementação de novas funcionalidades, aumentando a fidelização e aportes de usuários, obtendo assim um aumento da competitividade no mercado bancário. RESULTADO INOVAÇÃO: As soluções desenvolvidas proporcionam aos colaboradores e clientes maior segurança, disponibilidade e conectividade em todo o país, além de gerar novos conhecimentos e ampliar o escopo tecnológico do Banco. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :Em anos anteriores, novas e aprimoradas funcionalidades foram agregadas a fim de que o BB explorasse as tecnologias atuais, integrando-as com suas soluções de software existentes. Desta forma, foram concebidas melhorias incrementais em diversas metodologias processuais. Cada vez mais digital, o BB mantém sua presença digital em complementaridade à rede física e com a oferta de soluções. Portanto, o ano base de 2023, tem direcionado parte de seus esforços e recursos para o desenvolvimento de novos processos e sistemas internos digitais. Nesse contexto, conforme supracitado foram concebidas melhorias incrementais em softwares para o desenvolvimento de novos e aprimorados processos, como por exemplo, processamento de agendamento batch, novas técnicas e métodos de incidentes e segurança no App BB.Em resumo, as atividades de P&D executadas no ano-base inspiraram-se principalmente no Scrum e Kanban, sendo feitas adaptações de acordo com a prática de DevOps ao longo das sprints. No estado da arte do desenvolvimento de software é impossível a desvinculação desta metodologia. No entanto, a experimentação de técnicas de machine learning traz consigo uma série de desafios, consequentemente atividades diferenciadas de teste e construção de modelos de dados. Já que a pesquisa sempre precisa de uma contextualização, pois a todo momento de experimentação existia algum resultado que tem efeito prático entre as variáveis do modelo.Nos demais desenvolvimentos que ecoam na área de desenvolvimento de software, a inclusão da cultura de DevOps é crucial para que sejam realizadas as atividades de planejamento e criação listas de pendências, acompanhamento de bugs, gerenciamento do desenvolvimento de software com o Scrum, e a utilização de quadros do Kanban. Observou-se que com a prática houve maior flexibilidade e adaptabilidade durante o desenvolvimento (escrita de código-fonte, teste, revisão e integração do código), assegurando que as funcionalidades mais críticas fossem entregues primeiro. A entrega se torna parte da agregação de valor dos algoritmos. A conclusão do ciclo se dá na fase de operação, cuja estratégia envolve manter, monitorar e solucionar problemas dos programas de computador em ambiente de produção. Este fator apresentou-se como uma atividade chave para garantir a confiabilidade do sistema, a alta disponibilidade e o objetivo de tempo de inatividade igual a zero.
27262;2023;CNPJ: 00000000000191 RAZÃO SOCIAL :BANCO DO BRASIL SA ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27262;NÚMERO: 7 ID ÚNICO: 107483 NOME: Estudos e desenvolvimentos de modelos analíticos e de inteligência DESCRIÇÃO: Modelos analíticos têm um papel emblemático no âmbito tecnológico atual, já que geram insights que podem revelar novas oportunidades de mercado, como também permitem prever situações de riscos para orientar com mais assertividade a tomada de decisão. Em 2023, o Banco do Brasil (BB) se manteve no movimento de ampliação do uso de modelos com inteligência artificial (IA) em seus produtos, serviços, processos e experiências a partir de novos modelos e da reestruturação da Plataforma de Inteligência Artificial. Essa Plataforma surgiu em 2020 e nos últimos anos vem sendo desenhada pelo BB como acelerador digital, principalmente na disseminação de machine learning (ML) e deep learning (DL). A criação do Motor para Avaliação de Inteligência Artificial (MAIA), integrado à Plataforma de IA do Banco do Brasil, trouxe avanços significativos em termos de eficiência e compliance dos modelos desenvolvidos. Inclusive, em 2023 foram mais de trinta novas soluções disponibilizadas para clientes internos, externos e funcionários. Dessa forma, as iniciativas e os projetos do BB permitem que cada vez mais a IA se fixe no DNA dos negócios do Banco de forma intuitiva, completa e responsável.Assim, esse programa não apenas representa novas melhorias incrementais, como também abraça o desenvolvimento de novas tecnologias como fator chave de evolução de serviços. Isso se reflete nas carteiras de negócios, a exemplo do agronegócio, que no último trimestre do ano cresceu 4,5%. O BB entende que seus modelos analíticos e estatísticos têm um grande impacto para o alcance desse resultado. Dentre os principais, destacam-se os modelos de geolocalização para monitoramento de lavouras, nos quais têm como principal objetivo classificação de lavouras de soja e milho não apenas para validação do retorno financeiro do investimento, como também para compreender e prever eventos climáticos que impactem as culturas. Um aspecto interessante sobre estes algoritmos é a alta capacidade de análise computadorizada, nos quais são computadas imagens de satélites em pixel para o acolhimento de dados sobre as culturas dos grãos supracitados. Assim, os estudos focam na performance desses modelos para que estejam cada vez mais precisos nas informações em tempo real.Em parceria com o âmbito acadêmico, esse projeto conta com a exploração e a pesquisa matemática aplicadas na análise de risco financeiro. Os estudos abordam especificamente modelos de mensuração de risco para crédito, nos quais demonstram quais perfis de clientes terão um atraso de quitação dos seus débitos maior do que noventa dias. Porém, dado que o modelo possui um atraso de doze meses, isso prejudica na tomada das decisões. Logo, com as pesquisas busca-se a viabilidade técnica de uma nova abordagem estatística para que com um menor tempo de aplicação dos modelos, possa se entender a relação entre tempo e qualidade da informação para análise preditiva. Ainda sobre riscos, é notável que o BB é um dos maiores bancos do país, portanto, não é incomum que sejam abertos processos contra o Banco, ou vice-versa. Nesse sentido, existem estruturas de suporte e de controle nessas situações. Dentre essas ferramentas, destacam-se modelos de IA que tratam de problemas como a litispendência. Esse tipo de processo ocorre quando duas ou mais ações judiciais são iniciadas envolvendo as mesmas partes com causas e pedidos similares. Por ser considerado um problema processual, em 2023 foram treinados modelos para cálculos de similaridade no histórico de processos do Banco, para o tratamento automático de tais divergências legais. Concluímos que esse conjunto de atividades se torna um único projeto à medida que os dados se demonstram como ativos estratégicos a cada iniciativa, servindo-se essencialmente de inteligência analítica para aumento no corpo de conhecimento para a criação de novos negócios, a melhoria e ampliação dos produtos e serviços existentes. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: Software PALAVRAS CHAVE: Estatística, risco, modelo, modelagem, dado, informação, inteligência, inteligência artificial, IA, deep learning, machine learning, estratégia, aceleração digital NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Muito se discute sobre aprimoramento da pesquisa estatística aplicada na análise de risco financeiro, especificamente com foco na modelagem de dados para gerar informações de risco para políticas de créditos. Este fator apontar para o problema técnico-científico deste projeto que aborda sobre as rápidas transformações nas variáveis sociais, culturais e econômicas brasileiras que diretamente impactam na perda de performance dos modelos de risco. Tudo em razão do atraso da informação em doze meses, ou seja, atualmente no BB  identifica-se desvio nos modelos de risco passado um ano de execução. Logo, nesse cenário, poderia ser um ano inteiro de decisões imprecisas em relação às políticas de crédito. Os estudos aqui se concentraram na experimentação técnica sobre a degradação da performance de modelos com faixa de abrangência menor do que um ano. Apoiado por algoritmos em Python, a pesquisa correlaciona a curva de imprecisão dos modelos de acordo com as faixas de meses aplicadas por amostra. São observados os dados que seriam aplicados na perspectiva de um ano e comparados os resultados dos modelos aplicados com o horizonte de tempo reduzido. Com isso, são estudadas as relações entre ambos os cenários para  gerar um modelo da razão das estimativas com horizonte de tempos distintos. São experimentadas hipóteses para anteceder alertas sobre a degradação dos modelos de risco a partir de métodos por amostragem, otimizando o uso das informações disponíveis para reduzir o erro de predição. Essa experimentação permitiu que ao longo dos estudos constatar que a relação entre o horizonte de tempo para o evento fica mais forte quando está mais próxima do tempo original de análise. Observou-se também que para aqueles dados com metade dos meses, era possível obter boas estimativas para os erros do modelo com um horizonte de tempo de seis meses, isto é, metade do horizonte de tempo do modelo original. Esse ganho de conhecimento proporcionou ao BB reduzir a espera e aproximar-se da análise do seu credor em tempo real e controlar a degradação do modelo, por meio da antecipação de ajustes. Tomadas de decisão são importantes para definir estratégias, ainda mais nos campos jurídicos, pois possibilitam uma chance ímpar para priorizar objetivos e metas. O tratamento de litispendência dentro do BB era inteiramente manual e controlado com planilhas, dessa forma, um grupo de advogados era responsável por fazer as análises dos milhares de processos. Neste problema técnico-científico, abre-se margem para resolução por meio da codificação de algoritmos de automação deste fluxo. Inicialmente levantou-se a hipótese de utilização de se IA para tal, porém, entendeu-se que a inteligência artificial com treinamentos tradicionais tem apenas a capacidade de imitar a inteligência humana, por sua vez, o deep learning traz percepções e se adapta a diferentes cenários. Optou-se pelo treinamento de modelo com a técnica de DL para fazer cálculos de similaridade de processos. Todos os documentos e processos estão armazenados em uma base de dados (DB), o JED. Com uma tecnologia de OCR, os dados são capturados e transformados de binários para texto, então deposita-se esse conteúdo em outro DB (DB2), esse texto transformado é aplicado ao modelo. A partir disso, o modelo agrupa o histórico de processo por autores e faz a comparação de processo a processo. Destaca-se que o modelo devolve uma matriz de valores, gerando uma previsão de quão similares são essas petições, caso a similaridade dos dados seja maior que 80%, o processo é passado para um advogado atestar a litispendência. Com esses modelos, estimam-se ganhos financeiros no âmbito jurídico devida à alta asseguração. Entretanto os resultados não são limitantes, pois abrangem tanto o pareamento quanto o aumento no conhecimento entre a tecnologia e o jurídico. Assim, expandindo-se os horizontes da inteligência analítica para o BB, como também para o mercado. DESAFIO TECNOLÓGICO: A construção de modelos matemáticos e estatísticos já trazem consigo um grande peso de dificuldade. Para esse projeto, o fator de incerteza é a ausência de bases técnicas e de referências teóricas robustas sobre o tema, uma vez que é um trabalho de pesquisa único tanto na parte acadêmica quanto para o mercado. Dessa forma, a falta de fundamentação teórica pode resultar em decisões inadequadas sobre a aplicação e experimentação das variáveis e métodos estatísticos. Tais fatores acabam comprometendo a validade dos resultados gerados no estudo, de modo a gerar conclusões “falso-positivas” com relação aos dados analisados e aplicados em modelos. Portanto, por se tratar de uma metodologia de treino e análise que está sendo criada a partir de uma nova hipótese, a solução é executar ensaios de viabilidade técnica para diversos métodos estatísticos, nos quais de forma experimental, estão sendo combinados com bibliotecas da linguagem Python e conceitos matemáticos para alcançar alto grau de acurácia dos modelos de risco. Aliado a isso, outro desafio é a construção da base de dados para estudo, pois para conclusão da pesquisa foi preciso trabalhar com duas categorias de base de dados. Utilizou-se bases simuladas, suficientemente próximas às originais e dados oficiais do BB. O motivador desta ambiguidade de métodos é a de segurança da informação, já que muitas informações sensíveis estavam sendo consumidas e manipuladas na pesquisa. Certamente havia um risco real de que a base simulada gerasse divergência nos modelos. Por isso, a metodologia da atividade foi o fator chave para a solução, já que progressivamente, os estudos e a experimentação de novas variáveis e métodos, foram se apropriando tanto do conhecimento técnico quanto dos conceitos para mensurar, ou até mesmo prever os possíveis erros.Portanto, entendemos que o conhecimento nesse projeto é o fator principal. Inclusive para o segundo projeto supracitado, parte dos desafios técnicos foi parear o conhecimento técnico em tecnologia e dados com termos e técnicas jurídicas. Dado o fato que o time nunca havia trabalhado em projetos do campo Legal. Por isso, na parte inicial do projeto, foi realizada uma análise exploratória, na qual forneceu os dados para iniciar os estudos, no entanto, se depararam com uma barreira bem comum na ciência de dados, que é: o processamento. Constatou-se que havia uma latência maior nos metadados do que sobre o conteúdo do documento em si. Tal fator, ocasionava imprecisão ma análise e não dava o retorno esperado pelo estudo técnico. A opção viável, porém, muito mais complexa foi utiliza-se de técnicas de Linguagem Natural, ou NLP. Esse se tornou um grande desafio, pois é muito difícil mapear um idioma, já que os termos possuem variações a depender da região e do período histórico do processo. As palavras tornam-se obsoletas cada vez mais rápido, dificultando o treinamento dos algoritmos para a captura e o cálculo necessário para entender se há ou não uma possível litispendência.A captura das informações ficou a cargo de ferramentas de OCR, nas quais, foi preciso o estudo  aprofundado sobre o código open source para encontrar a origem dos problemas na captura de dados. Outra grande fonte de estudo no projeto, foi a ferramenta HDFS. Ela possui um conjunto de funcionalidades envolvendo o ecossistema de dados. Dentre os requisitos do projeto estava usar a computação em nuvem pública (técnica de multicloud descrita nas informações complementares) para treinamento do modelo. A técnica escolhida para externalizar os dados foi criar um caminho único com auxílio do HDFS e ferramentas do Hadoop para criação de um gateway que transaciona tais informações do servidor interno para externo, assegurando a informação do BB. Por fim, concluímos que as incertezas no programa corroboraram nos campos de conhecimento não apenas para superá-las, mas também para estimular o crescimento da inteligência artificial no BB e no mercado. METODOLOGIA: Com a estratégia de aceleração digital, o BB começou a traçar padrões de desenvolvimento para que todos os softwares, modelos de dados, treinamentos de IA etc., cumpram requisitos de segurança e compliance. Esse padrão se apropria de conceitos ágeis, que tende a ser uma forma de trabalho mais adaptativa do que preditiva, ou seja, nesses termos passam existir inúmeras variantes de metodologias independentes, como no caso das atividades voltadas para o estudo dos modelos de risco para políticas de créditos financeiro. Essa adaptação metodológica foi necessária pois esta pesquisa, em sua essência, definir define um novo conceito de validação da assertividade do modelo, o que requer forçadamente um retreino dos algoritmos existentes ou a aplicação de novos algoritmos.Inicialmente neste método, foram utilizadas bases de dados simuladas, porém, suficientemente próximas as bases reais originais do Banco. Isso era necessário por conta de questões legais (LGPD), pois são movimentadas diversas informações sensíveis. Essa metodologia vem sendo construída naturalmente com ferramentas táticas essenciais, como a utilização de método amostral para ensaio de desempenhos sobre os modelos. Assim, um modelo para o horizonte de 12 meses é colocado sobre análise em diversas faixas de tempo menores, sendo três meses, quatro meses ou até seis meses. Com isso, o pesquisador buscou uma relação entre as imprecisão do modelo em horizontes de tempo distintos para criar uma estimativa em razão dos dois. Essa avaliação apoiou na análise da capacidade de generalização do modelo, principalmente na questão de defasagem. Com isso são incluídos ou excluídos parâmetros para uma nova amostragem até que se obtenham os resultados esperados.O método cíclico assemelha-se bastante à metodologia ágil Kanban, alvo das atividades relacionadas aos modelos para tratamento jurídico. Apesar do time trabalhar periodicamente sob os ritos do Scrum, entendeu-se que seria mais bem aplicado o Kanban devido à alta complexidade das tarefas do projeto, já que era difícil de repartir as histórias de usuário (descrição curta, informal e em linguagem simples de funcionalidade em um sistema sob o ponto de vista do usuário) em uma única sprint. Este fato se dá pelas especificidades necessárias ao desenvolvimento estarem em alto nível, próximas à um projeto apartado. Para mitigar os possíveis entraves no método de desenvolvimento foram encurtadas as entregas para maior agilidade no fluxo de desenvolvimento. De forma geral, o projeto segue bastante os ritos do Scrum com papéis sendo definidos, estratégias de segregação de atividades para resolução do problema técnico-científico, definição de backlogs e sprints quinzenais. Destaca-se que a base de dados utilizada para treinamento de DL não tem limite de tempo, ou seja, não existe regra de esvaziamento. Portanto, as etapas de aquisição e preparação de dados acabam se prolongando para que a configuração dos dados do OCR fosse suficientemente variada para o treinamento de deep learning. Inclusive, nessa etapa do projeto foram definidos os métodos de treinamento. Isso implica na seleção das redes neurais, mas sempre levando em consideração a complexidade do histórico de informações de processos.Nas atividades descritas no elemento complementar desse projeto são em grande parte utilizadoras assíduas do método ágil com Scrum. Especificamente, no tratamento dos modelos de risco destaca-se a utilização do framework Crispy DM. Esse método é específico para o desenvolvimento. Quando aplicado dividiu-se em seis etapas fundamentais, sendo as três primeiras etapas com o objetivo da coleta e da organização dos dados a serem analisados. Enquanto as últimas três buscaram a criação do modelo, e a colocação do modelo em prática. O método favorece a melhoria incremental do modelo, pois é possível sempre voltar em algumas etapas para refinamento. INFORMAÇÃO COMPLEMENTAR: Em continuidade aos modelos de risco, destaca-se a pesquisa e os desenvolvimentos experimentais sobre modelagens de dados de reclamações no Banco Central (Bacen). Esses modelos treinados em machine learning são utilizados para mitigar o nível de reclamações por parte do cliente ao Bacen. Por meio do aprendizado dos padrões de dados, são geradas probabilidades sobre os tipos de reclamações que têm maior potencial para se chegar ao Bacen, assim é possível ter uma ação proativa de resolução do problema junto ao cliente, pois reclamações impactam diretamente no ranking de instituições financeiras do Banco Central. Porém, em atividades investigativas, observou-se que os dados utilizados para treinamento já estavam desatualizados, visto que o comportamento de seus clientes mudou bastante nos últimos anos. Portanto, para resolução deste problema técnico-científico seria indispensável a recalibração do modelo, que viria a ocorrer depois da atualização das tecnologias.Com o Stack tecnológico (inclui as linguagens de programação, as bibliotecas e os frameworks) anterior não se tinha visibilidade do status de funcionamento do modelo. Inclusive, houve momentos neste modelo em que surgiam problemas no banco de dados, sendo possível descobrir após meses. Assim, em 2023 com a atualização das tecnologias para a esteira do AppDynamics, o monitoramento tornou-se mais veloz, incluindo benefícios ao modelo que passou a trabalhar com dados coerentes, consequentemente mitigando os riscos. Esses resultados se dão por evoluções de software nas principais bibliotecas do modelo e da linguagem Python, que oferece recursos, como bibliotecas e métodos robustos para o algoritmo do modelo. Entretanto, essa atividade conta com um grande desafio: o big data. A volumetria de dados é intensa. Em um único dia são trafegadas cerca de dez milhões de mensagens que precisam ser gravadas e treinadas. Isso gerou um fator de incerteza, já que ao longo dos trabalhos, gargalos são identificados e interferem na informação preditiva do modelo, consequentemente na mitigação de riscos ao BB.Assertividade e acurácia são pontos fundamentais em modelos analíticos, isso fica claro nos modelos de agronegócio, principalmente nos classificadores para grãos. O núcleo de curadoria levantou um problema técnico-científico sobre a captura dos dados de imagens em satélites, pois se deparavam tanto com dados quanto modelos não tão precisos. Como solução, foram retreinados os modelos com Perceptron multicamadas. A técnica trata-se de uma rede neural com múltiplas camadas contendo indeterminados neurônios artificiais, tais fatores aumentam a capacidade de aprendizado de tarefas complexas, como processos de classificação e regressão. As melhorias têm demonstram bons frutos sobre as análises geoespaciais do modelo, mas trazem consigo a necessidade de avanços na performance. Já que foram identificados latências na resposta. Segundo os estudos, a principal causa era a utilização de uma única tabela para transação e consulta de dados. Para solucionar, foi gerada uma fac-símile para consulta de modo que as queries fossem mais eficientes. Foi dedicado bastante estudo primordialmente na estruturação de diferentes queries até identificar que o principal causador dos problemas de performance eram as tabelas.Por fim, a performance sempre ganha um destaque quando estamos falando em inteligência artificial, aliado com responsabilidade. Logo, o BB em sua estratégia de aceleração digital, focou em disseminar o conhecimento sobre o tema com a Plataforma de IA, além desse objetivo, essa estrutura de software coleta, prepara o dado, desenvolve o modelo e os treina. Com seu desenvolvimento em 2023, foram utilizados novos conhecimentos arquiteturais com a inferência na computação em nuvem interna (BB) e monitoração em nuvem pública (IBM). Concluímos então que o BB reforçou consideravelmente sua estrutura analítica para gerar inteligência e mais conhecimento a partir dos eventos monitorados por dados e IA. RESULTADO ECONÔMICO: Entende-se que a atuação da inteligência de dados intensifica as cadeias de valor do BB, acentuando a rentabilização e a ampliação de novos negócios que se tornam fatores de pioneirismo no mercado.   RESULTADO INOVAÇÃO: Com esse projeto, o BB abre um leque de novas formas de relacionamento e engajamento a partir de jornadas inteligentes de comunicação, decisões baseadas em dados e mergulha em novas oportunidades de negócios com inteligência analítica. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;NÚMERO: 1 ID ÚNICO: 109712 NOME: EXPERIMENTAÇÃO DE TECNOLOGIAS AVANÇADAS PARA OTIMIZAÇÃO DE PLATAFORMAS DE PAGAMENTO DESCRIÇÃO: A presente linha de pesquisa e desenvolvimento experimental visa identificar a viabilidade de soluções tecnológicas para aprimorar a resiliência, eficiência e segurança de plataformas de pagamentos, por meio da realização de experimentos e prototipações. Entre os principais objetivos, estão a melhoria da resiliência e observabilidade através da adoção de uma arquitetura de microsserviços com orquestração de contêineres, a redução da latência e a garantia da consistência dos dados com processamento em tempo real, a implementação de um Data Lake federado para aprimorar a governança e privacidade dos dados, a adoção de uma arquitetura Zero Trust para segurança das transações financeiras, e a modernização das automações com a integração de Machine Learning e funções serverless.Dentre os principais desafios tecnológicos enfrentados, destacam-se a complexidade na configuração e manutenção da arquitetura de microsserviços, a necessidade de ajustes constantes para balancear latência e consistência dos dados em processamento em tempo real, a integração entre diferentes serviços e a detecção eficiente de dados sensíveis sem gerar falsos positivos, a implementação de ambientes isolados e a modificação na arquitetura existente para adotar a Zero Trust Architecture, além da variabilidade na precisão dos modelos de Machine Learning e latências inesperadas na integração com eventos IoT.O marco crítico desta linha de pesquisa consistiu no enfrentamento à complexidade inerente à configuração e manutenção da arquitetura de microsserviços, juntamente com o desafio de equilibrar a latência e a consistência dos dados em um ambiente de processamento em tempo real. Este ponto crítico destacou-se não apenas pela sua complexidade técnica, mas também pela sua capacidade de influenciar diretamente a viabilidade das soluções tecnológicas propostas para aprimorar a resiliência, eficiência e segurança das plataformas de pagamentos.A necessidade de realizar ajustes constantes na arquitetura e nos algoritmos para otimizar o processamento de dados, sem comprometer a segurança ou a privacidade, representou um desafio significativo que poderia colocar em risco o sucesso do projeto. A complexidade aumentava com a integração de diferentes serviços, a detecção eficiente de dados sensíveis sem gerar falsos positivos, a implementação da Zero Trust Architecture e a variabilidade na precisão dos modelos de Machine Learning, juntamente com as latências inesperadas na integração com eventos IoT.Foram propostas como soluções para esses desafios, a adoção de ferramentas de automação para simplificar a gestão da malha de serviços, a utilização de sistemas de bancos de dados distribuídos para um melhor balanceamento entre latência e consistência, a implementação de algoritmos de aprendizado de máquina para aprimorar a detecção automática de dados sensíveis, a exploração de soluções de virtualização baseadas em contêineres para criar ambientes isolados mais eficientemente, e a utilização de plataformas de desenvolvimento low-code para acelerar a integração de modelos de ML e eventos IoT.Espera-se que, após o desenvolvimento e implementação dessas inovações, a plataforma de pagamentos se torne mais resiliente a falhas, com uma gestão de tráfego e observabilidade aprimoradas, processamento de dados mais rápido e confiável, uma governança de dados mais robusta e uma segurança de transações financeiras significativamente reforçada. Além disso, a automação inteligente e preditiva dos processos deverá melhorar a eficiência operacional, permitindo uma gestão mais ágil e adaptável às necessidades dinâmicas do mercado e dos usuários finais. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Processamento,  dados,  experimentação,  testes,  segurança,  integração,  migração,  serviços,  automação. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Uma das hipóteses testadas foi que a utilização de uma arquitetura de microsserviços com orquestração de contêineres e uma malha de serviços melhoraria a resiliência, observabilidade e gestão de tráfego da plataforma de pagamentos. O protótipo desenvolvido utilizou ferramentas para orquestração de contêineres e gestão de tráfego, segurança e observabilidade. Foram configuradas políticas de circuit breaker, retries e timeouts para melhorar a resiliência dos serviços. A observabilidade foi aprimorada com a integração de ferramentas de monitoramento e tracing distribuído. Os testes de falhas simuladas mostraram uma melhoria na resiliência e na observabilidade, mas a complexidade adicional da malha de serviços gerou desafios significativos nas fases de configuração e manutenção.Outra hipótese focou na implementação de uma camada de processamento de dados em tempo real com o objetivo de melhorar a latência e a consistência dos dados replicados. A prova de conceito utilizou tecnologias de processamento de dados em tempo real e captura de mudanças. Embora a latência end-to-end tenha sido reduzida, a complexidade da configuração e a necessidade de ajustes constantes para manter a performance ideal apresentaram desafios. Além disso, a consistência dos dados replicados foi comprometida em cenários de alta carga, indicando que ajustes adicionais seriam necessários para garantir a viabilidade da solução.A equipe também explorou a implementação de um Data Lake federado, em conjunto com políticas de segurança para melhorar a governança, privacidade e acessibilidade dos dados. A criação do Data Lake federado e a implementação das políticas de segurança melhoraram a governança e a privacidade dos dados. No entanto, a integração entre diferentes serviços apresentou inconsistências, e a detecção de dados sensíveis gerou falsos positivos, o que exigiu ajustes manuais frequentes. A acessibilidade dos dados pelos usuários finais foi aprimorada, mas a complexidade de gerenciamento aumentou.Além disso, foi avaliada a implementação de um sistema de segurança baseado em Zero Trust Architecture (ZTA) para aumentar a segurança das transações financeiras e a proteção dos dados sensíveis. Os testes de penetração e simulações de ataques confirmaram a eficácia da arquitetura Zero Trust em aumentar a segurança. No entanto, a implementação de ambientes isolados revelou-se complexa e exigiu modificações significativas na arquitetura existente, além de um impacto inicial no desempenho das aplicações, o que indicou a necessidade de otimizações adicionais.Outra abordagem investigada foi a utilização de técnicas de migração híbrida e incremental, com o objetivo de minimizar o tempo de inatividade e garantir a integridade dos dados durante a migração para a nuvem. A migração experimental utilizou uma combinação de ferramentas para transferência de dados edge, migração contínua e extensão on-premises. Embora a combinação dessas técnicas tenha proporcionado uma migração eficiente com tempo de inatividade mínimo, a complexidade da coordenação entre diferentes ferramentas e a necessidade de monitoramento constante para garantir a integridade dos dados apresentaram desafios significativos. Em alguns casos, houve inconsistências nos dados migrados, exigindo intervenções manuais para correção.Por fim, a modernização das automações utilizando orquestração de workflows com integração de Machine Learning (ML) e funções serverless com triggers baseados em eventos IoT foi avaliada para melhorar a eficiência operacional e permitir a automação inteligente e preditiva dos processos. A reimplementação de processos críticos utilizando essas tecnologias demonstrou uma melhoria na eficiência operacional. No entanto, a precisão dos modelos de ML variou, exigindo ajustes frequentes e re-treinamento. Além disso, a integração com eventos IoT apresentou latências inesperadas em alguns cenários, impactando a automação preditiva dos processos. DESAFIO TECNOLÓGICO: A complexidade na configuração e manutenção da malha de serviços, apesar de ter melhorado a resiliência e a observabilidade, trouxe desafios significativos. A gestão eficiente da comunicação entre os serviços em um ambiente tão dinâmico e distribuído representou um desafio técnico, principalmente considerando os mecanismos de segurança, roteamento de tráfego e monitoramento.  Foi levantada a hipótese de que a adoção de ferramentas de automação para a implantação e gerenciamento de políticas poderia simplificar esses processos. Essas ferramentas, integradas com sistemas de CI/CD, prometiam reduzir a carga operacional envolvida na gestão da malha de serviços. No entanto, era incerto se a automação poderia abranger todas as complexidades específicas de cada serviço, e em cenários de falhas inesperadas e comportamentos dinâmicos do tráfego.A implementação de tecnologias de processamento de dados em tempo real e captura de mudanças para reduzir a latência end-to-end, enquanto se mantém a integridade dos dados sob carga intensa, era um desafio técnico complexo, envolvendo ajustes finos constantes e balanceamento entre performance e consistência. Dado o problema, o time propôs a utilização de sistemas de bancos de dados distribuídos mais avançados, que ofereciam mecanismos de consistência eventual e resolução de conflitos. A hipótese era de que esses sistemas poderiam equilibrar melhor a latência e a consistência, mesmo sob condições de carga intensa. Contudo, a dúvida sobre se a complexidade de implementação e o overhead de gerenciamento desses sistemas justificariam os benefícios permanecia, considerando as especificidades do ambiente de produção.A integração entre diferentes serviços e a detecção eficaz de dados sensíveis sem gerar falsos positivos trouxeram alguns desafios a serem superados. A implementação de um sistema que harmoniza a governança e a privacidade dos dados, mantendo a acessibilidade e minimizando a necessidade de ajustes manuais, requer uma solução sofisticada de gerenciamento de dados e segurança. Na tentativa de mitigar os riscos e incertezas, foi levantada a hipótese de implementar algoritmos de aprendizado de máquina para aprimorar a detecção automática de dados sensíveis, reduzindo os falsos positivos. A equipe acreditava que, com treinamento adequado e dados suficientes, esses algoritmos poderiam aprender a identificar padrões complexos de dados sensíveis. Ainda assim, não se tinha tanta certeza quanto à eficácia desses algoritmos em ambientes reais e sua capacidade de adaptação a novos padrões de dados.A complexidade da implementação de ambientes isolados e a modificação significativa na arquitetura existente para adotar ZTA, junto com o impacto inicial no desempenho das aplicações, destacou-se um desafio técnico. A transição para uma arquitetura de confiança zero exigia uma reavaliação profunda das práticas de segurança existentes e poderia requerer otimizações adicionais para mitigar impactos no desempenho. Diante dessa complexidade, a equipe explorou a possibilidade de adotar soluções de virtualização baseadas em contêineres para criar ambientes isolados de forma mais eficiente. Essa abordagem prometia facilitar a implementação da arquitetura Zero Trust, minimizando o impacto no desempenho das aplicações.A integração de modelos de Machine Learning para automação inteligente e preditiva dos processos, juntamente com a implementação de funções serverless para triggers baseados em eventos IoT, introduziu variabilidade na precisão dos modelos e latências inesperadas. Desenvolver um sistema que efetivamente combine análise preditiva com automação em tempo real, mantendo a eficiência operacional sob diversas condições, era um desafio técnico significativo. Dado o problema, a equipe hipotetizou que a utilização de plataformas de desenvolvimento low-code para a criação de workflows poderia acelerar a integração de modelos de ML e eventos IoT. METODOLOGIA: Para lidar com os desafios técnicos encontrados nesta linha de pesquisa, foram organizadas as seguintes atividades:1- Prototipou-se uma plataforma baseada em microsserviços usando ferramentas de orquestração e gerenciamento de containers. Foram implementados padrões de circuit breaker, configurou-se tentativas e timeouts para aumentar a resiliência do serviço e integrou ferramentas de rastreamento distribuído e monitoramento.Métricas:Mediu-se o tempo de recuperação a partir de falhas simuladas e comparou-se com o sistema existente.Monitorou-se o tempo de atividade dos microsserviços individuais em condições normais e de teste de estresse.Mediu-se o tempo necessário para processar várias solicitações de transação dentro da arquitetura de microsserviços.2- Desenvolveu-se um sistema de prova de conceito usando tecnologias de processamento de dados em tempo real e mecanismos de captura de dados em alteração (Change Data Capture). Foram simulados altos volumes de transações para testar o desempenho e a integridade dos dados do sistema.Métricas:Mediu-se a latência do sistema acompanhando o tempo necessário para os dados serem processados e replicados em todo o sistema em tempo real.Acompanhou-se as inconsistências ou perdas de dados durante situações de alta carga.Mediu-se também o número máximo de transações que o sistema processou por segundo, mantendo uma latência e consistência aceitáveis.3- Foi prototipado um ambiente de Data Lake federado e implementou políticas de segurança para gerenciar o acesso a dados e proteger informações sensíveis. Foi testada ainda a capacidade do sistema de identificar e classificar dados sensíveis com precisão.Métricas:Mediu-se o tempo necessário para os usuários autorizados acessarem os dados exigidos no ambiente federado.Avaliou-se a precisão do sistema na identificação e classificação de dados sensíveis, medindo as taxas de falso positivo e falso negativo.4- Foi projetado e implementado um ambiente protótipo refletindo os princípios de Arquitetura de Confiança Zero (ZTA). Conduziram-se testes de penetração e simulações de ataque para avaliar a resiliência do sistema contra ameaças potenciais.Métricas:Mediu-se o tempo necessário para ataques simulados romperem as defesas do sistema, comparando com o ambiente pré-ZTA.Quantificou-se a redução nos vetores de ataque e vulnerabilidades expostas após a implementação do ZTA.Foi avaliado o impacto da implementação do ZTA no desempenho da aplicação, medindo mudanças na velocidade de processamento de transações e utilização de recursos.5- Foi criado um protótipo de um sistema em que processos críticos foram automatizados usando workflows que integraram modelos de ML e funções serverless, acionadas por eventos simulados de IoT.Métricas:Mediu-se a precisão das previsões orientadas por ML e seu impacto nos processos automatizados.Comparou-se o tempo necessário para executar processos específicos antes e depois da automação baseada em ML.Mediu-se o atraso entre o acionamento de um evento IoT e a resposta correspondente do sistema. INFORMAÇÃO COMPLEMENTAR: A gestão de dados tornou-se um foco crítico na indústria bancária moderna devido à grande quantidade de dados gerados em uma velocidade e variedade sem precedentes. Os sistemas tradicionais de gestão de dados têm dificuldade em lidar com esse volume crescente, resultando na necessidade de soluções mais flexíveis e escaláveis, como os data lakes. Esses sistemas são projetados para lidar com diversos tipos de dados, incluindo informações não estruturadas provenientes de mídias sociais, dispositivos IoT e várias aplicações empresariais. Essa evolução na gestão de dados é impulsionada pela necessidade de extrair insights acionáveis para áreas como gestão de riscos, detecção de fraudes e gestão de relacionamento com o cliente. Técnicas avançadas de análise e aprendizado de máquina dependem fortemente do acesso a grandes e variados conjuntos de dados, tornando os data lakes essenciais para o futuro das operações bancárias.No entanto, a adoção dessas tecnologias avançadas apresenta desafios significativos. A implementação de data lakes, microsserviços, arquiteturas de zero trust e modelos de aprendizado de máquina exige extensas etapas de prototipagem, testes e uma configuração cuidadosa para garantir confiabilidade, segurança e desempenho ideal. A interconectividade desses sistemas significa que fraquezas em qualquer um dos componentes podem afetar toda a infraestrutura. Por exemplo, garantir a consistência dos dados em tempo real entre várias fontes dentro de um data lake requer testes rigorosos para identificar e mitigar gargalos potenciais. Da mesma forma, a implementação de modelos de segurança de zero trust envolve testes detalhados de controles de acesso, processos de autenticação e protocolos de criptografia de dados para manter a segurança e integridade do ambiente bancário.O objetivo principal deste projeto foi integrar perfeitamente esses sistemas avançados — data lakes, microsserviços, arquiteturas de zero trust e modelos de aprendizado de máquina — em uma estrutura coesa, capaz de suportar as complexas necessidades de dados do setor bancário moderno. Os extensivos esforços de prototipagem foram fundamentais para alcançar essa integração, pois permitiram a identificação e resolução de possíveis problemas antes da implementação em grande escala. Por meio de testes rigorosos e desenvolvimento iterativo, o projeto garantiu que cada componente do sistema não apenas funcionasse de forma otimizada individualmente, mas também operasse harmoniosamente com os demais. Essa abordagem minuciosa foi crucial para entregar uma solução confiável, segura e escalável que atenda às demandas crescentes da indústria bancária.Gupta, P., & Ponnusamy, S. Beyond Banking: The Trailblazing Impact of Data Lakes on Financial Landscape. International Journal of Computer Applications, 975, 8887.Kang, H., Liu, G., Wang, Q., Meng, L., & Liu, J. (2023). Theory and application of zero trust security: A brief survey. Entropy, 25(12), 1595.Laigner, R., Zhou, Y., Salles, M. A. V., Liu, Y., & Kalinowski, M. (2021). Data management in microservices: State of the practice, challenges, and research directions. arXiv preprint arXiv:2103.00170.Leo, M., Sharma, S., & Maddulety, K. (2019). Machine learning in banking risk management: A literature review. Risks, 7(1), 29.Patil, K., Desai, B., Mehta, I., & Patil, A. (2023). A Contemporary Approach: Zero Trust Architecture for Cloud-Based Fintech Services. Innovative Computer Sciences Journal, 9(1).Vinoth, S., Vemula, H. L., Haralayya, B., Mamgain, P., Hasan, M. F., & Naved, M. (2022). Application of cloud computing in banking and e-commerce and related security threats. Materials Today: Proceedings, 51, 2172-2175. RESULTADO ECONÔMICO: Redução de custos operacionais e aumento da eficiência, diminuindo perdas por fraudes e vazamentos. A automação inteligente e a resposta ágil às demandas do mercado ampliaram a competitividade sem elevar proporcionalmente os custos. RESULTADO INOVAÇÃO: O desenvolvimento das tecnologias possibilitou a evolução considerável na resiliência, eficiência, e segurança das transações financeiras. A integração de Machine Learning otimizou a automação e a detecção de dados sensíveis, reforçando a segurança. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;NÚMERO: 2 ID ÚNICO: 109713 NOME: EXPERIMENTAÇÃO DE NOVAS TECNOLOGIAS PARA OTIMIZAÇÃO DE PAGAMENTOS E DIGITALIZAÇÃO DE SERVIÇOS BANCÁRIOS DESCRIÇÃO: As atividades de Pesquisa, Desenvolvimento e Inovação (P,D&I) focaram na criação de soluções tecnológicas a serem validadas científica e tecnologicamente. A pesquisa e experimentação foram direcionadas para evoluir as limitações existentes em termos de eficiência, consistência de dados e segurança, experimentando arquiteturas não utilizadas anteriormente. As propostas tecnológicas incluíram a implementação de uma arquitetura de componentes reativos e SPA, desenvolvimento de microserviços escaláveis, ferramentas de autenticação multifator e uma API com conteinerização e orquestração, visando estabelecer novas diretrizes para o desenvolvimento de soluções tecnológicas. O objetivo desta linha de pesquisa foi demonstrar a viabilidade técnica do desenvolvimento e integração de soluções avançadas para otimização de pagamentos e digitalização de serviços bancários.Especificamente, buscou-se melhorar a correção autônoma de repasses incorretos por meio da implementação de uma funcionalidade de autosserviço baseada em um aplicativo de página única (SPA) com arquitetura de componentes reativos. Além disso, foi desenvolvida uma arquitetura de microserviços com backend assíncrono e frontend com renderização no lado do cliente para capturar operações de clientes externos de forma eficiente.Outro foco do projeto foi a digitalização do segmento Private, onde se testou a hipótese de que uma ferramenta com autenticação multifator e governança adequada poderia proporcionar maior autonomia e controle aos clientes no cadastro de pessoas de confiança. Para isso, foi prototipada uma ferramenta com frontend modular e backend distribuído, utilizando comunicação assíncrona entre serviços. Adicionalmente, buscou-se estruturar uma plataforma para oferecer transferências internacionais como serviço, criando uma API específica com técnicas de conteinerização e orquestração de clusters de contêineres, visando garantir escalabilidade e eficiência na troca de mensagens.Os desafios enfrentados incluíram a sincronização de dados em tempo real na arquitetura de microserviços e a manutenção da consistência dos dados no SPA. Problemas de latência e inconsistência de dados também foram observados na integração com APIs de permissionamento. A implementação de autenticação multifator apresentou complexidades na integração segura e eficiente de diferentes formas de autenticação.O marco crítico do projeto surgiu como um ponto de decisão crucial, onde os desafios enfrentados na sincronização de dados em tempo real e na manutenção da consistência dos dados em arquiteturas complexas de microserviços e Single Page Applications (SPA) ameaçavam a viabilidade das soluções tecnológicas propostas. Este momento representou um teste significativo para a equipe de desenvolvimento, pois a eficiência, a consistência de dados e a segurança são fundamentais para a otimização de pagamentos e a digitalização de serviços bancários. A falta de otimização nessas áreas não só comprometeria a funcionalidade e a experiência do usuário final, mas também poderia resultar em falhas de segurança críticas, colocando em risco a integridade dos sistemas bancários digitais.Após os desenvolvimentos, espera-se alcançar interfaces mais responsivas e eficientes, maior segurança e autonomia para os usuários, e APIs que oferecessem menor latência e maior eficiência em operações de câmbio. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Autosserviço,  Componentes reativos,  Gerenciamento de estado,  Microserviços,  Escalonamento horizontal,  Sincronização de dados,  Governança,  Conteinerização,  Orquestração de contêineres,  Protocolo gRPC. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Foi testada a hipótese de que a implementação de uma funcionalidade de autosserviço baseada em um aplicativo de página única (SPA) utilizando uma arquitetura de componentes reativos poderia melhorar a correção autônoma de repasses incorretos por clientes. Para isso, foi prototipado um SPA utilizando uma arquitetura baseada em componentes reativos, implementados com uma linguagem de programação funcional. A reatividade foi avaliada quanto à sua capacidade de permitir atualizações dinâmicas na interface sem a necessidade de recarregamento da página, visando melhorar o tempo de resposta. Técnicas de gerenciamento de estado centralizado foram aplicadas para garantir a consistência dos dados na interface do usuário.Levou-se a experimento a ideia de que uma arquitetura de microserviços com backend assíncrono e frontend com renderização no lado do cliente poderia capturar operações de clientes externos boletadores de forma eficiente, independente da tecnologia utilizada pelo cliente. Para validar essa hipótese, foi criado um componente de software com uma arquitetura de microserviços, onde o backend foi estruturado usando um modelo de servidor assíncrono baseado em eventos, capaz de lidar com um grande número de conexões simultâneas. O frontend utilizou uma abordagem de renderização no lado do cliente com virtual DOM, visando uma interação mais fluida e responsiva. A integração com APIs de permissionamento foi realizada através de um protocolo de comunicação RESTful, consumindo dados de um banco de dados relacional com transações ACID para garantir a consistência dos dados. A computação em nuvem foi empregada para criar microserviços responsáveis pela gestão dos permissionamentos de câmbio, utilizando técnicas de escalonamento horizontal automático para lidar com picos de demanda. Durante os testes de carga, a eficiência e a escalabilidade da arquitetura de microserviços foram confirmadas, validando a hipótese. Contudo, desafios de sincronização de dados em tempo real foram identificados, exigindo a implementação de mecanismos de replicação de dados e otimizações nos protocolos de comunicação.Para a digitalização do segmento Private, foi testada a hipótese de que a implementação de uma ferramenta com autenticação multifator e governança adequada poderia proporcionar maior autonomia e controle aos clientes ao cadastrar pessoas de confiança para movimentar suas contas. Foi então realizada a prototipação de uma ferramenta com frontend desenvolvido utilizando um modelo de componentes modulares com shadow DOM para encapsulamento de estilos e backend estruturado com uma arquitetura de serviços distribuídos utilizando um barramento de mensagens para comunicação assíncrona entre serviços. A autenticação multifator foi implementada utilizando um protocolo de segurança robusto, combinando senha, token e biometria. A autorização foi gerenciada através de políticas de acesso definidas em um servidor de identidade que utiliza tokens JWT (JSON Web Tokens) para garantir a segurança das transações.Para estruturar uma plataforma para oferecer transferências internacionais como serviço, foi testada a hipótese de que a criação de uma API específica, utilizando técnicas de conteinerização e orquestração com clusters de contêineres, poderia garantir escalabilidade e isolamento de processos, proporcionando menor latência e maior eficiência na troca de mensagens. Para testar essa hipótese, foi criada uma API utilizando técnicas de conteinerização e orquestração com clusters de contêineres. A comunicação entre os serviços foi gerenciada por um protocolo de alta performance baseado em gRPC, visando menor latência e maior eficiência na troca de mensagens. A integração com serviços de gerenciamento de eventos, como sistemas de mensageria distribuída baseados em logs, foi fundamental para criar novos recursos para os serviços de negociação de câmbio, possibilitando a detecção e resposta a eventos em tempo real. DESAFIO TECNOLÓGICO: A arquitetura de microserviços apresentou problemas significativos na sincronização de dados em tempo real. A natureza distribuída dos microserviços resultou em inconsistências temporárias de dados, especialmente durante picos de demanda. A latência na propagação de mudanças entre diferentes instâncias de serviços e a dificuldade em garantir a consistência eventual dos dados foram questões críticas. Para mitigar esses problemas, levantou-se a hipótese de implementar um sistema de replicação de dados em tempo real utilizando Apache Kafka para mensageria distribuída. A integração desse sistema com a arquitetura existente e a garantia de baixa latência e alta consistência em um ambiente distribuído permaneceram desafios significativos, cuja eficácia ainda precisava ser validada em cenários de produção.O gerenciamento de estado centralizado no Single Page Application (SPA) enfrentou dificuldades na manutenção da consistência dos dados entre diferentes componentes reativos. A complexidade aumentou com a necessidade de gerenciar atualizações concorrentes e interações complexas entre componentes, resultando em estados inconsistentes e comportamentos inesperados na interface do usuário. A sincronização de estado entre o frontend e o backend também introduziu desafios adicionais, especialmente em condições de rede instáveis. Para resolver esses problemas, foi proposto o uso da biblioteca Redux para gerenciamento de estado, combinada com técnicas de controle de concorrência como optimistic updates e versionamento de estado. No entanto, a implementação dessas técnicas em um sistema altamente interativo e dinâmico poderia introduzir novos overheads de desempenho e complexidades, cuja eficácia ainda precisava ser rigorosamente testada.A integração com APIs de permissionamento, utilizando um protocolo de comunicação RESTful, encontrou problemas de latência e inconsistência de dados devido à complexidade das transações ACID no banco de dados. A latência nas operações de leitura e escrita e a dificuldade em manter a consistência de dados em um ambiente distribuído foram questões críticas. A necessidade de garantir a atomicidade, consistência, isolamento e durabilidade das transações complicou ainda mais a integração. Para melhorar a consistência e a latência das operações, foi sugerida a migração para CockroachDB, um banco de dados distribuído que oferece suporte a transações distribuídas. No entanto, a transição para esse novo sistema de banco de dados era uma tarefa que envolvia riscos significativos de interrupção de serviço e problemas de compatibilidade, além de exigir uma reengenharia substancial dos sistemas existentes.A implementação de autenticação multifator, combinando senha, token e biometria, apresentou desafios na integração dessas diferentes formas de autenticação de maneira segura e eficiente, sem comprometer a experiência do usuário. A coordenação entre diferentes métodos de autenticação e a garantia de que cada método fosse igualmente seguro e resistente a ataques foi uma tarefa complexa. A gestão de tokens e a sincronização de dados biométricos em tempo real também introduziram desafios de segurança e privacidade. Para simplificar a implementação, foi levantada a hipótese de utilizar o framework Auth0, que em tese, ofereceria suporte integrado para múltiplas formas de autenticação e políticas de segurança.A comunicação entre os serviços conteinerizados, gerenciada por gRPC, enfrentou desafios de latência e eficiência, especialmente em ambientes de alta concorrência e complexidade. A sobrecarga de serialização e deserialização de dados, juntamente com a gestão de conexões persistentes e a otimização do tráfego de rede, foram questões críticas. Para reduzir a latência e melhorar a eficiência, foi considerada a hipótese de introduzir técnicas de compressão de dados e otimização de protocolos de comunicação, juntamente com o uso de redes definidas por software (SDN) para gerenciar o tráfego de rede de maneira mais eficiente. METODOLOGIA: 1- Experimentos com Aplicações de Página Única (SPAs) baseadas em uma arquitetura de componentes reativos para a correção autônoma de transferências incorretas:Métricas:Medir o tempo que a interface leva para atualizar após uma interação do usuário, como clicar em um botão ou digitar um texto (responsividade).Avaliar a quantidade de dados transferidos entre o cliente e o servidor durante as atualizações da interface (eficiência).Acompanhar o número de correções autônomas bem-sucedidas feitas pelos clientes usando as SPAs em comparação com o método anterior (taxa de sucesso).2- Experimentos com uma arquitetura de microsserviços com backend assíncrono e renderização no cliente para capturar operações de clientes externos de forma eficiente:Métricas:Medir o tempo que uma solicitação do frontend leva para chegar ao backend e receber uma resposta (latência).Calcular o número de operações de clientes externos que o sistema pode processar em um determinado período (throughput).Monitorar a utilização de recursos do sistema (CPU, memória, rede) à medida que o número de solicitações aumenta (escalabilidade).3- Protótipos de uma ferramenta com autenticação multifator e governança adequada para fornecer maior autonomia e controle aos clientes no cadastro de pessoas de confiança para operar suas contas no Segmento Private:Métricas:Medir o tempo que um usuário leva para se autenticar com sucesso usando o sistema de autenticação multifator (tempo de autenticação).Acompanhar o número de tentativas de autenticação falhas e analisar os motivos do fracasso (segurança).Realizar pesquisas ou entrevistas com os usuários para coletar feedback sobre o nível de controle e autonomia proporcionado pela ferramenta (feedback do usuário).4- Protótipos de uma API para Transferências Internacionais usando técnicas de conteinerização e orquestração de clusters de contêineres:Métricas:Medir o tempo que uma solicitação leva para ser processada e uma resposta ser enviada pela API (latência).Calcular o número de solicitações que a API pode processar por segundo (throughput).Monitorar a utilização de recursos do ambiente conteinerizado (CPU, memória) à medida que o número de solicitações aumenta (escalabilidade). INFORMAÇÃO COMPLEMENTAR: À medida que o setor bancário enfrenta transformações digitais impulsionadas por mudanças nas necessidades dos clientes e a crescente tendência do open banking, as arquiteturas monolíticas tradicionais tornam-se cada vez mais inadequadas. Consequentemente, o setor está em um ponto crucial onde a adoção de microsserviços surge como uma necessidade estratégica para manter a competitividade e a eficiência operacional. No entanto, a transição exige uma abordagem cautelosa e iterativa, marcada por prototipagem e experimentações minuciosas.No mundo acelerado de hoje, agilidade e capacidade de resposta são fundamentais. O cenário do setor bancário está mudando rapidamente, com os clientes esperando experiências digitais perfeitas e novos jogadores fintech desafiando o status quo. Os microsserviços oferecem uma vantagem crucial ao permitir que os bancos desenvolvam e lancem novos recursos com uma velocidade sem precedentes. Ao contrário dos sistemas monolíticos, que muitas vezes paralisam a inovação devido à sua rigidez, os microsserviços fragmentam as aplicações em serviços independentes e gerenciáveis, permitindo ajustes ágeis em resposta às dinâmicas do mercado e às expectativas dos clientes.Além disso, os sistemas legados em muitos bancos evoluíram para se tornarem monstros monolíticos ao longo do tempo, criando uma estrutura labiríntica que é tanto dispendiosa quanto difícil de manter. Os microsserviços aliviam essa complexidade ao quebrar esses sistemas volumosos em unidades menores e compreensíveis. Esta abordagem modular não só melhora a manutenção, mas também facilita atualizações sem comprometer a estabilidade de todo o sistema.A capacidade de escalar serviços de forma independente é outra razão convincente para que os bancos adotem microsserviços. As arquiteturas tradicionais exigem a escalabilidade do sistema inteiro para atender à crescente demanda, resultando em utilização ineficiente de recursos e aumento de custos. Em contraste, os microsserviços permitem a escalabilidade direcionada de componentes específicos. Por exemplo, um banco pode escalar o microsserviço que gerencia transações móveis durante os horários de pico independentemente dos outros serviços, garantindo eficácia em custos e desempenho sustentado sob cargas pesadas.Engajar-se na prototipagem permite que os bancos experimentem diferentes estratégias arquitetônicas em um ambiente controlado antes de se comprometerem com mudanças amplamente disseminadas. Esta abordagem ajuda a identificar possíveis obstáculos, validar premissas e evitar erros custosos. Obter insights iniciais através de protótipos, portanto, reduz os riscos da jornada de migração como um todo.A mudança para uma arquitetura de microsserviços naturalmente introduz complexidades técnicas, como manter a consistência de dados em serviços distribuídos, gerenciar a comunicação entre serviços e orquestrar uma rede distribuída. Experimentando com vários mecanismos de replicação de dados e diferentes protocolos de comunicação entre serviços (como REST ou gRPC), os bancos podem encontrar o equilíbrio ideal entre desempenho e complexidade operacional. Além disso, avaliar diferentes ferramentas de descoberta e orquestração de serviços através da prototipagem permite que os bancos gerenciem efetivamente a natureza distribuída dos microsserviços.Auer, F., Lenarduzzi, V., Felderer, M., & Taibi, D. (2021). From monolithic systems to microservices: An assessment framework. Information and Software Technology, 137, 106600.Ayas, M. H., Leitner, P., & Hebig, R. (2023). An empirical study of the systemic and technical migration towards microservices. Empirical Software Engineering, 28(85).Bhole, K., Nareddy, R., & Laughridge, K. (n.d.). Opening banking through architecture re-engineering: A microservices-based roadmap. Deloitte United States.Wang, Y., Kadiyala, H., & Rubin, J. (2021). Promises and challenges of microservices: an exploratory study. Empirical Software Engineering, 26(4), 63. RESULTADO ECONÔMICO: Redução significativa do tempo de resposta para correções autônomas de transações pelos clientes. Isso melhorou a eficiência operacional, reduzindo custos com suporte e aumentando a satisfação do cliente. RESULTADO INOVAÇÃO: A inovação resultou em uma plataforma tecnológica segura e eficiente, melhorando a agilidade e escalabilidade dos serviços financeiros com autenticação robusta e técnicas de conteinerização, otimizando a gestão de transações. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;NÚMERO: 3 ID ÚNICO: 109714 NOME: DESENVOLVIMENTO E EVOLUÇÃO DE PLATAFORMA DE INVESTIMENTO COM MELHORIAS PARA JORNADA DO CLIENTE DESCRIÇÃO: O objetivo desta linha de pesquisa é demonstrar a viabilidade técnica da integração de jornadas e desenho de plataforma de investimentos de forma a melhorar a experiência do cliente. Para isso, foi organizado um conjunto de atividades experimentais buscando proposições a serem validadas em protótipos.Soluções de investimento envolvem consolidação de diferentes plataformas de produtos associados, de processos de autorização, de integração de esteiras e soluções considerando diferentes segmentos de negócios.Na solução atual, a diversidade e variedade de funcionalidades tornavam complexa e pouco óbvia uma solução única consolidando as funcionalidades envolvidas. Entendeu-se ser necessário construir nova solução sistêmica. Assim, foram propostos fluxos de comunicação entre as jornadas envolvidas de forma a garantir simplificação de jornadas fundamentais para a área de investimentos e maior sofisticação na entrega de ferramentas de atuação junto ao cliente. Como resultado, espera-se obter um novo produto digital reduzindo tempo alocado em atividades que requerem diferentes acessos além de uma plataforma mais amigável ao cliente.Para alcançar o objetivo, foram consideradas tecnologias e recursos em nuvem buscando composição para desenvolvimento e modernização da plataforma de investimentos. A partir da composição de protótipos, foi levado a teste composição modular buscando comprovação da inclusão de novas funcionalidades acopladas de maneira gradativa, conforme evolução, e com uma revisão de processos e origem de dados.Como marco crítico do desenvolvimento desta linha de pesquisa, tem-se um ponto de verificação quanto à validação da integração e comunicação entre as jornadas de investimentos – que envolve consolidar diferentes fluxos de produtos associados, processos de autorização e integração de diferentes esteiras. A complexidade e diversidade das funcionalidades colocam desafios quanto à sua integração,  falhas nesta etapa podem resultar em interrupções no serviço, problemas de comunicação e uma experiência de usuário insatisfatória, comprometendo a confiança no novo sistema.Outro marco crítico estava relacionado à avaliação da viabilidade técnica e de segurança para permitir o acesso à plataforma de investimentos através da internet, sem a necessidade de um computador conectado à rede do Banco. A não validação do processo de homologação desta fase se refletiria em falhas de segurança ou problemas de desempenho comprometendo a confiança dos clientes na nova solução – algo ainda mais importante considerando-se a dimensão investimentos. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Tecnologias em nuvem,  Tecnologias on-premise,  Branches,  Experiência do usuário. NATUREZA: Processo ELEMENTO TECNOLÓGICO: O objetivo desta linha de pesquisa é demonstrar a viabilidade técnica da integração de jornadas e desenho de plataforma de investimentos de forma a melhorar a experiência do cliente. O desenvolvimento das atividades experimentais organizadas buscando elementos para esta integração possibilitou um conjunto de aprendizados.Em termos de processo de desenvolvimento, os estudos permitiram comprovar tecnicamente o desenvolvimento do código produtivo baseado em um processo de tagueamento, permitindo que a criação de branches seja realizada a partir da última tag efetuada. Sendo a branch uma ramificação do código-fonte principal em um sistema de controle de versão, seria possível incluir novas funcionalidades, correções de bugs ou experimentos de forma isolada, sem afetar o código principal. Houve também comprovação técnica sobre a necessidade de criação de uma branch release separada, para garantir que tanto o ambiente quanto o código de homologação estivessem isolados do ambiente produtivo.Em relação à segurança para a externalização de forma a ser utilizada diretamente pelos clientes, tanto no desenvolvimento quanto nas informações trafegadas pela rede, foram avaliadas diferentes proposições para PoCs buscando entendimentos para possibilitar o acesso logado através da internet, sem a necessidade de um computador conectado à rede do Banco. Para garantir atendimento de requisitos de segurança, houve a compreensão da elaboração e execução de testes de intrusão para validação da utilização de autenticação multifatorial.Durante a execução de PoCs, a partir da realização de simulações, foi possível entender processo para geração de propostas, de painéis de gestão comercial e de operações executadas, utilizando bancos de dados relacionais otimizados como Aurora e Open Search. Dessa forma, houve comprovação técnica de processo de integração das jornadas necessárias para tais sistemas. Essas soluções permitiram a criação de uma central de validações que verifica as necessidades para executar operações reduzindo o tempo total de execução na visão do cliente.Além desses aprendizados, houve comprovação técnica da adoção de arquiteturas de microsserviços para facilitar a modularização das funcionalidades e a integração entre diferentes sistemas. Isso permitiria uma escalabilidade mais eficiente e a possibilidade de atualizar ou adicionar novos serviços sem impactar o sistema como um todo. Apesar desta adoção poder ser considerada trivial, sua adoção implicava desenvolvimentos complexos considerando diferentes jornadas. Isso porque cada microsserviço deveria ser desenvolvido, implantado e escalado de forma independente, utilizando a melhor tecnologia para cada caso específico, o que aumenta a flexibilidade e a resiliência do sistema – mas também aumenta a complexidade do desenvolvimento. A comunicação entre os microsserviços deve ser feita através de APIs bem definidas, permitindo uma integração mais robusta e segura. Isso coloca a necessidade de experimentações buscando comprovação técnica destas APIs. Houve, ainda, aprendizados sobre a aplicação de contêineres e orquestração com Kubernetes a partir de comprovações técnicas para processo de gestão de aplicações em ambientes de nuvem.A partir das comprovações técnicas alcançadas, além da oferta de produtos de investimento que colocam o Banco em uma melhor situação competitiva no mercado, foi possível melhorar significativamente a experiência do cliente. DESAFIO TECNOLÓGICO: O desenvolvimento de uma nova plataforma tecnológica para assessoria de investimentos enfrentou desafios significativos, especialmente no que diz respeito à integração sistêmica dentro de uma instituição bancário-financeira. Tratou-se de um processo especialmente complexo ao envolver riscos relacionados a informações sensíveis e a processos que podem afetar a disponibilidade de serviços ao cliente.Assim, foram consideradas um conjunto de hipóteses buscando melhor estratégia para composição da plataforma. Experimentos se voltaram a entender como criar uma solução baseada em uma arquitetura desacoplada, permitindo que cada aplicação, seja ela diretamente ou indiretamente envolvida, possa evoluir de forma independente. Entre as hipóteses em estudo, destacou-se a experimentação com arquitetura orientada a eventos, utilizando tecnologias como Kafka e soluções nativas da AWS, como SNS e SQS. O objetivo era comprovar ser possível alcançar desacoplamento e integração eficaz entre sistemas on-premises e na nuvem. Foi experimentado, também a aplicação de API Gateway para integrar soluções hospedadas em diferentes contas na nuvem, garantindo uma comunicação segura e eficiente.No que diz respeito ao armazenamento de dados, partiu-se da premissa que a escolha do banco de dados implicaria no sucesso ou não da solução de integração de plataformas em diferentes maturidades tecnológicas. Do ponto de vista de desempenho, esperava-se que soluções NoSQL fossem viáveis,  experimentos buscaram comprovar que esta tecnologia seria eficiente para lidar com grandes volumes de dados não estruturados. Para validar essa hipótese, foram realizadas provas de conceito e comparações de resultados utilizando configurações de busca do novo sistema. A utilização de Open Search, como hipóteses de solução, foi avaliada para determinar se poderia oferecer o desempenho necessário para personalizar consultas financeiras através de diversos filtros dinâmicos e personalizados. A capacidade de cruzar múltiplas fontes de dados e realizar cálculos complexos em tempo real foi um critério nessa avaliação.Para lidar com a disponibilização e uso de informações de alto consumo e baixa volatilidade, a proposta foi experimentar a implantação de um sistema de cache distribuído, com atualização das informações através de eventos. Buscava-se comprovação técnica de um acesso mais rápido e eficiente aos dados, reduzindo a carga sobre os sistemas de backend. Experimentos consideraram a arquitetura seguindo o conceito de micro frontends, utilizando o framework Angular e BFFs (backend for frontend). A premissa considerada era que essa abordagem poderia modularizar a interface do usuário, permitindo que diferentes equipes trabalhem em partes distintas da aplicação de forma independente, acelerando o desenvolvimento e a implementação de novas funcionalidades.O processo de gestão de branches também apresentou desafios. Isso porque tanto a arquitetura de backend quanto a de frontend deveriam permitir autonomia e desenvolvimento colaborativo, sem comprometer a organização do código. A solução de branching deveria ser flexível o suficiente para suportar diferentes fluxos de trabalho, mas também estruturada para garantir a integridade e a qualidade do código. Para isso, foram levadas a teste ferramentas de controle de versão como GitLab e GitHub para avaliação da melhor abordagem para gestão de branches. Além disso, diversos testes e provas de conceito foram realizados para avaliar a viabilidade da migração dos repositórios da plataforma – entre GitLab e GitHub. A hipótese levada a teste era que essa migração não apenas padronizaria a ferramenta, mas também viabilizaria métodos de deployment como Canary e Blue-Green. Esses métodos permitem a implementação gradual de novas versões, minimizando riscos e facilitando a detecção e correção de problemas. METODOLOGIA: Para superar os desafios técnicos e comprovar viabilidade técnica do desenvolvimento almejado, esta linha de pesquisa organizou a seguinte metodologia de P&D:1- Estudos e experimentos com tecnologias baseadas em nuvem para o desenvolvimento e modernização da plataforma. Protótipos com design modular foram testados para avaliar a viabilidade de implementação gradual de novas funcionalidades.Métricas:- Eficiência na integração de novos módulos- Flexibilidade da arquitetura para expansão- Latência do sistema2- Desenvolvimento de protótipos para garantir acesso externo seguro à plataforma via internet, incluindo a validação de autenticação multifatorial e testes de penetração para proteger informações sensíveis.Métricas:- Taxa de sucesso de autenticação- Tempo médio de resposta- Número de vulnerabilidades identificadas e corrigidas- Número de tentativas de acesso não autorizado ou falhas de segurança- Tempo de uptime e confiabilidade da plataforma em cenários de acesso externo3- Criação e desenvolvimento de protótipos para explorar a arquitetura orientada a eventos com tecnologias como Kafka e AWS SNS/SQS, visando alcançar o desacoplamento e a integração confiável e escalável entre sistemas locais e baseados em nuvem.Métricas:- Latência de processamento de eventos- Taxa de sucesso de entrega de mensagens- Número máximo de eventos processados por segundo sem degradação de desempenho- Tempo de uptime e confiabilidade dos serviços de eventos- Tempo médio para a aplicação reagir e processar eventos recebidos4- Experimentos para avaliar o Open Search para o manejo de consultas financeiras personalizadas com filtros dinâmicos, explorando a capacidade do sistema de integrar múltiplas fontes de dados e realizar cálculos complexos em tempo real.Métricas:- Tempo médio de resposta- Tempo médio para realizar cálculos complexos e fornecer respostas em tempo real- Eficiência (latência) na integração e cruzamento de múltiplas fontes de dados5- Desenvolvimento e avaliação de protótipos de sistemas de cache distribuído para otimizar o acesso a dados de alta consumo e baixa volatilidade atualizados por meio de eventos. Foram avaliadas diferentes soluções de cache e estratégias para garantir a recuperação eficiente de dados e reduzir a carga nos sistemas de backend.Métricas:- Tempo de acesso ao cache- Taxa de acerto do cache- Impacto do sistema de cache na redução da carga e latência dos sistemas de backend- Tempo médio para atualizar o cache em resposta a eventos e mudanças nos dados6- Desenvolvimento e avaliação de protótipos para uma arquitetura de micro frontends, usando Angular e Backend for Frontends (BFFs), com o objetivo de modularizar a interface do usuário e acelerar o desenvolvimento. Foram realizados testes para avaliar diferentes componentes de UI e padrões de comunicação entre micro frontends e BFFs.Métricas:- Tempo médio para criar e integrar novos componentes de UI utilizando a arquitetura de micro frontends- Latência das interações entre micro frontends e BFFs INFORMAÇÃO COMPLEMENTAR: Estruturas bancárias e financeiras, construídas a partir de sistemas monolíticos inflexíveis e desatualizados, podem se beneficiar significativamente da migração para uma arquitetura de microsserviços. Esse estilo arquitetônico, que estrutura um aplicativo como uma coleção de serviços fracamente acoplados, pode introduzir a agilidade e escalabilidade necessárias para a oferta competitiva de produtos e serviços aos clientes. Isso porque ao decompor sistemas complexos em serviços menores e independentes, os bancos podem acelerar o desenvolvimento e a implantação de novos recursos, respondendo mais rapidamente às expectativas evoluídas dos clientes e às pressões competitivas das empresas de fintech. Essa mudança também permite o uso eficiente de recursos, particularmente crucial durante os períodos de pico de transações em tempo real experimentados.No entanto, a transição para microsserviços não é isenta de complexidades. Assim, a experimentação é fundamental para lidar, de forma eficaz, com esses desafios. Por meio de protótipos, os bancos podem avaliar a viabilidade da transição, considerando tecnologias como Kubernetes e Docker descritas nas fontes e determinando seu alinhamento com a infraestrutura existente e protocolos de segurança. Além disso, a experimentação é essencial para definir estratégias de migração de dados, garantindo o desempenho de bancos de dados e avaliando a adequação de soluções NoSQL para gerenciar a grande quantidade de dados sensíveis e confidenciais tratados por instituições financeiras.Por fim, essa abordagem iterativa permite um processo de gerenciamento de mudança mais seguro permitindo que os clientes se adaptem com fluxos de comunicação mais amigáveis.Referências bibliográficas:- Aydemir, F., & Basçiftçi, F. (2022). Building a performance efficient core banking system based on the microservices architecture. Journal of Grid Computing, 20(4), 37.- Maestro, A., & Surantha, N. (2023). Scalability Evaluation of Microservices Architecture for Banking Systems in Public Cloud. In International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (pp. 103-115). Cham: Springer Nature Switzerland.- Mazzara, M., Dragoni, N., Bucchiarone, A., Giaretta, A., Larsen, S. T., & Dustdar, S. (2018). Microservices: Migration of a mission critical system. IEEE Transactions on Services Computing, 14(5), 1464-1477. RESULTADO ECONÔMICO: Estima-se aumento de receita advindo do maior uso e maior contratação de produtos de investimento a partir da modernização e melhoria dos processos atuais. RESULTADO INOVAÇÃO: Simplificação de rotinas a partir de consolidação sistêmica de processos,  modernização dos sistemas resultando em menor taxa de erros e problemas. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;NÚMERO: 4 ID ÚNICO: 109715 NOME: DESENVOLVIMENTO DE SOLUÇÕES DE TESTES PARA MONITORAMENTO ATIVO COMO ESTRATÉGIA PARA MINIMIZAR A INDISPONIBILIDADE DE SISTEMAS DESCRIÇÃO: Apesar da evolução tecnológica alcançada por estruturas de software, a indisponibilidade destes sistemas ainda é um problema inclusive em corporações com recursos e especialização técnica diferenciados. Em um ambiente bancário, este problema é crítico uma vez que envolve transações que devem ser processadas em tempo real.Há uma série de fatores que podem resultar nesta indisponibilidade. Sistemas bancários de software integram uma variedade de serviços e funcionalidades configurando estruturas complexas que aumentam os riscos de erros e falhas,  esse potencial é aumentado pelo fato de ainda haver parte dos sistemas em plataforma legada mainframe – que são únicos e rígidos. A necessidade de atualização, de manutenção e de escalabilidade, ainda que inerentes a sistemas de software, aumentam os riscos de indisponibilidade quando considerada a estrutura complexa de grandes organizações.Resolver este problema passa por um conjunto de ações. Esta linha de pesquisa se dedica à concepção e desenvolvimento relacionados a testes de forma a criar processo para detecção e falhas nas jornadas de clientes. Foi possível entender a aplicação de conceitos de monitoração sintética à automação de testes,  além de se configurar como uma abordagem ativa, foi possível alcançar um desenvolvimento compatível com diferentes programas e clientes (a partir de APIs) e integrável a outros testes existentes na corporação. Atividades experimentais também se voltaram a entender como lidar com ambientes em diferentes maturidades tecnológicas e garantir uma automação de testes que permitisse um monitoramento único, sem a necessidade de ser duplicado. Foram organizados estudos que considerassem uma nova orquestração de execuções com entendimentos para construção uma mesma pipeline com retornos apartados entre si permitindo o monitoramento necessário.Os desenvolvimentos dos processos de automação de testes lidaram com a complexidade existente associada a cenários, tecnologias e ambientes distintos. Em um contexto bancário, com milhões de usuários, funcionalidades e transações, havia riscos de não ser possível uma solução que abrangesse a diversidade e criticidade de situações. Ainda que exista no mercado uma gama de ferramentas para testes e monitoração ativa, não havia soluções que apresentassem o desempenho necessário a esse contexto ainda que ajustadas e personalizadas. Entre as tecnologias consideradas, destacam-se Dynatrace, New Relic Synthetics, Pingdom e Catchpoint.Estudos considerando o estado da arte científico indicaram que a aplicação de monitoração sintética em automação de testes para detectar falhas em tempo real é incipiente e não tem sido atualizada. Estudos existentes geralmente se concentram em ambientes de desenvolvimento homogêneos ou modernos, com lacunas considerando soluções que abordem a integração de testes automatizados em infraestruturas mistas (com coexistência entre sistemas legados e novos serviços em nuvem ou arquiteturas distribuídas). Também há poucas referências sobre como criar pipeline que integre diferentes tipos de testes (unitários, integrados, funcionais e não funcionais).Como resultado, a execução desta linha de pesquisa trouxe, como novidade, uma abordagem ativa e integrada de testes para monitoração que minimizaram situações de indisponibilidade.Caso o desenvolvimento não fosse compatível com os outros testes relacionados a desenvolvimentos específicos (vale lembrar que testes unitários, integrados, funcionais e não funcionais são aplicados a cada desenvolvimento feito na empresa), a automação não seria viável. Caso não fosse possível uma solução única para testes em ambientes tecnológicos distintos, havia riscos de aumentar as situações de indisponibilidade. Tais fatos impediriam a conclusão da linha de pesquisa configurando-se, portanto, como marco crítico. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Monitoração sintética,  modularização,  testes automatizados,  automação. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Na solução existente, a execução de testes seguia estratégia de monitoração passiva com realização de testes para monitoramento a partir da experiência real do usuário em cenários autênticos. Considerando sistemas bancários, era possível ter uma análise detalhada de cada transação e sua interação com o cliente. No entanto, isso não se refletia em sistemas mais disponíveis. Como esse tipo de monitoramento depende da análise de eventos que já ocorreram, a detecção de problemas acontece de forma tardia,  além disso, sua cobertura não é abrangente a ponto de considerar a variedade de componentes interconectados dos sistemas bancários. Tais fatores indicaram que a abordagem passiva para testes de monitoramento era insuficiente para evitar a indisponibilidade dos sistemas. Com isso, era necessário entender novas estratégias para automação de testes com estudos voltados a conceitos de monitoração ativa.As principais abordagens desta monitoração – discutidas por Shahin, Babar e Zhu (2017) – forneceram insights em termos de contribuições teóricas, mas careciam de casos e aplicações práticas. Houve comprovação da aplicação de conceitos de monitoração sintética (que monitora a partir de simulações) para a automação de testes almejada. Diferente de outras abordagens, os testes podem ser totalmente automatizados proporcionando uma monitoração contínua do sistema de forma a identificar problemas assim que eles ocorrem,  a monitoração sintética também conseguiu abranger uma ampla gama de funcionalidades e tecnologias que coexistem e precisam ser testados de maneira integrada.Para isso, era necessário entender como emular as interações do usuário e dos fluxos de transação considerando a complexidade das inúmeras variáveis, cenários, produtos e serviços. Levou-se a teste a premissa de que a abordagem da linguagem de programação a ser utilizada na plataforma poderia simplificar o cenário alvo. Foi possível comprovar a aplicação de DSLs (domain-specific language) para desenvolvimento destes cenários. Uma Domain-Specific Language (DSL) é uma linguagem de programação utilizada para um domínio particular de aplicação. Nesse caso, sua aplicação permitiu abstrair a complexidade citada ao mesmo tempo em que permitiu que os especialistas do domínio, que possuem conhecimento detalhado das operações bancárias, definissem cenários de teste sem necessidade de compreender todas as nuances técnicas de cada sistema. Diferente de outras abordagens, como Model-Based Testing (MBT), a DLS permitiu a flexibilidade para permitir a personalização dos testes para cenários específicos sem perder a generalidade necessária para ser aplicada a diferentes sistemas.Com a abordagem de automação de testes definida, estudos foram direcionados para integrar à esteira de desenvolvimento os diversos tipos de testes — unitários, integrados, funcionais e não funcionais — necessários no design, desenvolvimento e implantação dos produtos de software da empresa. A problemática de integrar diversos tipos de testes em uma esteira de desenvolvimento contínuo é amplamente abordada na literatura especializada em engenharia de software e práticas DevOps – Erich, Amrit e Daneva (2017),  Rahman, Helms, Williams, Parnin (2015),  Lwakatare, Kuvaja & Oivo (2016). Ainda assim, há lacunas no estado da arte sobre abordagens e ferramentas que facilitem a integração e execução automatizada desses diferentes tipos de testes em um fluxo contínuo,  consolidá-los em uma esteira maior que ofereça uma visão abrangente e coerente de todas as atividades de teste é dificultada pela falta de frameworks e metodologias que abordem a heterogeneidade dos ambientes de desenvolvimento. Dentre as hipóteses consideradas, houve comprovação de nova composição de pipeline, com abordagem modularizada, de forma a criar processo para o provisionamento de ambientes e testes dinâmicos e apartados. DESAFIO TECNOLÓGICO: Como hipótese de solução, buscou-se entendimentos em monitoração ativa de forma a ser possível testar uma composição para a automação de testes que diminuísse situações de indisponibilidades. Kim, G., Behr, K., Spafford, G. (2013) ofereceram insights sobre a importância de combinar automação de testes com monitoramento ativo como forma de resolver problemas de disponibilidade de sistemas. Adkins, H., Beyer, B., Blankinship, P., Lewandowski, P., Oprea, A., Stubblefield, A. (2020) reforçavam que o monitoramento passivo não seria suficiente para garantir a disponibilidade contínua dos sistemas sugerindo a adoção de práticas de monitoramento ativo e automação.Considerando o monitoramento ativo, entre as principais técnicas estava a análise de logs em tempo real,  porém, havia limites quanto a analisar grandes volumes de dados relacionados à alta demanda por capacidade de processamento e armazenamento. Outra hipótese considerada era a monitoração baseada em transações a partir da criação de scripts automatizados que simulam diferentes transações típicas do usuário ou do sistema. Esses scripts seriam executados periodicamente para verificar se as transações estão sendo processadas conforme o esperado, detectando falhas ou problemas de desempenho. A literatura especializada aponta limitações da abordagem, em especial a dificuldade de cobrir todos os caminhos de uso ou funcionalidades, deixando áreas críticas sem monitoramento efetivo.A criação e desenvolvimento de estrutura de monitoração ativa enfrentou barreiras associadas à insuficiência do conhecimento em fornecer soluções que equilibrem precisão, cobertura e desempenho dos testes, sem sobrecarregar os recursos do sistema – conforme Shahin, Babar e Zhu (2017). Era incerto ser possível superar a contradição técnica de minimizar a disponibilidade do sistema a partir de uma ferramenta que pode reforçar tais falhas.Outra barreira diz respeito à simulação de sistemas complexos e dinâmicos que possa abranger todas as variáveis, cenários, produtos e serviços envolvidos em sistemas bancários. Tal barreira é reforçada por existir uma lacuna no conhecimento técnico-científico sobre como desenvolver e manter testes sintéticos altamente personalizados e flexíveis que possam automaticamente ajustar-se às mudanças nos sistemas que monitoram.Delineamentos experimentais se voltaram a entender a viabilidade da monitoração sintética lidando com as barreiras da emulação do sistema. Como hipótese de solução, testou-se a aplicação de DSLs (Domain-Specific Languages) buscando comprovação de flexibilidade nos testes. A premissa considerada era que criar linguagens específicas de domínio que permitam aos desenvolvedores especificarem comportamentos de testes de forma mais natural e intuitiva poderia simplificar a criação e manutenção de testes sintéticos R. Gupta, S. Kranz, N. Regnat, B. Rumpe and A. Wortmann. (2021) fornecem um mapeamento sistemático do uso de DSLs em testes automatizados de forma a simplificar a definição de testes sintéticos especialmente considerando sistemas complexos.A integração almejada de todas as esteiras de teste em um fluxo contínuo de monitoramento lidou com incertezas sobre como harmonizar ferramentas e processos de teste heterogêneos em uma esteira de desenvolvimento integrada,  a literatura especializada vem abordando a falta de frameworks ou metodologias que possam abordar tal questão.Estudos mostravam a necessidade de mais pesquisas sobre a interoperabilidade entre as diferentes ferramentas de monitoração ativa (Forsgren, N.,  Humble, J.,  Kim, G. (2018)). Experimentos apontavam para dificuldades na configuração e manutenção das integrações entre as ferramentas de monitoração e pipelines de outros testes. Estudos considerando o estado da arte trataram da adaptação de ferramentas de monitoração para diferentes ambientes e necessidades específicas de negócios, mas indicaram a necessidade de suporte melhor para análise preditiva e detecção de anomalias. METODOLOGIA: Para superar os desafios relacionados a novos processos de automação de testes visando soluções para cenários de indisponibilidade, foram organizadas as seguintes atividades:1- Estudos teórico e prático sobre metodologias de monitoração ativa buscando comprovações técnicas para configuração de testes- Percentual de metodologias aplicadas com sucesso em testes práticos.2- Estudos sobre monitoração sintética para entendimento de aplicações buscando referências para composição de solução- Quantidade de aplicações de monitoração sintética analisadas.- Número de referências e casos de uso documentados.3- Delineamentos experimentais para guiar emulação de cenários em ambiente de testes de forma a entender viabilidade da abordagem de monitoração sintética- Percentual de cenários que replicaram com precisão os ambientes de produção.4- Elaboração e execução de PoCs para realização de testes com usuários reais de forma a comparar achados do sistema emulado- Percentual de PoCs que resultaram em insights aplicáveis para a melhoria do sistema.5- Delineamentos experimentais para mapear os sistemas a coexistirem, suas características e principais problemas que possam estar prejudicando a comunicação entre eles e gerando indisponibilidade- Número de sistemas mapeados e documentados.- Número de problemas de comunicação identificados e resolvidos.6- Experimentos considerando modularização levando a teste o provisionamento de ambientes de testes dinâmicos apartados, - Tempo médio de provisionamento dos ambientes de teste.7- Experimentos buscando proposição de execução de testes regressivos de forma apartada de testes integrados, - Percentual de falhas detectadas nos testes regressivos antes da integração.8- Estudos buscando elementos e referências para subsidiar nova proposição de orquestração automática do provisionamento de múltiplos ambientes de teste para múltiplos artefatos associados a uma única pipeline.- Número de elementos e referências identificados.- Número de proposições de orquestração automática validadas. INFORMAÇÃO COMPLEMENTAR: A concepção de pipeline modular foi uma abordagem diferenciada para a execução de testes. Apesar de inicialmente parecer óbvia, ao separar esteiras entre mainframe e nuvem, o envolvimento da execução de testes foi um complicador. Esta abordagem permitiu que diferentes módulos de teste operassem de forma independente, mas coordenada. Era incerto ser possível dividir o processo de teste em segmentos distintos, cada um focado em aspectos específicos da funcionalidade do sistema. Havia que se comprovar se esta maior flexibilidade na gestão dos testes, possibilitando ajustes dinâmicos e a reutilização de módulos em diferentes cenários de teste, seria possível sem a necessidade de reconfiguração completa da pipeline.Este desenvolvimento buscava, na modularização, comprovações da execução de múltiplas cadeias de testes simultaneamente reduzindo o tempo total necessário para testes abrangentes. Além disso, essa abordagem facilitaria a identificação e isolamento de falhas entre ambientes mainframe e em nuvem, uma vez que cada módulo poderia ser avaliado individualmente.Para a implementação de testes integrados automáticos, foram adotadas ferramentas que permitem a simulação de interações complexas entre diferentes sistemas. Utilizou-se uma combinação de frameworks de automação de teste e linguagens de script específicas que, juntas, oferecem a flexibilidade necessária para modelar o comportamento do usuário e as transações entre sistemas heterogêneos. Essa abordagem permite a criação de cenários de teste altamente personalizáveis e adaptáveis capazes de simular uma vasta gama de operações bancárias em diferentes plataformas. A capacidade de automatizar esses testes integrados é fundamental para avaliar a interoperabilidade e o desempenho do sistema como um todo proporcionando insights valiosos sobre a robustez e a resiliência das integrações implementadas.Vale pontuar o papel das PoCs e os experimentos voltados a soluções de automação até então inexistentes. Isso permitiu identificar os principais desafios técnicos especialmente relacionados à integração de sistemas legados com plataformas em nuvem, o que exigiu uma abordagem criativa para superar as barreiras de compatibilidade e comunicação. Como resultado, houve refino das estratégias e soluções viabilizando entendimentos para a automação sem afetar questões de disponibilidade. Como resultado, foi possível estabelecer novos padrões para a realização de testes em sistemas bancários complexos e heterogêneos.Referências bibliográficas:- Adkins, H., Beyer, B., Blankinship, P., Lewandowski, P., Oprea, A., & Stubblefield, A. (2020). Building secure and reliable systems: best practices for designing, implementing, and maintaining systems. O'Reilly Media.- Erich, F. M., Amrit, C., & Daneva, M. (2017). A qualitative study of DevOps usage in practice. Journal of software: Evolution and Process, 29(6), e1885.- Forsgren, N., Humble, J., & Kim, G. (2018). Accelerate: The science of lean software and devops: Building and scaling high performing technology organizations. IT Revolution.- Gupta, R., Kranz, S., Regnat, N., Rumpe, B., & Wortmann, A. (2021). Towards a systematic engineering of industrial domain-specific languages. In 2021 IEEE/ACM 8th International Workshop on Software Engineering Research and Industrial Practice (SER&IP) (pp. 49-56). IEEE.- Kim, G., Behr, K., & Spafford, G. (2013). The phoenix project: A novel about IT. DevOps, and helping your business win, 345.- Lwakatare, L. E., Kuvaja, P., & Oivo, M. (2016). An exploratory study of devops extending the dimensions of devops with practices. Icsea, 104.- Rahman, A. A. U., Helms, E., Williams, L., & Parnin, C. (2015). Synthesizing continuous deployment practices used in software development. In 2015 Agile Conference (pp. 1-10). IEEE.- Shahin, M., Babar, M. A., & Zhu, L. (2017). Continuous integration, delivery and deployment: a systematic review on approaches, tools, challenges and practices. IEEE access, 5, 3909-3943. RESULTADO ECONÔMICO: Possibilidade de monitoração proativa de jornadas de clientes com redução de custos associada a menor indisponibilidade dos serviços. RESULTADO INOVAÇÃO: Processo de monitoração sintética para jornadas e APIs,  Novo processo para testes integrados considerando ambiente em nuvem e mainframe. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;NÚMERO: 5 ID ÚNICO: 109716 NOME: PROPOSIÇÃO DE ESTRATÉGIA DE INTEGRAÇÃO APLICADAS A JORNADAS DE CLIENTES A PARTIR DE SISTEMAS HÍBRIDOS DESCRIÇÃO: Sistemas de integração têm diferentes desafios associados aos contextos e estruturas em que são aplicados considerando uma empresa centenária do setor bancário-financeiro com um legado de equipamentos e softwares que evoluem e/ou passam a coexistir durante os anos.A jornada do cliente refere-se a todo o caminho percorrido na interação com o Banco – desde o primeiro contato até a conclusão de uma transação. Esta interação envolve diferentes pontos de contato e canais (como agências físicas e aplicativos móveis). Durante a jornada, são criados um conjunto de dados – pessoais (como nome e data de nascimento), financeiros (como saldo de contas e histórico de transações), de interação (como histórico de navegação e visitas a agências físicas) e de comportamento (como preferências de comunicação). Tal variedade constitui um contexto complexo que deve comportar diferentes sistemas – como CRMs, de análise de dados, plataformas omnichannel, entre outros. Neste cenário, integrar diferentes jornadas em uma única estrutura com visão coesa é um desafio, principalmente em um ambiente de milhões de clientes com acessos a ferramentas transacionais em tempo real.Assim, esta linha de pesquisa volta-se a entender e demonstrar a viabilidade técnica de sistema integrado considerando as diferentes jornadas de clientes e as diferentes estruturas em que estão baseadas (mainframe e nuvem). Para isso, organiza atividades experimentais construindo proposições que serão validadas em protótipos.As limitações das soluções atuais envolviam petabytes de dados de jornadas armazenados de forma descentralizada, desatualizados, incorretos e/ou repetidos, alguns em planilhas em formato Excel. Isso inviabilizava o recebimento, tratamento e processamento dos dados de forma automática e a sua democratização para as diferentes áreas. As comunicações eram organizadas em formatos síncronos refletindo em processos desconexos e, por isso, muitas vezes pouco aproveitáveis e com muitos gargalos para uma operação de alta demanda.Assim, foram organizadas atividades buscando proposições para:1- Criar processo de unificação de dados – propor e entender2- Criar processo para consolidar e analisar dados oriundos de múltiplos canais3- Criar processo de comunicação assíncrona para garantir dados e eventos sincronizados em tempo real e independente do canal ou sistema de origem4- Criar processos com garantia de interoperabilidade entre sistemas legados e modernos.As atividades permitiram a composição de protótipos de sistema de integração de jornadas de clientes a ter viabilidade técnica testada em ambiente controlado.Vale ressaltar que os testes não podem ser feitos em ambiente de produção por envolver dados sensíveis e protegidos legalmente,  também não seria possível arriscar eventuais indisponibilidades dos sistemas causados por testes em ambiente reais e produtivos,  por isso, ambientes de testes com capacidade similares aos contextos de produção foram criados, de forma centralizada para os orquestradores e descentralizada em cada canal, para simular os desafios tecnológicos associados.Em uma estrutura financeira centenária, construída com tecnologias de maturidades tecnológicas inferiores às atuais, a possibilidade deste novo processo de integração configura-se como uma novidade significativa. Como resultado, será possível posicionar esta estrutura em uma configuração compatível com o atual e moderno sistema bancário-financeiro viabilizando uma atuação mais competitiva.Como pontos críticos de decisão, destaca-se que a incapacidade de integrar dados do sistema legado com sistemas modernizados, em tempo de transação, poderia inviabilizar a continuidade do projeto. Isso porque não seria possível ter acesso aos dados necessários para a construção almejada. A interoperabilidade entre esses sistemas não é uma solução óbvia, ainda que sua necessidade seja comum, configurando-se um desafio na proposição de soluções e em muitos testes iterativos. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Integração de sistemas,  Fluxos de dados,  Jornadas de clientes,  Omnichannel,  Tecnologias assíncronas,  Interoperabilidade,  Plataformas legadas,  Plataformas mainframe,  Estruturas em nuvem,  Protótipos,  Testes iterativos. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Esta linha de pesquisa voltou-se a entender a viabilidade técnica de um sistema integrado considerando as diferentes jornadas de clientes e as diferentes estruturas em que estão baseadas (mainframe e nuvem). O desenvolvimento das atividades possibilitou um conjunto de aprendizados.1- Criação de processo de unificação de dados.Houve aprendizados sobre nova arquitetura para basear a pipeline de transferência de dados externos para plataforma interna. Foram estudadas práticas de ETL para entender especificidades de processos de ingestão, processamento, e armazenamento de grandes volumes de dados. A partir de estudos sobre ferramentas de infrastructure as code, foi possível entender arquitetura com capacidade de gerenciar recursos de múltiplos provedores de cloud de forma declarativa,  também houve comprovação técnica de que o Terraform teria habilidade em manter um estado dos recursos, facilitando a gestão de dependências e a atualização de infraestruturas com complexidade reduzida.2- Criação de processo para consolidar e analisar dados oriundos de múltiplos canais.A solução deveria lidar com o tamanho dos arquivos recebidos,  na sua forma bruta, era impossível que fossem transferidos. Foram realizados estudos de estratégias de particionamento de dados, e experimentos para entender aplicações de crawlers para detectar esquemas de dados e triggers para automatizar os processos de ETL. A partir do entendimento de lógicas complexas do Glue, foi possível criar processo de partição de arquivos para posterior processamento e unificação. Apesar de problemas com o tamanho dos arquivos serem comuns ao desenvolvimento de software, ao incluir sistemas mistos e diferentes jornadas, sua situação era peculiar refletindo-se em um desenvolvimento não óbvio. O Glue é amplamente utilizado para simplificar tarefas complexas de ETL ao automatizar o trabalho envolvido na preparação de dados para análise. Com questões de particionamento de arquivos, o Glue configura-se como uma solução não óbvia porque a utilização de crawlers e triggers para automatizar os processos de ETL não é comumente usada em soluções de integração.Como novidade, houve aprendizados para criação de API responsável pelas mensagens que fica conectada no VPC endpoint de forma a fazer a inscrição e receber as mensagens conforme fossem finalizadas pelo serviço. A premissa foi que um VPC Endpoint facilitaria a comunicação direta e segura entre o atual VPC e outros serviços da nuvem. Houve entendimento de criação de um heartbeat em que após 30 segundos sem receber mensagens, ele finaliza a conexão entre servidor e cliente. A novidade é viabilizar um processo em que os eventos são expostos ao cliente de forma direta e sem depender do tempo de sessão – não possível com a solução mais usual de aplicar API Gateway.3- Criação de processo de comunicação assíncrona.Houve aprendizados sobre a aplicação de SSE (server sent events) para lidar com comunicações assíncronas tanto para inscrições quanto para mensagens garantindo integração entre o sistema do cliente e o servidor. Tecnologias SSE são comumente utilizadas em situações em que há uma necessidade contínua de atualizações em tempo real do servidor para o cliente,  em projetos que não envolve dimensão tempo real e que demanda melhoria na eficiência da comunicação entre cliente e servidor, a implementação de SSE configura-se como uma solução não óbvia.4- Criação de processos com interoperabilidade entre sistemas legados e modernos.Aprendizados permitiram a criação de processo de integração entre origem de dados no mainframe e respectivo processamento em base modernizada. Houve entendimento de peças novas com a proposição de processo que flexibiliza o legado considerando elementos da tecnologia DMPS – sistema focado na migração e processamento de dados criado e desenvolvido com a necessidade de transferir dados de clientes de um mainframe para uma plataforma na nuvem superando o desafio de interoperabilidade entre sistemas mistos. DESAFIO TECNOLÓGICO: Criar sistemas de integração de jornadas de clientes conta com riscos associados à comunicação entre ambiente externo e interno em um cenário altamente regulado. Muitas vezes as soluções usuais não podem ser utilizadas. Configura-se como desafio adicional não ser possível construir e testar a solução em ambiente produtivo por riscos de incorrer em situações de indisponibilidade dos sistemas que afetam o Banco como um todo. Assim, estudos foram organizados buscando proposições e hipóteses a serem validadas experimentalmente a partir de protótipo.1- Criar processo de unificação de dados. Usualmente, soluções de unificação de dados envolvem o desenvolvimento de processos prévios de padronização. Considerando jornadas de clientes, essa proposição não era factível devido à diversidade e complexidade legada dos dados, incluindo a alta volumetria, diferentes layouts de arquivos e a complexidade da lógica de programação. Havia que se testar tecnologiass lidando-se com riscos de terem comportamentos imprevisíveis quando aplicados a um conjunto específico de dados e requisitos. Entre as proposições estudadas estava a utilização de ferramentas de ETL e implementação de data lake para armazenar dados em sua forma bruta buscando avaliar a efetividade de transformação de dados com padronização posterior.2- Criar processo para consolidar e analisar dados oriundos de múltiplos canais. Usualmente, cria-se um ambiente transitório onde os dados são inicialmente recebidos e armazenados antes de serem processados e integrados no ambiente final de análise. Entretanto, a solução traz uma contradição técnica pois, ao mesmo tempo que ele é feito para lidar com dados diferentes, esta diversidade aliada à volumetria de dados traz o problema de qualidade dos dados. A falta de protocolo pré-definido para transferência de arquivos – pois tem que considerar quais tecnologias dos canais estão envolvidas – configurava-se como um desafio uma vez que a troca de arquivos é uma parte essencial do funcionamento do ambiente transitório. Se a troca de arquivos não for adequada, em qualidade e tempo, pode-se comprometer a eficácia do ambiente transitório, tornando-o incapaz de cumprir seu propósito de transformar e padronizar dados antes da integração final. A necessidade de integrar o ambiente transitório (centralizado) com o data mesh (descentralizado) foi outro desafio enfrentado. Há riscos de que falhas nesta orquestração podem levar a inconsistências e atrasos na disponibilização dos dados.3- Criar processo de comunicação assíncrona para garantir dados e eventos sincronizados em tempo real e independente do canal ou sistema de origem. Esse tipo de comunicação envolve a manutenção do estado da comunicação e a garantia da entrega ordenada considerando requisitos de interoperabilidade. O desafio é garantir que as operações sejam idempotentes e que haja mecanismos de compensação para desfazer operações em caso de falhas. Um problema crítico enfrentado é que essa comunicação envolvia a manipulação de arquivos grandes. Delineamentos levaram a experimento técnicas de fragmentação e reagrupamento para fragmentar arquivos grandes em partes menores, transmiti-los de forma assíncrona e reagrupá-los no destino. Pela complexidade envolvida, havia riscos de introdução de falhas de transmissão e tempos de resposta lentos.4- Criar processos com garantia de interoperabilidade entre sistemas legados e modernos. Optou-se por construir um processo que fosse paralelo de forma a não ocasionar falhas de integração. Tal escolha trouxe a contradição pois esse paralelismo traz riscos de latência e inconsistência de dados,  também pode ocasionar falhas de integração em que dados podem ser perdidos ou corrompidos. Havia riscos também de que os dados levassem tempo para se propagar, resultando em estados inconsistentes temporários.Os desafios foram tratados a partir de delineamentos voltados a compor proposições levadas à validação experimental via prototipação. METODOLOGIA: Para lidar com os desafios técnicos encontrados nesta linha de pesquisa, foram organizadas as seguintes atividades:1- Criar processo de unificação de dados.-  Estudos sobre processos de integração utilizando ferramentas em nuvem e processos de ETL a serem testados buscando proposições a serem levadas a teste, - Experimentos considerando diferentes arquiteturas buscando viabilidade técnica de transferência e tratamento de dados, -  Testes em ambiente controlado para validar a eficácia das proposições a serem consideradas no protótipo.Métricas:-  Percentual de dados integrados sem erros, -  Tempo necessário para completar a integração, -  Grau de consistência dos dados antes e depois da integração.2- Criar processo para consolidar e analisar dados oriundos de múltiplos canais.-  Estudos avançados sobre particionamento de dados buscando proposições para lidar com grandes volumes de dados, -  Testes de integração entre sistemas internos para validar a eficácia da transferência e análise de dados de múltiplos canais, -  Experimentos e testes de cenários para medir o comportamento do sistema em diferentes condições de carga e fluxo de dados.Métricas:-  Eficiência de particionamento a partir do tempo e recursos necessários para particionar dados, -  Integridade dos dados a partir do grau de preservação dos dados após particionamento, -  Desempenho de transferência considerando velocidade e confiabilidade na transferência de dados particionados.3- Criar processo de comunicação assíncrona-  Estudos sobre protocolos de comunicação assíncrona, -  Testes de comunicação entre cliente e servidor para validar a latência e a taxa de entrega de mensagens, -  Criação e validação de protótipos para testar a resiliência e desempenho em ambiente controlado.Métricas:-  Latência de comunicação com análise do tempo de resposta entre cliente e servidor, -  Taxa de entrega de mensagens considerando o percentual de mensagens entregues com sucesso, -  Resiliência a partir da capacidade de recuperação após falhas de comunicação.4- Criar processos com garantia de interoperabilidade entre sistemas legados e modernos.-  Estudos sobre arquitetura de sistemas buscando entendimento de práticas de ETL e protocolos de comunicação entre mainframe e nuvem, -  Testes de integração entre sistemas legados e modernos buscando validação da interoperabilidade em ambiente controlado, -  Experimentos considerando infraestrutura como código (Terraform) para testar a gestão de recursos e atualização de infraestruturas.Métricas:-  Segurança considerando o número de vulnerabilidades detectadas, -  Desempenho a partir do tempo de resposta e taxa de transferência de dados, -  Compatibilidade considerando o número de sistemas legados e modernos suportados.Essas metodologias e métricas ajudarão a determinar se as proposições devem ser incluídas no protótipo, baseando-se nos resultados dos experimentos e testes realizados. INFORMAÇÃO COMPLEMENTAR: A necessidade de construção de estratégias de convivência entre sistemas legados (mainframe) e em nuvem (pública e privada) é inerente a processos de integração entre tais plataformas. Isso porque a transferência de uma à outra, ou a reconstrução do que existe no legado para o moderno, são processos tão complexos ao ponto de serem impraticáveis.Essa estratégia de convivência é caracterizada pela singularidade de cada implementação não havendo solução pré-definida ou universal. Sistemas mainframe, em particular, são fortemente acoplados e intrinsicamente enraizados nas operações de negócios que atendem e desenhados especificamente para aquela atuação. Esta característica de acoplamento forte e especificidade ao negócio torna sua integração com infraestruturas de cloud, que são por natureza mais modulares e flexíveis, um desafio técnico. Estudos internacionais, tanto teóricos quanto direcionados a casos específicos, corroboram o argumento ao demonstrar uma variedade de estratégias adaptativas que organizações ao redor do mundo têm empregado para superar esses desafios.Além disso, a estratégia de convivência deve ser sensível às nuances do contexto organizacional e aos requisitos únicos do negócio relacionado. Isso significa que uma solução adequada para uma empresa dificilmente será aderente à outra.Por fim, a evolução contínua das tecnologias de cloud adicionam dinamismo ao desafio de integração entre estruturas. Novas ferramentas e abordagens surgem constantemente com potencial para melhorar a interoperabilidade e a eficiência operacional. No entanto, a incorporação dessas novidades em um ambiente que inclui sistemas mainframe fortemente acoplados e específicos ao negócio exige uma estratégia adaptativa que reconheça e mantenha a integridade dos sistemas legados. Porém, a partir de atividades experimentais, é possível criar estratégias inovadoras que permitem avançar o estado da arte em estratégias de integração de estruturas heterogêneas.Como subsídio para o desenvolvimento desta linha de pesquisa, destacam-se as seguintes referências bibliográficas:- Chen, Y., Cheung, C. M. K., & Tan, C.-W. (2018). Omnichannel business research: Opportunities and challenges. Decision Support Systems, 109, 1-4.- Dingre, S. (2023). Data integration: Exploring challenges and emerging technologies for automation. International Journal of Science and Research (IJSR), 12(12), 1395.- Mainardes, E. W., Rosa, C. A. D. M., & Nossa, S. N. (2020). Omnichannel strategy and customer loyalty in banking. International Journal of Bank Marketing, 38(4), 799-822.- ORGANIZAÇÃO PARA A COOPERAÇÃO E DESENVOLVIMENTO ECNONÔMICO (OCDE). Manual de Frascati: Proposta de práticas exemplares para inquéritos sobre investigação e desenvolvimento experimental. Coimbra: OCDE, 2007.- Papathomas, A., & Konteos, G. (2023). Financial institutions digital transformation: the stages of the journey and business metrics to follow. Journal of Financial Services Marketing, 1–17. RESULTADO ECONÔMICO: Ganhos relacionados ao melhor entendimento do cliente. Economia associada à criação de uma integração que permite comunicação unidirecional contínua e à menor capacidade utilizada do servidor. RESULTADO INOVAÇÃO: Possibilidade de processamento de dados a partir da criação de nova estratégia de convivência entre mainframe e nuvem. Novo modelo de integração que viabiliza geração de dados para tomada de decisão estratégica. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;"NÚMERO: 6 ID ÚNICO: 109717 NOME: SOLUÇÕES PARA PREVENÇÃO A FRAUDE E SISTEMAS DE AUTENTICAÇÃO PARA PLATAFORMA BANCÁRIA AGNÓSTICA AO BANCO DESCRIÇÃO: O projeto teve como objetivo principal avaliar a eficácia de um sistema de autenticação multifatorial (MFA) avançado, integrando tecnologias de reconhecimento biométrico, OTP via SMS, tokens de hardware e algoritmos de inteligência artificial (IA) para a detecção de comportamentos fraudulentos, visando aumentar a segurança nas transações e operações de login em plataformas bancárias. Antes da implementação deste sistema, as plataformas dependiam de métodos de autenticação menos sofisticados, o que deixava margem para uma maior ocorrência de fraudes e acessos não autorizados. Com a integração do MFA e algoritmos de IA, esperava-se uma redução significativa nessas incidências, melhorando assim a segurança e a confiança nas operações bancárias online. Sinigaglia, F., Carbone, R., Costa, G., & Zannone, N. (2020).Durante o desenvolvimento do projeto, foram realizadas análises comparativas entre usuários de um sistema de autenticação de fator único e aqueles utilizando o sistema MFA proposto. Os resultados obtidos mostraram uma diminuição considerável nas tentativas bem-sucedidas de fraude e nos acessos não autorizados no grupo que utilizou o MFA, confirmando a eficácia da abordagem multifatorial. Ajustes foram feitos nos algoritmos de IA para otimizar a detecção de fraudes e minimizar falsos positivos, melhorando a experiência do usuário sem comprometer a segurança. Além disso, foram realizadas adaptações nos métodos de autenticação e na interface de usuário, visando um melhor equilíbrio entre segurança e usabilidade.Após os desenvolvimentos e ajustes, o projeto demonstrou a viabilidade e os benefícios de integrar um sistema de autenticação multifatorial avançado em plataformas bancárias. A implementação de tecnologias biométricas, juntamente com a aplicação de algoritmos de IA para análise comportamental, provou ser uma estratégia eficaz na prevenção de fraudes e acessos não autorizados.A demonstração bem-sucedida da eficácia do sistema de autenticação multifatorial (MFA) avançado, que integra tecnologias de reconhecimento biométrico, OTP (One-Time Password) via SMS, tokens de hardware e algoritmos de inteligência artificial para a detecção de comportamentos fraudulentos em plataformas bancárias pode ser considerado um ponto chave nas atividades de desenvolvimento deste projeto. Esse marco foi alcançado após a conclusão de um estudo comparativo abrangente, que revelou uma diminuição significativa nas incidências de fraudes e acessos não autorizados entre os usuários que adotaram o sistema MFA em comparação com aqueles que dependiam de métodos de autenticação menos sofisticados.A realização de ajustes finos nos algoritmos de IA para aprimorar a detecção de atividades fraudulentas e a redução de falsos positivos, juntamente com as adaptações na interface de usuário para facilitar a usabilidade, representou um avanço significativo. Esse resultado comprovou a viabilidade do desenvolvimento do sistema MFA proposto sobre métodos de autenticação, além de fortalecer a segurança e conveniência para as operações bancárias online. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Autenticação,  MFA (Autenticação Multifatorial),  IA,  fraude,  segurança,  algoritmos,  reconhecimento biométrico,  OTP via SMS,  tokens de hardware. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: São elementos tecnologicamente inovadores deste projeto:Experimentos foram conduzidos com o objetivo de obter insumos para a criação de uma solução de middleware eficiente, que fosse capaz de garantir a segurança cibernética. Um dos experimentos envolveu a simulação de ataques de repetição e man-in-the-middle contra diversas configurações de autenticação. Observou-se que a vulnerabilidade diminuía significativamente ao utilizar-se uma combinação inédita de TLS (Transport Layer Security) com autenticação baseada em tokens de hardware. Esta combinação possibilitou a obtenção de novos conhecimentos: observou-se uma redução substancial nas vulnerabilidades a ataques de repetição e man-in-the-middle (MitM). No caso dos ataques de repetição, os tokens de hardware, que incorporam elementos temporais ou de uso único, impediram eficazmente a reutilização de mensagens interceptadas. Em relação aos ataques MitM, a integração de TLS com tokens de hardware tornou extremamente difícil a inserção de atacantes na comunicação, garantindo a detecção imediata de qualquer tentativa de interceptação ou alteração.Além disso, esses conhecimentos possibilitaram o desenvolvimento de um protocolo de comunicação personalizado. Este protocolo integra criptografia de ponta a ponta, assegurando que os dados permaneçam criptografados durante todo o trajeto, do emissor ao receptor. Também implementa um sistema de autenticação baseado em desafio-resposta, onde um desafio é enviado e deve ser corretamente respondido, aumentando a segurança em relação ao uso de senhas estáticas.Além disso, no desenvolvimento do sistema de pontuação de risco dinâmico, empregou-se uma abordagem customizada na análise de dados para extrair padrões associados a transações fraudulentas. Utilizando uma combinação avançada de técnicas de aprendizado de máquina, como redes neurais convolucionais (CNNs) e máquinas de vetores de suporte (SVMs), analisaram-se milhões de transações. Um dos avanços significativos foi a compreensão de como realizar a aplicação de Análise de Componentes Principais (PCA) para a redução da dimensionalidade dos dados. Esse entendimento permitiu descobrir características latentes fortemente indicativas de fraude. Essa descoberta foi fundamental para o desenvolvimento de um modelo híbrido, que combinou CNNs para detecção de padrões complexos e SVMs para classificação baseada nas características latentes, permitindo a identificação e a prevenção proativa de fraudes.Ao aplicar técnicas de explicabilidade em modelos de ML complexos, foram realizados experimentos para avaliar a eficácia de diversos métodos na identificação de características importantes. Um desses experimentos envolveu o emprego de SHAP (SHapley Additive exPlanations) para decompor as previsões de uma rede neural profunda treinada para detectar fraudes, comparando os resultados com aqueles obtidos por meio do uso de LIME (Local Interpretable Model-agnostic Explanations). Observou-se que SHAP fornecia uma visão mais coerente e abrangente das contribuições das características, facilitando o ajuste preciso do modelo para reduzir a taxa de falsos positivos sem comprometer a eficácia na detecção de fraudes. Baesens, B., Höppner, S., & Verdonck, T. (2021).Em relação às abordagens híbridas de modelagem, foram realizados experimentos comparando o desempenho de modelos complexos, como redes neurais profundas, com modelos mais simples, como árvores de decisão, em conjuntos de dados de transações bancárias. Um dos experimentos mais significativos envolveu a implementação de um sistema de votação ensemble, onde as previsões de múltiplos modelos simples eram combinadas para gerar uma previsão final. Esse sistema foi comparado com o desempenho de um único modelo complexo, revelando que, embora o modelo complexo apresentasse uma precisão ligeiramente superior, o sistema de votação ensemble oferecia melhor interpretabilidade, facilidade de manutenção e maior resiliência a mudanças nos padrões de dados. DESAFIO TECNOLÓGICO: Um dos principais obstáculos da linha desta linha de pesquisa foi a integração harmoniosa de tecnologias de reconhecimento biométrico, OTP via SMS e tokens de hardware em um ambiente bancário agnóstico. Esse desafio exigiu uma complexa coordenação de sistemas e protocolos de segurança para assegurar a interoperabilidade e eficácia na autenticação. Além disso, a calibração dos algoritmos de IA, a fim de identificar comportamentos fraudulentos com precisão, sem gerar um número excessivo de falsos positivos, representou um desafio técnico significativo, afetando diretamente a experiência do usuário e a eficiência operacional do sistema.Levantou-se a hipótese de adotar frameworks de desenvolvimento e APIs de segurança padronizadas que facilitassem a integração de diferentes tecnologias de autenticação. No entanto, a eficácia dessa abordagem permaneceu incerta, dada a diversidade de dispositivos e sistemas operacionais utilizados pelos clientes. Paralelamente, a codificação de técnicas de aprendizado de máquina mais sofisticadas e a alimentação constante dos algoritmos de IA com novos dados de transações poderiam, em teoria, melhorar a detecção de fraudes, minimizando os falsos positivos.Um dos obstáculos mais notáveis foi encontrar o equilíbrio certo entre segurança aprimorada e usabilidade. Embora o sistema MFA tenha demonstrado eficácia na redução de acessos não autorizados e tentativas de fraude, a complexidade adicional na experiência de autenticação gerou feedback negativo dos usuários, indicando uma barreira potencial à adoção plena do sistema. Além disso, a implementação eficiente de múltiplas formas de verificação sem causar atrasos significativos no acesso dos usuários legítimos apresentou-se como um desafio, dada a necessidade de processar e validar cada forma de autenticação de maneira rápida e segura.Para a autenticação multifatorial, foi considerada a possibilidade de integrar frameworks e serviços especializados em segurança e autenticação, visando simplificar a implementação de tecnologias biométricas ao mesmo tempo em que se mantinha a conformidade com regulamentações de privacidade. A ideia de utilizar Inteligência Artificial e Machine Learning, por meio do SageMaker, para análise comportamental em tempo real e identificação de padrões de uso suspeitos foi vista como uma potencial adição de segurança.A experimentação de modelos de ML complexos e ""caixas-pretas"", como redes neurais profundas, pode oferecer um alto desempenho na detecção de fraudes, mas muitas vezes à custa da interpretabilidade. Isso representa um desafio significativo quando é necessário explicar a base de uma determinada decisão a clientes ou a reguladores, especialmente em casos de disputa. Além disso, a conformidade com regulamentações que exigem transparência nas decisões automatizadas, impõe a necessidade de modelos que possam fornecer insights claros sobre seus processos de tomada de decisão. Baesens, B., Höppner, S., & Verdonck, T. (2021).Para abordar essa complexidade, a equipe ponderou a adoção de técnicas de interpretabilidade e explicabilidade de modelos, como LIME (Local Interpretable Model-agnostic Explanations) e SHAP (SHapley Additive exPlanations). Essas técnicas visam decompor as previsões de modelos complexos em contribuições compreensíveis de cada característica, proporcionando insights sobre como diferentes atributos influenciam as decisões do modelo.Além disso, foi considerada a possibilidade de implementar abordagens híbridas, como a combinação de CNN + árvore de decisão, cruzando modelos de alta precisão, mas de difícil interpretação, com modelos mais simples e explicáveis, utilizando, por exemplo, árvores de decisão ou modelos lineares para as camadas de decisão finais. Contudo, observou-se que a integração dessas técnicas e abordagens poderia apresentar desafios próprios, incluindo a complexidade adicional no treinamento e na manutenção dos modelos. METODOLOGIA: Pesquisa sobre Frameworks de Desenvolvimento e APIs de Segurança: Foram estudados e avaliados frameworks e APIs que facilitaram a integração de diversas tecnologias de autenticação, como reconhecimento biométrico, OTP via SMS e tokens de hardware, em um ambiente bancário.Experimentação com Técnicas de Aprendizado de Máquina Avançadas: Técnicas de machine learning sofisticadas foram implementadas e testadas, alimentando constantemente os algoritmos de IA com novos dados de transações para aprimorar a detecção de fraudes e minimizar falsos positivos.Avaliação de Usabilidade versus Segurança: Estudos de usuário foram realizados para encontrar o equilíbrio certo entre segurança aprimorada e usabilidade, considerando a complexidade na experiência de autenticação e o feedback dos usuários.Desenvolvimento de Métodos de Verificação Rápida e Segura: Foram pesquisados e testados métodos para processar e validar múltiplas formas de autenticação de maneira rápida e segura, sem causar atrasos significativos no acesso dos usuários legítimos.A integração de tecnologias biométricas, mantendo a conformidade com as regulamentações de privacidade, foi explorada por meio de frameworks e serviços especializados em segurança e autenticação.Uso de Inteligência Artificial para Análise Comportamental: O uso de IA e machine learning, por meio de ferramentas como SageMaker, para análise comportamental em tempo real e identificação de padrões de uso suspeitos foi considerado como uma adição de segurança.Exploração de Técnicas de Interpretabilidade e Explicabilidade de Modelos: Técnicas como LIME e SHAP foram estudadas e aplicadas para tornar as decisões de modelos de ML complexos mais interpretáveis e explicáveis, proporcionando insights sobre como diferentes atributos influenciam as decisões do modelo.Implementação de Abordagens Híbridas para Modelagem: A combinação de modelos de alta precisão, mas de difícil interpretação, com modelos mais simples e explicáveis, como árvores de decisão ou modelos lineares, para as camadas de decisão finais foi testada.Avaliação da Complexidade no Treinamento e Manutenção dos Modelos: Os desafios relacionados à complexidade adicional no treinamento e na manutenção dos modelos decorrentes da integração de técnicas de interpretabilidade e explicabilidade foram analisados.Estudo sobre a Aceitação de Explicações por Reguladores e Usuários Finais: Foi investigado como as explicações geradas pelos modelos foram recebidas por reguladores e usuários finais, considerando as incertezas quanto à aceitação dessas explicações. INFORMAÇÃO COMPLEMENTAR: O desenvolvimento de um sistema de autenticação multifatorial avançado, aliado à inteligência artificial, trouxe benefícios tangíveis não apenas para a segurança das operações em plataformas bancárias, mas também para a experiência geral do usuário. A utilização de reconhecimento biométrico, OTP via SMS e tokens de hardware, combinada com a capacidade analítica dos algoritmos de IA, resultou em um processo de autenticação mais robusto e personalizado. Isso não só dificultou ações fraudulentas, mas também aprimorou a capacidade dos sistemas em oferecer uma experiência de uso fluida e segura, ajustando-se automaticamente às necessidades e comportamentos dos usuários. Tal desenvolvimento tecnológico representa um avanço significativo na forma como as instituições financeiras abordam a segurança e a usabilidade, promovendo um ambiente online mais seguro e confiável.Além disso, a otimização dos processos de autenticação e a eficiência na detecção de tentativas de fraude têm um impacto positivo direto na operacionalidade das instituições financeiras. A redução de custos associados a fraudes e a melhoria na eficiência operacional são claros indicadores do valor agregado por essas tecnologias. Adicionalmente, a adaptação e integração de serviços de cloud computing, como AWS Cognito e Lambda, na gestão de identidades e autenticações, e o uso de SageMaker para o desenvolvimento de modelos de machine learning, ressaltam a importância da flexibilidade e da escalabilidade dos sistemas de segurança.Além disso, nos experimentos realizados para aprimorar a segurança das transações através de autenticação multifatorial, foi explorada a eficácia da biometria comportamental combinada com a autenticação de dois fatores (2FA). Este estudo envolveu a análise de padrões comportamentais, que são únicos para cada indivíduo. A integração desses dados comportamentais com métodos tradicionais de 2FA, como SMS ou tokens de autenticação de aplicativos, resultou em um sistema de autenticação robusto que diminuiu ainda mais a possibilidade de acesso não autorizado.É importante salientar também que diversas equipes conduziram experimentos e testagens para validar a eficácia e robustez dos sistemas propostos. A equipe de segurança cibernética simulou ataques de repetição e man-in-the-middle, descobrindo que a combinação de TLS com autenticação baseada em tokens de hardware reduzia vulnerabilidades em 70%, resultando em um protocolo de comunicação personalizado com criptografia de ponta a ponta e autenticação baseada em desafio-resposta, que melhorou o tempo de resposta a ameaças em 40%.A equipe de ciência de dados desenvolveu um sistema de pontuação de risco dinâmico utilizando redes neurais convolucionais (CNNs) e máquinas de vetores de suporte (SVMs). A aplicação de PCA reduziu a dimensionalidade dos dados em 50%, aumentando a precisão da detecção de fraudes em 15%, com uma taxa de detecção de 92% e redução de falsos positivos para 5%. Técnicas de explicabilidade como SHAP e LIME foram empregadas para ajustar modelos de ML complexos, com SHAP reduzindo a taxa de falsos positivos em 10% sem comprometer a eficácia.A equipe de desenvolvimento comparou sistemas de votação ensemble com modelos complexos, constatando que o ensemble oferecia uma precisão de 88% e maior resiliência de manutenção em comparação com a precisão de 90% do modelo complexo. Adicionalmente, a equipe de autenticação explorou a biometria comportamental combinada com autenticação de dois fatores (2FA), resultando em um sistema que reduziu a possibilidade de acesso não autorizado em 25%.REFERÊNCIAS BIBLIOGRÁGICAS:Sinigaglia, F., Carbone, R., Costa, G., & Zannone, N. (2020). A survey on multi-factorauthentication for online banking in the wild. Computers & Security, 95, 101745.https://doi.org/10.1016/j.cose.2020.101745Baesens, B., Höppner, S., & Verdonck, T. (2021). Data engineering for fraud detection. DecisionSupport Systems, 142, 113492. https://doi.org/10.1016/j.dss.2021.113492 RESULTADO ECONÔMICO: A redução significativa de fraudes e acessos não autorizados diminuiu prejuízos financeiros, elevando a confiança dos usuários e potencializando a economia operacional. RESULTADO INOVAÇÃO: A integração eficaz de autenticação multifatorial e IA aprimorou a detecção de comportamentos fraudulentos, elevando padrões de segurança e usabilidade das soluções. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;"NÚMERO: 7 ID ÚNICO: 109718 NOME: EXPERIMENTAÇÃO DE JORNADA AGNÓSTICA PARA UNIFICAÇÃO DE SERVIÇOS BANCÁRIOS EM UMA PLATAFORMA ÚNICA DESCRIÇÃO: Menos de um terço dos maiores bancos do mundo estão investindo em ecossistemas financeiros digitais de uma “maneira significativa”. Uma pesquisa diz que cerca de 25% dessas organizações estão investindo em testes piloto de sistemas bancários centrais. (MIT Technology Review Insights, & Boston Consulting Group, (2023).Diante disso, a presente linha de pesquisa propõe o desenvolvimento de mecanismos que viabilizem a modernização de serviços estruturais do banco, com ênfase na integração eficaz de sistemas mainframe antigos com as tecnologias de cloud computing modernas para evolução na entrega de serviços ao cliente. Especificamente, procurou-se desenvolver um robusto mecanismo de serialização e deserialização para facilitar a comunicação entre os sistemas legados e as plataformas modernas, otimizar o armazenamento e a performance na cloud, reforçar a segurança dos dados e gerenciar eficientemente as transações e falhas entre microserviços.Foi trabalhada uma abordagem experimental para testar a viabilidade técnica de alguns componentes a serem aplicados futuramente na infraestrutura da empresa. Foram desenvolvidos e testados protótipos de algoritmos de serialização e deserialização personalizados para converter dados de EBCDIC para UTF-8, utilizando técnicas de mapeamento de caracteres e normalização de dados. Estruturas aninhadas em JSON/XML foram prototipadas para representar campos compostos e registros hierárquicos dos mainframes utilizando esquemas JSON Schema e XML Schema para validação. Esses protótipos foram validados através de testes com conjuntos de dados representativos para verificar a integridade e precisão dos dados convertidos.Explorou-se o uso de Elastic Block Store (EBS) para otimizar o armazenamento na cloud e a performance, criando protótipos que foram submetidos a testes de carga e desempenho. Além disso, tecnologias de emulação de terminais foram experimentadas para garantir a compatibilidade com aplicações COBOL, avaliando a eficácia dessas soluções em ambientes controlados.A segurança foi significativamente reforçada através de experimentações com AWS IAM e AWS KMS para o gerenciamento de identidades, acessos e chaves de criptografia. Foram realizados testes de segurança utilizando ferramentas como OpenVAS e Nessus para identificar possíveis vetores de ataque. A viabilidade da atualização para os protocolos TLS 1.2 e 1.3 foi testada através de protótipos que passaram por rigorosos testes de desempenho e segurança com as ferramentas como JMeter, Gatling e SSL Labs, garantindo que a configuração mitigava vulnerabilidades sem introduzir novos vetores de ataque.Para gerenciar transações e falhas entre microserviços, foi desenvolvido um protótipo de sistema de ""sagas"". Este protótipo foi testado em condições normais e de falha utilizando ferramentas de simulação como Chaos Monkey, assegurando que as sagas mantinham a integridade dos dados mesmo em operações complexas e distribuídas. A coordenação entre microserviços foi experimentada com orquestradores de sagas como Netflix Conductor e Axon Framework.O marco crítico deste projeto foi a necessidade de desenvolver e testar protótipos de mecanismos de serialização e deserialização, junto com a introdução de APIs agnósticas para a integração entre sistemas mainframe legados e tecnologias de cloud computing. Esses desenvolvimentos experimentais possibilitaram uma comunicação eficaz entre as diferentes arquiteturas tecnológicas, superando os desafios de incompatibilidade e limitações de agilidade impostas pelos sistemas antigos. Além disso, a adoção de Elastic Block Store (EBS) melhorou o armazenamento e a performance na cloud.Espera-se que o banco tenha uma infraestrutura de TI mais ágil, segura e capaz de responder rapidamente às demandas do mercado e às expectativas dos clientes. A integração eficiente dos sistemas legados com as novas tecnologias deve permitir a introdução de serviços digitais inovadores com uma experiência de usuário aprimorada. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: API,  agnóstica,  arquitetura,  acesso,  Restful,  integração,  distribuídas,  dados,  criptografia. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para superar o desafio de integrar sistemas que utilizavam chamadas de procedimento remoto (RPC) e formatos de dados proprietários, foi necessário realizar uma série de experimentações técnicas focadas nos mecanismos de serialização/deserialização de dados e nos protocolos de autenticação segura. Durante os testes, foi implementado um mecanismo robusto de serialização/deserialização que converteu os tipos de dados proprietários dos mainframes em formatos JSON ou XML, utilizados pelas APIs RESTful. Os experimentos demonstraram que a conversão de dados poderia ser realizada com uma taxa de sucesso de 98%, garantindo a integridade e a precisão dos dados.No re-hosting, a compreensão sobre a arquitetura de armazenamento da cloud e as capacidades de I/O levou à experimentação de soluções de armazenamento de alto desempenho. Configurou-se o ambiente para maximizar a taxa de transferência e a IOPS (Input/Output Operations Per Second). Os testes de desempenho indicaram que a configuração otimizada resultou em uma melhoria de 40% na taxa de transferência e um aumento de 35% na IOPS comparado com as soluções de armazenamento tradicionais. Exploraram-se tecnologias de emulação de terminais avançadas, integrando soluções que ofereciam latência reduzida e maior compatibilidade com as aplicações COBOL existentes. Os resultados mostraram que a latência foi reduzida em 25%, garantindo uma experiência de usuário final ininterrupta e eficiente.A segurança foi reforçada através do desenvolvimento de um sistema de gerenciamento de identidade e acesso, juntamente com uma solução personalizada para gestão de chaves de criptografia. Estudos realizados apontaram que a ativação do TLS não era suficiente. Foi necessário entender as versões do protocolo TLS, as cifras suportadas e a importância da ordem de preferência dessas cifras. Constatou-se que versões anteriores do TLS (1.0 e 1.1) eram vulneráveis a ataques, como o POODLE, e que cifras mais robustas, como ECDHE (Elliptic Curve Diffie-Hellman Ephemeral) para o intercâmbio de chaves, ofereciam mais segurança e melhor desempenho. Com essas compreensões, configurou-se o TLS 1.2 (e posteriormente o 1.3) como padrão para todas as comunicações entre microserviços, especificando uma lista de cifras que excluía algoritmos conhecidos por suas vulnerabilidades. Os testes de segurança indicaram que a implementação do TLS 1.2 e 1.3 reduziu a vulnerabilidade a ataques em 95%.Para cada operação de negócio que exigia várias transações entre microserviços, delineou-se uma série de passos (sagas), identificando claramente as operações de compensação para cada passo em caso de falhas. Isso exigiu uma análise profunda dos fluxos de negócios e uma modelagem cuidadosa para garantir que todas as situações de falha fossem adequadamente tratadas, mantendo a integridade dos dados. Os experimentos de simulação de falhas indicaram que a implementação de sagas reduziu a perda de dados em 98% e melhorou a resiliência do sistema em 45%.Compreendeu-se que a criptografia AES-256, por si só, não garantia segurança completa sem uma gestão de chaves eficaz. Desenvolveu-se um serviço personalizado para gerenciamento de chaves que oferecesse não apenas armazenamento seguro, mas também políticas de rotação de chaves e controles de acesso granulares para as chaves de criptografia. Além de adotar o AES-256 para criptografia de dados em repouso e em trânsito, configurou-se a rotação automática de chaves, garantindo que as chaves antigas fossem substituídas regularmente sem interromper os serviços. Isso foi complementado por políticas de acesso restritas às chaves, onde apenas serviços e indivíduos autorizados poderiam requisitar o uso de chaves para criptografia e descriptografia. Os testes de segurança e eficiência mostraram que a rotação automática de chaves reduziu o risco de comprometimento das chaves em 90% e assegurou a continuidade dos serviços sem interrupções. DESAFIO TECNOLÓGICO: Configurou-se um grande desafio prover a integração de sistemas com formatos de dados proprietários dos mainframes. A principal complexidade técnica estava na incompatibilidade de tipos de dados. Mainframes frequentemente utilizavam formatos binários compactos, como EBCDIC, enquanto JSON e XML utilizam UTF-8. Além disso, os dados dos mainframes incluíam campos compostos e registros hierárquicos, sem correspondentes diretos em JSON ou XML, exigindo estruturas aninhadas e algoritmos de serialização/deserialização personalizados. A preservação da integridade dos dados era crucial, pois qualquer erro poderia comprometer a precisão dos dados financeiros e de transações críticas. Propôs-se um mecanismo robusto de serialização/deserialização, incluindo mapeamentos personalizados, validações rigorosas e técnicas de otimização de código. No entanto, havia incerteza sobre se essa abordagem poderia lidar com todas as variações dos dados dos mainframes e se os testes confirmariam uma taxa de sucesso suficientemente alta na conversão.Outro desafio significativo foi garantir a segurança nas comunicações entre microserviços, especialmente devido às vulnerabilidades em versões antigas do protocolo TLS. As versões anteriores do TLS, como 1.0 e 1.1, eram suscetíveis a ataques POODLE e BEAST, que exploravam fraquezas na implementação do protocolo para interceptar e decifrar dados sensíveis. A complexidade técnica aumentou devido à necessidade de garantir compatibilidade com cifras seguras sem comprometer o desempenho. Além disso, a configuração inadequada de cifras poderia resultar em vetores de ataque adicionais, como ataques de downgrade, onde um atacante força a utilização de uma versão ou cifra mais fraca do protocolo. Considerou-se a configuração do ambiente para utilizar versões mais seguras do TLS (1.2 e 1.3) e a seleção de cifras robustas, priorizando aquelas que ofereciam um equilíbrio entre segurança e desempenho. A implementação exigiria configuração detalhada de servidores e clientes para garantir que apenas cifras seguras fossem utilizadas, além de definir uma política de preferência de cifras. Contudo, havia incerteza sobre se essa abordagem poderia realmente mitigar os riscos sem introduzir novos problemas de desempenho ou compatibilidade.Manter a integridade dos dados em transações complexas entre microserviços foi outro desafio crítico. A principal dificuldade técnica envolveu a gestão de transações que exigiam múltiplas operações em diferentes microserviços, onde falhas parciais poderiam deixar o sistema em um estado inconsistente. Se uma dessas operações falhasse, era necessário garantir que todas as operações anteriores fossem revertidas para manter a integridade dos dados. Uma das soluções propostas foi a implementação de operações de compensação para cada passo da transação, mas isso se mostrou complexo, pois exigia a definição de lógicas de reversão específicas para cada tipo de operação. A abordagem baseada em sagas foi considerada, onde cada transação seria dividida em uma série de passos com operações de compensação definidas. Entretanto, permanecia a dúvida sobre se essa abordagem poderia realmente garantir a integridade dos dados em todas as possíveis condições de falha.A gestão eficaz de chaves de criptografia representou outro desafio técnico complexo. A criptografia AES-256, embora robusta, exigia uma gestão eficaz de chaves para garantir a segurança completa dos dados. A principal dificuldade envolveu a implementação de um sistema de gerenciamento de chaves que pudesse realizar a rotação automática de chaves sem interromper os serviços em execução. Delineou-se a hipótese de criar um serviço personalizado para gerenciamento de chaves que incluísse técnicas avançadas de criptografia e hardware seguro para armazenar as chaves e controles de acesso granulares para garantir que apenas entidades autorizadas pudessem utilizar as chaves. METODOLOGIA: A metodologia de desenvolvimento seguiu uma sequência estruturada para abordar as hipóteses e superar os desafios tecnológicos. Para a integração de sistemas com formatos de dados proprietários dos mainframes, iniciou-se com a análise dos formatos de dados, identificando e documentando todos os formatos proprietários utilizados, incluindo EBCDIC, formatos binários compactos e estruturas de registros hierárquicos. Foram analisadas as diferenças entre EBCDIC e UTF-8, bem como os impactos na conversão de dados numéricos e de datas. Desenvolveram-se algoritmos de serialização e deserialização personalizados para converter dados de EBCDIC para UTF-8, utilizando técnicas de mapeamento de caracteres e normalização de dados. Estruturas aninhadas em JSON/XML foram projetadas para representar campos compostos e registros hierárquicos dos mainframes, utilizando esquemas JSON Schema e XML Schema para validação. Testes de validação foram realizados utilizando conjuntos de dados de teste representativos, verificando a integridade e precisão dos dados convertidos. Testes automatizados com frameworks como JUnit e TestNG garantiram a robustez dos algoritmos de conversão.Na segurança das comunicações entre microserviços, a análise de vulnerabilidades das versões antigas do TLS foi o primeiro passo, seguida pela configuração do ambiente para utilizar TLS 1.2 e 1.3 com cifras robustas como ECDHE, AES-GCM e ChaCha20-Poly1305. Ferramentas de análise de segurança como OpenVAS e Nessus foram utilizadas para identificar possíveis vetores de ataque. Configurou-se o ambiente para utilizar TLS 1.2 e 1.3, priorizando cifras robustas. Realizaram-se testes de desempenho utilizando ferramentas como Apache JMeter e Gatling para avaliar o impacto das novas cifras. Testes de segurança com SSL Labs e Wireshark garantiram que a configuração mitigava vulnerabilidades sem introduzir novos vetores de ataque. A compatibilidade da nova configuração de TLS com todos os microserviços e sistemas legados foi testada em ambientes isolados, ajustando configurações conforme necessário.Para manter a integridade dos dados em transações complexas entre microserviços, analisaram-se as transações, identificando dependências e pontos de falha potenciais. Utilizaram-se diagramas de sequência e fluxogramas para visualizar o fluxo de transações. Desenvolveram-se operações de compensação específicas para cada tipo de operação, utilizando padrões de design como Command e Memento. Implementaram-se operações de compensação com frameworks de transações distribuídas como Apache Camel e Spring Boot. Transações complexas foram divididas em passos menores com operações de compensação definidas, utilizando o padrão de saga. A coordenação entre microserviços foi implementada com orquestradores de sagas como Netflix Conductor e Axon Framework. Testes em condições normais e de falha garantiram que as sagas mantinham a integridade dos dados, utilizando ferramentas de simulação de falhas como Chaos Monkey.Na gestão eficaz de chaves de criptografia, analisaram-se os requisitos de criptografia e desenvolveu-se um serviço personalizado para gerenciamento de chaves, incluindo técnicas avançadas de criptografia e hardware seguro. Identificaram-se e documentaram-se todos os requisitos de criptografia, incluindo tipos de dados a serem protegidos e políticas de acesso. Um serviço personalizado para gerenciamento de chaves foi experimentado, utilizando técnicas avançadas de criptografia. Políticas de rotação automática de chaves foram definidas utilizando frameworks como HashiCorp Vault e AWS KMS, garantindo a substituição regular das chaves sem interrupções. Implementaram-se controles de acesso granulares utilizando RBAC e ABAC para garantir que apenas serviços e indivíduos autorizados possam utilizar as chaves de criptografia. O acesso às chaves foi monitorado e auditado utilizando ferramentas de SIEM como Splunk e ELK Stack. INFORMAÇÃO COMPLEMENTAR: A execução de estudos, pesquisas e experimentos aqui apresentados trouxeram uma série de benefícios significativos para o time de pesquisadores e, por extensão, para o projeto de modernização dos serviços bancários como um todo. Estes benefícios abrangeram diversas áreas, desde a melhoria da eficiência operacional até a garantia de segurança e compliance. A seguir são apresentados alguns benefícios adquiridos:Ao estudar e testar a interoperabilidade entre sistemas mainframe e cloud, a equipe conseguiu desenvolver soluções que permitiram uma integração eficaz entre essas plataformas. Como consequência, foi possível perceber uma maior a flexibilidade operacional, permitindo que sistemas legados e modernos trabalhassem conjuntamente.Otimização de Processos de Modernização: A análise detalhada das estratégias de modernização de mainframe permitiu uma transição mais suave e eficiente para arquiteturas modernas, reduzindo riscos e custos associados à migração, além de minimizar interrupções operacionais.REFERÊNCIAS BIBLIOGRÁFICAS:MIT Technology Review Insights, & Boston Consulting Group. (2023, October 23).Seeking a successful path to core modernization. MIT Technology Review.https://www.technologyreview.com/2023/10/23/1082061/seeking-a-successful-path-to-core-modernization/ RESULTADO ECONÔMICO: Retenção e captação de novos clientes,  apresentação de estruturas modernizadas,  ganhos de produtividade e elevação de competitividade dada a modernização de serviços. RESULTADO INOVAÇÃO: Estruturação de novas arquiteturas que trouxeram jornadas modernizadas, ganhos de escalabilidade,  obtenção de novos conhecimentos para estruturação de aplicações em ambiente cloud. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;NÚMERO: 8 ID ÚNICO: 109719 NOME: SOLUÇÕES PARA ESTRUTURAÇÃO DE COMPONENTES DE SEGURANÇA EM LOGIN UNIFICADO DESCRIÇÃO: A presente linha de pesquisa propõe o desenvolvimento de um sistema de login unificado, empregando a tecnologia OpenID Connect e soluções da AWS. Este sistema tem como meta fornecer um processo de autenticação seguro e eficiente para os clientes, mitigando riscos associados a ameaças cibernéticas.Para atingir esse objetivo, foram estudados e experimentados componentes para prover a integração de sistemas legados com novas arquiteturas, incluindo eventos e microsserviços, e a implementação de métodos de autenticação avançados. Destacam-se a autenticação multifatorial e a detecção de anomalias com base em inteligência artificial, visando a escalabilidade, alta disponibilidade e conformidade com normas de segurança de dados.Espera-se que, com a implementação deste sistema de login unificado, a empresa aprimore a segurança e a eficiência do processo de autenticação. A experimentação de OpenID Connect, soluções da AWS, autenticação multifatorial e detecção de anomalias por IA deve criar uma plataforma segura e adaptável às necessidades dos clientes, contribuindo para a proteção contra ameaças cibernéticas e melhorando a experiência do usuário.O marco crítico desta linha de pesquisa foi a implementação bem-sucedida de um sistema de login unificado que integra OpenID Connect e soluções específicas da AWS para oferecer autenticação segura e eficiente. Esse sistema incorporou autenticação multifatorial e utilizou inteligência artificial para a detecção de anomalias, abordando diretamente as vulnerabilidades associadas a ameaças cibernéticas.A integração efetiva de sistemas legados com arquiteturas modernas foi realizada por meio de eventos e microsserviços, assegurando a escalabilidade e alta disponibilidade do sistema. Esse processo atendeu estritamente às normas de segurança de dados vigentes. O desenvolvimento desse sistema de login unificado visa fortalecer a segurança contra ameaças cibernéticas e melhorar a experiência do usuário ao simplificar o processo de autenticação. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: OpenID,  unificado,  integrar,  login,  segurança,  arquitetura,  autenticação,  sistema,  criptografia NATUREZA: Serviço ELEMENTO TECNOLÓGICO: A hipótese central desta linha de pesquisa foi a de que a criação de um sistema de login unificado, utilizando OpenID Connect em uma infraestrutura de cloud pública, poderia aprimorar significativamente o processo de autenticação para usuários de variados canais bancários. Este estudo visou comprovar a viabilidade técnica das soluções propostas, superando as deficiências inerentes aos sistemas de autenticação fragmentados. Para isso, foram conduzidos experimentos rigorosos e detalhados, com foco em diversas inovações tecnológicas.Foram realizados experimentos para desenvolver algoritmos de criptografia customizados, com o objetivo de proporcionar uma segurança superior em comparação às soluções tradicionais. A pesquisa inicial envolveu uma análise detalhada de algoritmos de criptografia existentes, como AES, RSA e criptografia homomórfica, identificando suas limitações e vulnerabilidades em cenários de autenticação. Utilizando Python e bibliotecas criptográficas, foi criado um novo algoritmo que combinava criptografia simétrica e assimétrica para garantir a segurança dos tokens JWT, incorporando técnicas de criptografia homomórfica para permitir a análise segura dos dados. A eficácia do novo algoritmo foi avaliada por meio de testes de penetração e análise de vulnerabilidades, que demonstraram resistência a ataques de força bruta, ataques de replay e outras ameaças comuns. Os resultados dos testes indicaram uma redução de 30% no tempo de criptografia e descriptografia, além de um aumento de 50% na resistência a ataques.Um método em Python foi desenvolvido e implementado como uma função Lambda, permitindo a rotação automática de chaves e a aplicação de políticas de acesso dinâmicas. A avaliação do desempenho incluiu a medição da latência de requisição e a rotatividade de chaves, resultando em uma latência média de 200 ms e uma rotação de chaves sem interrupção do serviço.A integração da biblioteca scikit-learn permitiu o desenvolvimento de um modelo de aprendizado profundo para analisar os dados comportamentais em tempo real. Este modelo foi treinado utilizando um conjunto de dados históricos de autenticação, identificando padrões normais e anômalos. Testes de desempenho garantiram que as funções Lambda pudessem escalar automaticamente e lidar com um grande volume de requisições, proporcionando uma validação eficiente das credenciais e uma análise comportamental precisa. A precisão do modelo foi avaliada em 95%, com uma capacidade de processamento de até 1.000 requisições por segundo.Para complementar, foi desenvolvido um sistema de logging que incorporava análise comportamental baseada em inteligência artificial. O sistema de logging coletava e armazenava logs detalhados das tentativas de autenticação. Técnicas de processamento de linguagem natural (NLP) foram implementadas para permitir a análise dos logs e a identificação de padrões suspeitos. Testes de análise, utilizando um conjunto de logs históricos e simulados, avaliaram a precisão do sistema na identificação de padrões suspeitos, resultando em uma detecção precisa de atividades suspeitas. O sistema de logging reduziu a taxa de falsos positivos para 2% e aumentou a detecção de atividades anômalas em 40%.Experimentos foram executados para comprovar a viabilidade da criação de um Gateway, que serviria para rotear todas as requisições de autenticação para as funções Lambda apropriadas, configurando políticas de acesso detalhadas para garantir a segurança das requisições. Um sistema de controle de acesso dinâmico foi desenvolvido para ajustar as políticas IAM em tempo real com base nas análises comportamentais e na avaliação contínua de risco. Testes de segurança garantiram que o Gateway e as políticas de acesso fossem configurados corretamente, proporcionando uma segurança aprimorada e uma experiência de usuário melhorada. O Gateway reduziu as tentativas de acesso não autorizado em 30% e melhorou o tempo de resposta em 20%. DESAFIO TECNOLÓGICO: Ao desenvolver algoritmos de criptografia customizados, o desafio técnico foi garantir que essas operações fossem executadas com latência mínima, mesmo em cenários de alta carga. A complexidade incluiu o gerenciamento de chaves, onde se buscou garantir a segurança das chaves enquanto se minimizava o tempo de acesso. A paralelização também foi um ponto crítico, exigindo a divisão das operações de criptografia para serem processadas em paralelo sem introduzir vulnerabilidades. Além disso, a integração com hardware, utilizando aceleração por hardware como AES-NI, foi considerada para melhorar a performance. A hipótese de solução envolveu a implementação de criptografia simétrica utilizando AES-GCM com aceleração por hardware através de AES-NI. Paralelamente, desenvolveu-se um módulo em Rust para operações críticas de criptografia, utilizando bindings com Python através de FFI (Foreign Function Interface). A ideia foi reduzir a latência ao máximo possível. No entanto, a integração segura entre Rust e Python, garantindo que não houvesse vazamento de memória ou vulnerabilidades de segurança, foi um desafio significativo e requereria testes rigorosos.A rotação automática de chaves e a aplicação de políticas dinâmicas no AWS KMS apresentaram problemas específicos, como a consistência das chaves, onde se buscou garantir que todas as instâncias de serviço utilizassem a chave correta após a rotação. A sincronização de políticas também foi um desafio, exigindo a aplicação de políticas de acesso dinâmicas em tempo real sem causar inconsistências. A latência de atualização foi outro ponto crítico, buscando-se minimizar o tempo de propagação das atualizações de chaves e políticas. A hipótese de solução envolveu o desenvolvimento de um mecanismo de cache distribuído utilizando Redis para armazenar temporariamente as chaves criptográficas e políticas de acesso. Utilizou-se AWS Step Functions para orquestrar a rotação de chaves e a aplicação de políticas, garantindo que a rotação ocorresse de maneira sequencial e controlada.Implementar um sistema de logging que utilizasse técnicas de NLP para análise comportamental em tempo real envolveu desafios específicos, como o volume de dados, onde se buscou processar grandes volumes de logs rapidamente. O treinamento contínuo foi outro ponto crítico, buscando-se re-treinar modelos de NLP continuamente sem interromper o serviço. A precisão do modelo também foi um desafio, onde se buscou garantir que o modelo de NLP mantivesse alta precisão ao longo do tempo. Para o treinamento contínuo, utilizou-se um sistema de aprendizado incremental que atualizasse o modelo de NLP com novos dados periodicamente, sem a necessidade de re-treinar o modelo inteiro. A complexidade de gerenciar a infraestrutura de Kafka e garantir a escalabilidade e a baixa latência do processamento de logs foi significativa e requereria experimentação contínua.A integração de análises comportamentais e algoritmos de aprendizado de máquina foi explorada como uma tentativa para melhorar a detecção de ameaças. A coleta de dados de acesso e a subsequente análise por modelos de aprendizado de máquina, incluindo redes neurais convolucionais (CNNs) e algoritmos de detecção de anomalias, foram postos como possível soluções, que visavam identificar padrões de comportamento suspeitos. A seleção desses modelos foi baseada na sua capacidade de processar grandes volumes de dados e aprender com eles. No entanto, o sucesso da aplicação desses modelos levantava algumas incertezas, dada a dependência de conjuntos de dados de treinamento extensos e representativos. Além disso, a capacidade dos atacantes de adaptar suas estratégias para contornar os padrões aprendidos pelos modelos introduziu uma variável, questionando a capacidade de resposta e adaptação contínua dos algoritmos em um ambiente de ameaças com mudanças constantes. METODOLOGIA: Para superar os desafios, delineou-se uma metodologia de desenvolvimento focada em atividades experimentais. Essa metodologia envolveu uma série de ações e experimentos específicos para tratar cada barreira identificada, com testes rigorosos para confirmação dos resultados.Para os algoritmos de criptografia customizados, as atividades experimentais incluíram a análise de performance, onde se implementaram protótipos de algoritmos de criptografia AES-GCM com e sem aceleração por hardware (AES-NI). Mediu-se a latência e o throughput em diferentes cenários de carga. Desenvolveu-se um módulo em Rust para operações críticas de criptografia e criaram-se bindings com Python utilizando FFI. Realizaram-se testes de integração para verificar a segurança e desempenho, além de auditorias de segurança para identificar possíveis vazamentos de memória e vulnerabilidades Os experimentos e testes envolveram benchmarking para comparar o desempenho entre implementações com e sem aceleração por hardware, testes de carga para avaliar a estabilidade e eficiência do módulo de criptografia, e testes de integração para monitorar o uso de memória e identificar possíveis vulnerabilidades.Para a rotação automática de chaves e a aplicação de políticas dinâmicas no AWS KMS, implementou-se um cache distribuído utilizando Redis para armazenamento temporário de chaves e políticas. Criaram-se fluxos de trabalho com Step Functions para a rotação de chaves e aplicação de políticas dinâmicas, e configurou-se AWS SNS para notificação em tempo real sobre atualizações de chaves. Os experimentos e testes incluíram a verificação da consistência das chaves e políticas em diferentes instâncias de serviço após a rotação, medição do tempo de propagação das atualizações de chaves e políticas, e avaliação da sincronização de políticas de acesso em tempo real, garantindo que não ocorressem inconsistências.No desenvolvimento de funções Lambda escaláveis, implementou-se um SQS para gerenciar requisições de autenticação, combinada à utilização de base de dados NoSQL para armazenamento de estado entre execuções de funções Lambda, e implementou-se um cache local para armazenar temporariamente as credenciais validadas. Os experimentos e testes incluíram a simulação de cenários com grande volume de requisições para avaliar a capacidade de escalabilidade das funções Lambda, verificação do tempo de execução das funções Lambda em diferentes cargas de trabalho, e avaliação da consistência do estado armazenado e do cache local, garantindo a integridade dos dados.Para o sistema de logging com análise comportamental em tempo real, implementou-se Apache Kafka para ingestão e processamento em tempo real dos logs, utilizando spaCy para análise de linguagem natural com um pipeline de pré-processamento otimizado. Desenvolveu-se um sistema de aprendizado incremental para atualização contínua do modelo de NLP. Os experimentos e testes incluíram o processamento de grandes volumes de logs para avaliar a capacidade de Kafka e do pipeline de NLP, validação da precisão do modelo de NLP com dados reais e atualização incremental, e medição da latência do sistema de logging e análise comportamental em tempo real.Configurou-se API Gateway para rotear requisições de autenticação e implementou-se um sistema de controle de acesso dinâmico utilizando AWS IAM e Lambda. As políticas IAM foram ajustadas em tempo real com base em análises comportamentais e avaliação contínua de risco, utilizando Step Functions para orquestrar mudanças. Utilizou-se AWS CloudWatch para monitoramento contínuo e ajuste dinâmico de políticas. Os experimentos e testes incluíram a verificação da propagação instantânea e segura das políticas de acesso, avaliação da latência e eficácia do sistema de avaliação de risco em tempo real, e auditoria de segurança para garantir que as políticas dinâmicas não introduzissem vulnerabilidades. INFORMAÇÃO COMPLEMENTAR: A adoção de um sistema de login unificado utilizando OpenID Connect na AWS representa um avanço significativo na forma como os usuários finais acessam serviços bancários através de diferentes canais. Ao centralizar o processo de autenticação por meio do Amazon Cognito User Pool, configurado para autenticar contra o diretório LDAP do banco, o projeto não só simplificou o acesso para os usuários, eliminando a necessidade de múltiplas credenciais, como também elevou os padrões de segurança e desempenho em comparação com sistemas de autenticação isolados. A utilização de um Amazon Cognito Custom Authentication Flow e triggers de Lambda para a interação com o diretório LDAP foi fundamental para alcançar esse objetivo, permitindo uma integração segura e eficaz que facilita a autenticação dos usuários e a emissão de tokens JWT.A segurança do sistema foi ainda mais fortalecida pelo uso do AWS KMS para criptografar os tokens gerados, juntamente com o desenvolvimento de funções Lambda específicas para validar as credenciais dos usuários e gerar tokens de acesso. Essas medidas, combinadas com a implementação de um API Gateway para gerenciar as solicitações de autenticação e adicionar controles de acesso com políticas IAM, contribuíram para uma arquitetura robusta, capaz de lidar com picos de acesso sem comprometer a performance. A escalabilidade automática proporcionada pelo AWS Lambda e Amazon Cognito, juntamente com a segurança garantida pela criptografia e pelos testes de penetração bem-sucedidos, demonstram o sucesso do projeto em oferecer uma solução eficiente e segura para o acesso a serviços bancários online. RESULTADO ECONÔMICO: Retenção de clientes, maior nível de segurança em transações financeiras e no acesso a contas PF e PJ. RESULTADO INOVAÇÃO: Novas compreensões quanto ao desenvolvimento e aplicação de mecanismos avançados de autenticação multifatorial, de biometria etc., elevação dos níveis de segurança, unificação de jornadas de login e autenticação. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27834;2023;CNPJ: 17298092000130 RAZÃO SOCIAL :BANCO ITAU BBA S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, com carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.22-1/00 PORTE Demais ID EMPRESA/ANO :27834;NÚMERO: 9 ID ÚNICO: 109720 NOME: SOLUÇÕES AVANÇADAS EM CLOUD PARA ESTRUTURAÇÃO DE SERVIÇOS BANCÁRIOS DISTRIBUIDOS DESCRIÇÃO: Esta linha de pesquisa tem como objetivo obter compreensões técnicas para melhorar a eficiência, confiabilidade e escalabilidade de sistemas distribuídos, através da adoção de estratégias inovadoras de gerenciamento de dados, como CRDTs para resolução de conflitos, particionamento horizontal combinado com réplicas para tolerância a falhas, e a aplicação de protocolos de atomicidade e idempotência, como o uso de logs de transações e o protocolo de commit de duas fases. Além disso, explora-se a integração da tecnologia blockchain com o uso de contêineres Docker e o algoritmo de consenso Proof of Authority para aprimorar a segurança e a flexibilidade operacional. Essas abordagens visam assegurar que sistemas distribuídos possam gerenciar dados de forma eficaz, mantendo a consistência e a disponibilidade em ambientes com atualizações concorrentes e potenciais falhas.Anteriormente, eram enfrentados desafios consideráveis relacionados à sincronização de dados e à tolerância a falhas, que comprometiam sua disponibilidade e escalabilidade. A dificuldade em assegurar a consistência de dados em cenários de atualizações concorrentes, juntamente com a vulnerabilidade a falhas inesperadas, limitava o desempenho e a confiabilidade desses sistemas. A ausência de estratégias eficientes para gerenciar a distribuição e a replicação de dados, bem como para garantir a atomicidade e a idempotência das transações, representava obstáculos significativos para a expansão e a eficiência operacional.Espera-se que os sistemas distribuídos alcancem um novo patamar de eficiência e robustez. A adoção de CRDTs e estratégias de particionamento horizontal com réplicas promete melhorar a disponibilidade e a escalabilidade, enquanto minimiza a latência adicional e assegura a convergência para um estado consistente. A implementação de logs de transações, checkpoints e o protocolo de commit de duas fases deve fortalecer a atomicidade e a idempotência, aumentando a confiabilidade do sistema mesmo diante de falhas. Além disso, a integração da tecnologia blockchain, com ênfase na flexibilidade operacional e na segurança das transações, abre caminhos para a inovação em sistemas distribuídos.O marco crítico deste projeto foi a implementação bem-sucedida e a integração de estratégias de gerenciamento de dados inovadoras, especificamente o uso de CRDTs (Conflict-free Replicated Data Types) para a resolução de conflitos, o particionamento horizontal com réplicas para aumentar a tolerância a falhas, e a aplicação de mecanismos de atomicidade e idempotência, como logs de transações e o protocolo de commit de duas fases. Este ponto de verificação representou uma mudança fundamental na abordagem de gestão de sistemas distribuídos, marcando a transição para uma nova fase de eficiência, confiabilidade e escalabilidade. A integração bem-sucedida dessas estratégias indicou a capacidade do projeto de superar desafios anteriores relacionados à sincronização de dados e tolerância a falhas, estabelecendo uma base sólida para a continuidade do desenvolvimento e aprimoramento do sistema.Além disso, a exploração da tecnologia blockchain, em conjunção com o uso de contêineres Docker e o algoritmo de consenso Proof of Authority, emergiu como um marco crítico, destacando inovações significativas em segurança e flexibilidade operacional. Esta etapa foi crucial para demonstrar o potencial de tais tecnologias em fortalecer a integridade e a confiabilidade dos sistemas distribuídos, ao mesmo tempo em que oferece novas possibilidades para a gestão eficaz de dados em cenários complexos de atualizações concorrentes e falhas potenciais. Este marco não apenas sinalizou a superação de limitações prévias, mas também pavimentou o caminho para futuras inovações e aprimoramentos na gestão de sistemas distribuídos, alinhando-se com os objetivos de longo prazo do projeto de alcançar um patamar superior de eficiência e robustez. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Sistemas,  dados,  distribuídos,  transações,  réplicas,  desenvolvimentos,  consistência,  estratégias,  falhas,  CRDTs. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para resolver conflitos de dados sem intervenção externa, desenvolveu-se um modelo baseado em CRDTs (Conflict-free Replicated Data Types). O experimento envolveu a simulação de atualizações concorrentes em réplicas de dados distribuídas, utilizando um cluster de servidores distribuídos em três data centers distintos. Cada nó aplicou operações de escrita e leitura concorrentes, incluindo incrementos, decrementos e atualizações de registros em um banco de dados distribuído. Realizaram-se 10.000 operações concorrentes em um período de 24 horas.Os resultados mostraram que todas as réplicas convergiram para um estado consistente, apesar das atualizações concorrentes. A consistência eventual foi alcançada em 98,5% das operações dentro de um intervalo de latência de 200 ms a 500 ms, sendo adequada para aplicações onde a tolerância a latências temporárias é aceitável. Observou-se que, embora a consistência eventual não garantisse imediatismo na uniformidade dos dados, ela foi eficaz em cenários onde a latência não era crítica.Estudou-se o impacto de diferentes estratégias de particionamento de dados e tolerância a falhas sobre a performance e a confiabilidade de sistemas distribuídos. Desenvolveu-se um sistema de teste que aplicava particionamento horizontal de dados, distribuindo-os por diversos nós, cada um responsável por uma fração do conjunto de dados. Foi construído um mecanismo de réplica para garantir a tolerância a falhas, acompanhado de benchmarks para avaliar o desempenho sob diferentes cenários de carga e falhas simuladas. O sistema foi testado com um conjunto de dados de 1 TB, particionado em 10 nós, cada um com 100 GB de dados.Realizaram-se simulações de falhas em 20% dos nós, e a carga de trabalho variou de 1.000 a 10.000 operações por segundo. A estratégia de particionamento horizontal, combinada com a réplica de dados, mostrou-se eficiente na melhoria da disponibilidade e da escalabilidade do sistema, com uma disponibilidade de 99,9% e uma redução de latência de 30% em comparação com sistemas sem particionamento. No entanto, o aumento na quantidade de réplicas introduziu latência adicional nas operações de escrita, devido à necessidade de sincronização entre as réplicas. A análise revelou a importância de um balanceamento cuidadoso entre a quantidade de réplicas e a distribuição de carga para otimizar tanto a performance quanto a confiabilidade do sistema.Foi desenvolvido um sistema de rastreamento de transações, utilizando identificadores únicos e uma tabela de controle de estado. Os experimentos envolveram 5.000 transações concorrentes em um ambiente de teste com 5 nós distribuídos. A aplicação de timestamps únicos e logs de transações foi eficaz na prevenção de corrupção de dados, garantindo que apenas a versão mais recente de um dado fosse considerada válida, com uma taxa de sucesso de 99,7%.O sistema de rastreamento de transações demonstrou ser essencial para evitar a duplicação de operações, assegurando que cada transação fosse processada uma única vez, mesmo sob condições de alta carga e instabilidade de rede. A duplicação de operações foi reduzida em 95% em comparação com sistemas sem rastreamento.Foi levado a experimento o protocolo de commit de duas fases (2PC) dentro de um middleware de transação distribuída. Os testes envolveram a execução de 1.000 transações distribuídas entre 10 bancos de dados, avaliando a latência e a consistência dos dados. A latência média das transações foi de 300 ms, com um desvio padrão de 50 ms. Os testes revelaram que, embora o 2PC aumentasse a latência devido à necessidade de comunicação adicional e mecanismos de espera entre os sistemas, ele ofereceu uma garantia robusta de atomicidade em transações distribuídas, com uma taxa de sucesso de 99,8%. A coordenação eficaz entre os sistemas para alcançar um consenso antes de efetivar as mudanças foi essencial para evitar estados inconsistentes entre os bancos de dados envolvidos. DESAFIO TECNOLÓGICO: Garantir a consistência de dados em um ambiente distribuído, especialmente ao lidar com atualizações concorrentes, apresentou desafios significativos. A aplicação de CRDTs (Conflict-free Replicated Data Types) enfrentou dificuldades na implementação devido à complexidade de garantir que todas as réplicas convergissem para um estado consistente. A latência variável entre os nós e a possibilidade de falhas de rede tornaram a consistência eventual um objetivo incerto.A hipótese de solução envolveu a implementação de algoritmos de sincronização mais robustos, como o Algoritmo de Sincronização de Vetores (Vector Clocks), e a utilização de técnicas de gossip para disseminar atualizações de maneira mais eficiente. Além disso, propôs-se a utilização de uma camada de middleware que monitorasse e ajustasse dinamicamente os parâmetros de sincronização com base na carga de rede e na latência observada. No entanto, a eficácia desses algoritmos em cenários de alta carga e falhas frequentes permaneceu uma incerteza, especialmente em ambientes com topologias de rede complexas e dinâmicas.O particionamento horizontal de dados e a implementação de mecanismos de réplica para garantir a tolerância a falhas apresentaram desafios significativos. A distribuição desigual da carga entre os nós resultou em hotspots que afetaram o desempenho do sistema. Além disso, a sincronização das réplicas introduziu latências adicionais nas operações de escrita, comprometendo a performance.A hipótese de solução sugeriu a utilização de algoritmos de balanceamento de carga dinâmicos, como o Algoritmo de Consistência de Hash Dinâmica (Dynamic Consistent Hashing), que redistribuíssem os dados de maneira mais equitativa em tempo real. Propôs-se também a implementação de técnicas de sharding adaptativo, onde a granularidade dos shards poderia ser ajustada dinamicamente com base na carga de trabalho. No entanto, a complexidade de implementar e testar esses algoritmos em um ambiente de produção levantou dúvidas sobre sua viabilidade prática, especialmente em termos de sobrecarga computacional e impacto na latência das operações.Garantir a atomicidade e idempotência das transações distribuídas foi outro desafio crítico. A aplicação de timestamps únicos e logs de transações enfrentou problemas de precisão e sincronização, especialmente em ambientes com alta latência de rede. A hipótese de solução propôs a utilização de relógios lógicos, como o Algoritmo de Relógio de Lamport, e a implementação de um protocolo de sincronização de tempo mais preciso entre os nós, utilizando técnicas de NTP (Network Time Protocol) aprimoradas com PTP (Precision Time Protocol).Além disso, sugeriu-se a implementação de um sistema de checkpointing distribuído que permitisse a recuperação de estados consistentes em caso de falhas. Contudo, a precisão e a escalabilidade desses relógios lógicos em um ambiente distribuído de larga escala permaneceram incertas, levantando dúvidas sobre a capacidade de garantir a atomicidade e idempotência de maneira consistente, especialmente em cenários com alta variabilidade de latência de rede.A implementação do algoritmo de consenso Proof of Authority (PoA) apresentou desafios relacionados à segurança e à integridade das transações. Embora o PoA oferecesse vantagens em termos de velocidade e consumo de recursos, a centralização do poder de validação em um número limitado de nós levantou preocupações sobre vulnerabilidades a ataques e falhas de segurança.A hipótese de solução envolveu o desenvolvimento de um sistema de rotação de autoridades, onde os nós de validação seriam selecionados de maneira rotativa com base em critérios de performance e confiabilidade.No entanto, a eficácia desses mecanismos em mitigar riscos de segurança sem comprometer a performance permaneceu uma incerteza. A complexidade adicional introduzida pela rotação de autoridades e pela verificação contínua das ações dos nós poderia afetar negativamente a eficiência do sistema. METODOLOGIA: Para garantir a consistência de dados em ambientes distribuídos, foram implementados algoritmos de sincronização robustos, como o Algoritmo de Sincronização de Vetores (Vector Clocks). Além disso, desenvolveu-se uma camada de middleware para monitoramento dinâmico da latência de rede e ajuste dos parâmetros de sincronização. Os experimentos delineados incluíram a criação de um ambiente de testes simulado com diferentes topologias de rede e variações de latência para avaliar a eficácia dos algoritmos de sincronização. Executaram-se testes de carga para observar o comportamento dos dados replicados sob condições de alta concorrência e falhas de rede. Para confirmação, realizaram-se testes de consistência eventual, verificando se todas as réplicas convergiam para o mesmo estado após um período de tempo, e analisaram-se logs de transações para identificar conflitos e medir o tempo de resolução dos mesmos.No que tange ao particionamento de dados e à tolerância a falhas, foi implementado o Algoritmo de Consistência de Hash Dinâmica, e desenvolveram-se técnicas de sharding adaptativo para ajustar a granularidade dos shards com base na carga de trabalho. Foram simulados diferentes cenários de carga para testar a eficácia dos algoritmos de balanceamento de carga e sharding adaptativo, bem como o monitoramento do desempenho do sistema em termos de latência, throughput e distribuição de carga entre os nós. Para confirmação, realizaram-se testes de desempenho com cargas de trabalho variáveis para avaliar a distribuição de carga e identificar hotspots, além de analisar logs de operações de escrita e leitura para medir a latência introduzida pela sincronização de réplicas.Para garantir a atomicidade e idempotência em transações distribuídas, implementaram-se relógios lógicos, como o Algoritmo de Relógio de Lamport, e protocolos de sincronização de tempo aprimorados com NTP e PTP. Desenvolveu-se também um sistema de checkpointing distribuído para recuperação de estados consistentes em caso de falhas. Os experimentos incluíram a criação de um ambiente de testes distribuído para avaliar a precisão e a escalabilidade dos relógios lógicos, bem como a simulação de falhas de rede e nós para testar a eficácia do sistema de checkpointing na recuperação de estados consistentes. Para confirmação, realizaram-se testes de precisão de timestamps em diferentes cenários de latência de rede e verificou-se a atomicidade e idempotência das transações através da análise de logs de transações e estados recuperados.A criação do protocolo de commit de duas fases (2PC) foi otimizada através da redução de etapas de comunicação e da implementação de técnicas de pré-commit. Desenvolveu-se um protocolo híbrido combinando 2PC com técnicas de consenso baseadas em Paxos ou Raft. Os experimentos incluíram a simulação de transações distribuídas para avaliar a latência e a eficiência do protocolo 2PC otimizado, além de testes de carga para medir o impacto das otimizações na performance do sistema. Para confirmação, realizaram-se testes de latência de transações em diferentes cenários de carga e falhas de rede, além de analisar logs de transações para verificar a consistência dos dados e a eficácia das otimizações.Para o desenvolvimento do algoritmo de consenso Proof of Authority (PoA), foi desenvolvido um sistema de rotação de autoridades baseado em critérios de performance e confiabilidade. Os experimentos incluíram a avaliação da performance do sistema de rotação de autoridades em diferentes cenários de carga e confiabilidade dos nós. Para confirmação, realizaram-se testes de segurança para identificar vulnerabilidades e medir a eficácia dos mecanismos de verificação e auditoria, além de analisar logs de performance e confiabilidade dos nós para avaliar a eficácia do sistema de rotação de autoridades. INFORMAÇÃO COMPLEMENTAR: O desenvolvimento desta linha de pesquisa possibilita avanços significativos no desenvolvimento e estruturação de sistemas distribuídos na empresa, destacando-se o uso de CRDTs para uma eficaz resolução de conflitos e garantia de consistência de dados sem intervenção externa, mesmo em cenários de atualizações concorrentes. A adoção de estratégias de particionamento horizontal e réplicas aumenta a tolerância a falhas e a escalabilidade, enquanto a implementação de logs de transações e o protocolo de commit de duas fases fortalecem a atomicidade e a idempotência das operações. Além disso, a integração da tecnologia blockchain, com o uso de contêineres Docker e o algoritmo de consenso Proof of Authority, introduz uma nova dimensão de segurança e flexibilidade operacional. RESULTADO ECONÔMICO: A implementação de CRDTs e réplicas diminui custos operacionais ao melhorar eficiência e escalabilidade dos sistemas, reduzindo a necessidade de manutenção e otimizando recursos. RESULTADO INOVAÇÃO: A integração de CRDTs e blockchain aprimora a gestão de dados e segurança em sistemas distribuídos, possibilitando inovações e aplicações mais robustas e seguras. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 1 ID ÚNICO: 109721 NOME: EXPERIMENTAÇÃO DE NOVAS TECNOLOGIAS PARA OTIMIZAÇÃO DE PAGAMENTOS E DIGITALIZAÇÃO DE SERVIÇOS BANCÁRIOS DESCRIÇÃO: As atividades de Pesquisa, Desenvolvimento e Inovação (P,D&I) focaram na criação de soluções tecnológicas a serem validadas científica e tecnologicamente. A pesquisa e experimentação foram direcionadas para evoluir as limitações existentes em termos de eficiência, consistência de dados e segurança, experimentando arquiteturas não utilizadas anteriormente. As propostas tecnológicas incluíram a implementação de uma arquitetura de componentes reativos e SPA, desenvolvimento de microserviços escaláveis, ferramentas de autenticação multifator e uma API com conteinerização e orquestração, visando estabelecer novas diretrizes para o desenvolvimento de soluções tecnológicas. O objetivo desta linha de pesquisa foi demonstrar a viabilidade técnica do desenvolvimento e integração de soluções avançadas para otimização de pagamentos e digitalização de serviços bancários.Especificamente, buscou-se melhorar a correção autônoma de repasses incorretos por meio da implementação de uma funcionalidade de autosserviço baseada em um aplicativo de página única (SPA) com arquitetura de componentes reativos. Além disso, foi desenvolvida uma arquitetura de microserviços com backend assíncrono e frontend com renderização no lado do cliente para capturar operações de clientes externos de forma eficiente.Outro foco do projeto foi a digitalização do segmento Private, onde se testou a hipótese de que uma ferramenta com autenticação multifator e governança adequada poderia proporcionar maior autonomia e controle aos clientes no cadastro de pessoas de confiança. Para isso, foi prototipada uma ferramenta com frontend modular e backend distribuído, utilizando comunicação assíncrona entre serviços. Adicionalmente, buscou-se estruturar uma plataforma para oferecer transferências internacionais como serviço, criando uma API específica com técnicas de conteinerização e orquestração de clusters de contêineres, visando garantir escalabilidade e eficiência na troca de mensagens.Os desafios enfrentados incluíram a sincronização de dados em tempo real na arquitetura de microserviços e a manutenção da consistência dos dados no SPA. Problemas de latência e inconsistência de dados também foram observados na integração com APIs de permissionamento. A implementação de autenticação multifator apresentou complexidades na integração segura e eficiente de diferentes formas de autenticação.O marco crítico do projeto surgiu como um ponto de decisão crucial, onde os desafios enfrentados na sincronização de dados em tempo real e na manutenção da consistência dos dados em arquiteturas complexas de microserviços e Single Page Applications (SPA) ameaçavam a viabilidade das soluções tecnológicas propostas. Este momento representou um teste significativo para a equipe de desenvolvimento, pois a eficiência, a consistência de dados e a segurança são fundamentais para a otimização de pagamentos e a digitalização de serviços bancários. A falta de otimização nessas áreas não só comprometeria a funcionalidade e a experiência do usuário final, mas também poderia resultar em falhas de segurança críticas, colocando em risco a integridade dos sistemas bancários digitais.Após os desenvolvimentos, espera-se alcançar interfaces mais responsivas e eficientes, maior segurança e autonomia para os usuários, e APIs que oferecessem menor latência e maior eficiência em operações de câmbio. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Autosserviço,  Componentes reativos,  Gerenciamento de estado,  Microserviços,  Escalonamento horizontal,  Sincronização de dados,  Governança,  Conteinerização,  Orquestração de contêineres,  Protocolo gRPC. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Foi testada a hipótese de que a implementação de uma funcionalidade de autosserviço baseada em um aplicativo de página única (SPA) utilizando uma arquitetura de componentes reativos poderia melhorar a correção autônoma de repasses incorretos por clientes. Para isso, foi prototipado um SPA utilizando uma arquitetura baseada em componentes reativos, implementados com uma linguagem de programação funcional. A reatividade foi avaliada quanto à sua capacidade de permitir atualizações dinâmicas na interface sem a necessidade de recarregamento da página, visando melhorar o tempo de resposta. Técnicas de gerenciamento de estado centralizado foram aplicadas para garantir a consistência dos dados na interface do usuário.Levou-se a experimento a ideia de que uma arquitetura de microserviços com backend assíncrono e frontend com renderização no lado do cliente poderia capturar operações de clientes externos boletadores de forma eficiente, independente da tecnologia utilizada pelo cliente. Para validar essa hipótese, foi criado um componente de software com uma arquitetura de microserviços, onde o backend foi estruturado usando um modelo de servidor assíncrono baseado em eventos, capaz de lidar com um grande número de conexões simultâneas. O frontend utilizou uma abordagem de renderização no lado do cliente com virtual DOM, visando uma interação mais fluida e responsiva. A integração com APIs de permissionamento foi realizada através de um protocolo de comunicação RESTful, consumindo dados de um banco de dados relacional com transações ACID para garantir a consistência dos dados. A computação em nuvem foi empregada para criar microserviços responsáveis pela gestão dos permissionamentos de câmbio, utilizando técnicas de escalonamento horizontal automático para lidar com picos de demanda. Durante os testes de carga, a eficiência e a escalabilidade da arquitetura de microserviços foram confirmadas, validando a hipótese. Contudo, desafios de sincronização de dados em tempo real foram identificados, exigindo a implementação de mecanismos de replicação de dados e otimizações nos protocolos de comunicação.Para a digitalização do segmento Private, foi testada a hipótese de que a implementação de uma ferramenta com autenticação multifator e governança adequada poderia proporcionar maior autonomia e controle aos clientes ao cadastrar pessoas de confiança para movimentar suas contas. Foi então realizada a prototipação de uma ferramenta com frontend desenvolvido utilizando um modelo de componentes modulares com shadow DOM para encapsulamento de estilos e backend estruturado com uma arquitetura de serviços distribuídos utilizando um barramento de mensagens para comunicação assíncrona entre serviços. A autenticação multifator foi implementada utilizando um protocolo de segurança robusto, combinando senha, token e biometria. A autorização foi gerenciada através de políticas de acesso definidas em um servidor de identidade que utiliza tokens JWT (JSON Web Tokens) para garantir a segurança das transações.Para estruturar uma plataforma para oferecer transferências internacionais como serviço, foi testada a hipótese de que a criação de uma API específica, utilizando técnicas de conteinerização e orquestração com clusters de contêineres, poderia garantir escalabilidade e isolamento de processos, proporcionando menor latência e maior eficiência na troca de mensagens. Para testar essa hipótese, foi criada uma API utilizando técnicas de conteinerização e orquestração com clusters de contêineres. A comunicação entre os serviços foi gerenciada por um protocolo de alta performance baseado em gRPC, visando menor latência e maior eficiência na troca de mensagens. A integração com serviços de gerenciamento de eventos, como sistemas de mensageria distribuída baseados em logs, foi fundamental para criar novos recursos para os serviços de negociação de câmbio, possibilitando a detecção e resposta a eventos em tempo real. DESAFIO TECNOLÓGICO: A arquitetura de microserviços apresentou problemas significativos na sincronização de dados em tempo real. A natureza distribuída dos microserviços resultou em inconsistências temporárias de dados, especialmente durante picos de demanda. A latência na propagação de mudanças entre diferentes instâncias de serviços e a dificuldade em garantir a consistência eventual dos dados foram questões críticas. Para mitigar esses problemas, levantou-se a hipótese de implementar um sistema de replicação de dados em tempo real utilizando Apache Kafka para mensageria distribuída. A integração desse sistema com a arquitetura existente e a garantia de baixa latência e alta consistência em um ambiente distribuído permaneceram desafios significativos, cuja eficácia ainda precisava ser validada em cenários de produção.O gerenciamento de estado centralizado no Single Page Application (SPA) enfrentou dificuldades na manutenção da consistência dos dados entre diferentes componentes reativos. A complexidade aumentou com a necessidade de gerenciar atualizações concorrentes e interações complexas entre componentes, resultando em estados inconsistentes e comportamentos inesperados na interface do usuário. A sincronização de estado entre o frontend e o backend também introduziu desafios adicionais, especialmente em condições de rede instáveis. Para resolver esses problemas, foi proposto o uso da biblioteca Redux para gerenciamento de estado, combinada com técnicas de controle de concorrência como optimistic updates e versionamento de estado. No entanto, a implementação dessas técnicas em um sistema altamente interativo e dinâmico poderia introduzir novos overheads de desempenho e complexidades, cuja eficácia ainda precisava ser rigorosamente testada.A integração com APIs de permissionamento, utilizando um protocolo de comunicação RESTful, encontrou problemas de latência e inconsistência de dados devido à complexidade das transações ACID no banco de dados. A latência nas operações de leitura e escrita e a dificuldade em manter a consistência de dados em um ambiente distribuído foram questões críticas. A necessidade de garantir a atomicidade, consistência, isolamento e durabilidade das transações complicou ainda mais a integração. Para melhorar a consistência e a latência das operações, foi sugerida a migração para CockroachDB, um banco de dados distribuído que oferece suporte a transações distribuídas. No entanto, a transição para esse novo sistema de banco de dados era uma tarefa que envolvia riscos significativos de interrupção de serviço e problemas de compatibilidade, além de exigir uma reengenharia substancial dos sistemas existentes.A implementação de autenticação multifator, combinando senha, token e biometria, apresentou desafios na integração dessas diferentes formas de autenticação de maneira segura e eficiente, sem comprometer a experiência do usuário. A coordenação entre diferentes métodos de autenticação e a garantia de que cada método fosse igualmente seguro e resistente a ataques foi uma tarefa complexa. A gestão de tokens e a sincronização de dados biométricos em tempo real também introduziram desafios de segurança e privacidade. Para simplificar a implementação, foi levantada a hipótese de utilizar o framework Auth0, que em tese, ofereceria suporte integrado para múltiplas formas de autenticação e políticas de segurança.A comunicação entre os serviços conteinerizados, gerenciada por gRPC, enfrentou desafios de latência e eficiência, especialmente em ambientes de alta concorrência e complexidade. A sobrecarga de serialização e deserialização de dados, juntamente com a gestão de conexões persistentes e a otimização do tráfego de rede, foram questões críticas. Para reduzir a latência e melhorar a eficiência, foi considerada a hipótese de introduzir técnicas de compressão de dados e otimização de protocolos de comunicação, juntamente com o uso de redes definidas por software (SDN) para gerenciar o tráfego de rede de maneira mais eficiente. METODOLOGIA: 1- Experimentos com Aplicações de Página Única (SPAs) baseadas em uma arquitetura de componentes reativos para a correção autônoma de transferências incorretas:Métricas:Medir o tempo que a interface leva para atualizar após uma interação do usuário, como clicar em um botão ou digitar um texto (responsividade).Avaliar a quantidade de dados transferidos entre o cliente e o servidor durante as atualizações da interface (eficiência).Acompanhar o número de correções autônomas bem-sucedidas feitas pelos clientes usando as SPAs em comparação com o método anterior (taxa de sucesso).2- Experimentos com uma arquitetura de microsserviços com backend assíncrono e renderização no cliente para capturar operações de clientes externos de forma eficiente:Métricas:Medir o tempo que uma solicitação do frontend leva para chegar ao backend e receber uma resposta (latência).Calcular o número de operações de clientes externos que o sistema pode processar em um determinado período (throughput).Monitorar a utilização de recursos do sistema (CPU, memória, rede) à medida que o número de solicitações aumenta (escalabilidade).3- Protótipos de uma ferramenta com autenticação multifator e governança adequada para fornecer maior autonomia e controle aos clientes no cadastro de pessoas de confiança para operar suas contas no Segmento Private:Métricas:Medir o tempo que um usuário leva para se autenticar com sucesso usando o sistema de autenticação multifator (tempo de autenticação).Acompanhar o número de tentativas de autenticação falhas e analisar os motivos do fracasso (segurança).Realizar pesquisas ou entrevistas com os usuários para coletar feedback sobre o nível de controle e autonomia proporcionado pela ferramenta (feedback do usuário).4- Protótipos de uma API para Transferências Internacionais usando técnicas de conteinerização e orquestração de clusters de contêineres:Métricas:Medir o tempo que uma solicitação leva para ser processada e uma resposta ser enviada pela API (latência).Calcular o número de solicitações que a API pode processar por segundo (throughput).Monitorar a utilização de recursos do ambiente conteinerizado (CPU, memória) à medida que o número de solicitações aumenta (escalabilidade). INFORMAÇÃO COMPLEMENTAR: À medida que o setor bancário enfrenta transformações digitais impulsionadas por mudanças nas necessidades dos clientes e a crescente tendência do open banking, as arquiteturas monolíticas tradicionais tornam-se cada vez mais inadequadas. Consequentemente, o setor está em um ponto crucial onde a adoção de microsserviços surge como uma necessidade estratégica para manter a competitividade e a eficiência operacional. No entanto, a transição exige uma abordagem cautelosa e iterativa, marcada por prototipagem e experimentações minuciosas.No mundo acelerado de hoje, agilidade e capacidade de resposta são fundamentais. O cenário do setor bancário está mudando rapidamente, com os clientes esperando experiências digitais perfeitas e novos jogadores fintech desafiando o status quo. Os microsserviços oferecem uma vantagem crucial ao permitir que os bancos desenvolvam e lancem novos recursos com uma velocidade sem precedentes. Ao contrário dos sistemas monolíticos, que muitas vezes paralisam a inovação devido à sua rigidez, os microsserviços fragmentam as aplicações em serviços independentes e gerenciáveis, permitindo ajustes ágeis em resposta às dinâmicas do mercado e às expectativas dos clientes.Além disso, os sistemas legados em muitos bancos evoluíram para se tornarem monstros monolíticos ao longo do tempo, criando uma estrutura labiríntica que é tanto dispendiosa quanto difícil de manter. Os microsserviços aliviam essa complexidade ao quebrar esses sistemas volumosos em unidades menores e compreensíveis. Esta abordagem modular não só melhora a manutenção, mas também facilita atualizações sem comprometer a estabilidade de todo o sistema.A capacidade de escalar serviços de forma independente é outra razão convincente para que os bancos adotem microsserviços. As arquiteturas tradicionais exigem a escalabilidade do sistema inteiro para atender à crescente demanda, resultando em utilização ineficiente de recursos e aumento de custos. Em contraste, os microsserviços permitem a escalabilidade direcionada de componentes específicos. Por exemplo, um banco pode escalar o microsserviço que gerencia transações móveis durante os horários de pico independentemente dos outros serviços, garantindo eficácia em custos e desempenho sustentado sob cargas pesadas.Engajar-se na prototipagem permite que os bancos experimentem diferentes estratégias arquitetônicas em um ambiente controlado antes de se comprometerem com mudanças amplamente disseminadas. Esta abordagem ajuda a identificar possíveis obstáculos, validar premissas e evitar erros custosos. Obter insights iniciais através de protótipos, portanto, reduz os riscos da jornada de migração como um todo.A mudança para uma arquitetura de microsserviços naturalmente introduz complexidades técnicas, como manter a consistência de dados em serviços distribuídos, gerenciar a comunicação entre serviços e orquestrar uma rede distribuída. Experimentando com vários mecanismos de replicação de dados e diferentes protocolos de comunicação entre serviços (como REST ou gRPC), os bancos podem encontrar o equilíbrio ideal entre desempenho e complexidade operacional. Além disso, avaliar diferentes ferramentas de descoberta e orquestração de serviços através da prototipagem permite que os bancos gerenciem efetivamente a natureza distribuída dos microsserviços.Auer, F., Lenarduzzi, V., Felderer, M., & Taibi, D. (2021). From monolithic systems to microservices: An assessment framework. Information and Software Technology, 137, 106600.Ayas, M. H., Leitner, P., & Hebig, R. (2023). An empirical study of the systemic and technical migration towards microservices. Empirical Software Engineering, 28(85).Bhole, K., Nareddy, R., & Laughridge, K. (n.d.). Opening banking through architecture re-engineering: A microservices-based roadmap. Deloitte United States.Wang, Y., Kadiyala, H., & Rubin, J. (2021). Promises and challenges of microservices: an exploratory study. Empirical Software Engineering, 26(4), 63. RESULTADO ECONÔMICO: Redução significativa do tempo de resposta para correções autônomas de transações pelos clientes. Isso melhorou a eficiência operacional, reduzindo custos com suporte e aumentando a satisfação do cliente. RESULTADO INOVAÇÃO: A inovação resultou em uma plataforma tecnológica segura e eficiente, melhorando a agilidade e escalabilidade dos serviços financeiros com autenticação robusta e técnicas de conteinerização, otimizando a gestão de transações. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;"NÚMERO: 2 ID ÚNICO: 109722 NOME: NOVAS TECNOLOGIAS PARA SOLUÇÕES DE CRÉDITO E INVESTIMENTO DESCRIÇÃO: A presente linha de pesquisa objetivou explorar a viabilidade técnica da aplicação de avançados recursos tecnológicos, tais como APIs, funções serverless e inteligência artificial, no contexto de uma plataforma de processamento. A iniciativa se inseriu no âmbito de Pesquisa, Desenvolvimento e Inovação (P,D&I), com o intuito de superar desafios prementes como a lentidão no processamento de requisições, vulnerabilidades de segurança e ineficiências na análise de crédito. No decorrer do projeto, foram conduzidos experimentos e prototipações. Essas atividades experimentais tiveram como objetivo principal investigar, de maneira empírica, a eficácia das tecnologias propostas em endereçar os problemas identificados.Para aprimorar a eficiência do sistema, adotou-se uma abordagem experimental focada na utilização de APIs e funções serverless. Os experimentos visaram quantificar o impacto dessas tecnologias na redução do tempo de resposta às requisições de acesso. Paralelamente, a segurança da plataforma foi objeto de um robusto conjunto de testes, incluindo simulações de ataques DDoS e testes de intrusão, com o propósito de identificar e corrigir vulnerabilidades. A performance da plataforma foi igualmente submetida a uma avaliação experimental, com a otimização de bancos de dados relacionais e a implementação de serviços de busca. Uma central de validações foi criada e testada, com o intuito de verificar sua eficácia em agilizar operações e elevar a satisfação do cliente. No âmbito da análise de crédito, a viabilidade e eficácia de um sistema de inteligência artificial foram testadas, visando avaliar sua precisão nas decisões de crédito e o potencial para redução do risco de inadimplência.Os desafios encontrados durante a fase experimental, tais como o aumento da latência sob carga extrema e a identificação de vulnerabilidades de segurança críticas, forneceram insights valiosos para o aprimoramento contínuo do projeto. A complexidade e latência introduzidas pela centralização das validações e pelos modelos de machine learning foram meticulosamente analisadas, contribuindo para o refinamento das soluções propostas.O marco crítico desta linha de pesquisa manifestou-se ao confrontar a escalabilidade e eficiência das funções serverless sob cenários de alta carga, juntamente com a necessidade de equilibrar a segurança, a integridade e a performance da plataforma. Esse ponto decisivo foi evidenciado durante os experimentos que simularam altas cargas de requisições, revelando um aumento significativo na latência que ultrapassava os limites aceitáveis de desempenho, especialmente acima de 20.000 requisições por minuto. Essa observação apontou para uma limitação crítica na arquitetura puramente serverless, sugerindo a necessidade de uma abordagem híbrida que pudesse manter a eficiência em condições normais de uso, mas também escalasse de maneira eficaz sob demanda extrema.Além disso, apesar de a maioria dos ataques ter sido mitigada com sucesso, a identificação de vulnerabilidades críticas em componentes específicos, como a interface de administração, destacou a necessidade de melhorias substanciais para assegurar a externalização segura da plataforma. A complexidade introduzida pela central de validações, embora efetiva em reduzir atritos e melhorar a experiência do usuário, resultou em um aumento na latência do sistema, especialmente em cenários com múltiplas validações simultâneas, marcando outro ponto crítico que exigia atenção.As expectativas pós-desenvolvimento, fundamentadas nos resultados experimentais, incluíam uma plataforma significativamente mais eficiente, segura e com performance otimizada. A implementação experimental de modelos de machine learning antecipava decisões de crédito mais precisas, com um compromisso com a transparência das decisões automatizadas. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Experimental,  dados,  carga,  processamento,  operações,  desenvolvimento,  hipótese,  otimização. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para validar a hipótese de que a utilização de APIs poderia melhorar a eficiência do sistema, experimentos foram realizados criando funções serverless que processavam requisições de acesso. Utilizou-se uma ferramenta para gerar múltiplas requisições simultâneas, simulando cenários de alta carga. As métricas de desempenho foram coletadas para analisar o tempo de resposta, a latência e a taxa de erro. Observou-se uma redução de 40% no tempo de resposta em comparação com a abordagem tradicional. Observou-se também que, em cenários de carga extrema (acima de 20.000 requisições por minuto), a latência aumentou significativamente, revelando a necessidade de otimizações ou de uma arquitetura híbrida que combine funções serverless com servidores dedicados.A segurança na externalização da plataforma foi outra área de experimentação. Implementações para gerenciar a autenticação de usuários e proteção contra ataques DDoS foram testadas. Testes de intrusão foram realizados para simular ataques cibernéticos e avaliar a robustez das defesas. A plataforma resistiu a maioria dos ataques simulados, mas algumas vulnerabilidades críticas foram identificadas em componentes específicos, como a interface de administração. Isso indicou que melhorias adicionais eram necessárias antes de considerar a externalização segura da plataforma. A integridade dos dados foi mantida, mas o tempo de resposta aumentou durante os ataques, sugerindo a necessidade de mecanismos de mitigação mais eficazes.A performance da plataforma foi testada com a hipótese de que bancos de dados relacionais otimizados e serviços de busca poderiam melhorar significativamente o desempenho. Durante as PoCs, operações de (re)alocação de recursos foram simuladas e painéis de gestão operacional e de transferência de clientes foram gerados. Os resultados mostraram uma melhoria de 50% na velocidade de consulta e uma redução no tempo de processamento de transações. A implementação de índices complexos para melhorar a performance das consultas trouxe em um aumento significativo no tempo de escrita, o que pode impactar negativamente operações em tempo real. A utilização de CPU e memória aumentou, sugerindo a necessidade de balanceamento entre otimização de consultas e eficiência de escrita.A criação de uma central de validações foi experimentada para verificar se poderia reduzir o tempo necessário para executar operações e melhorar a satisfação do cliente. Processos de validação foram orquestrados que verificavam diversas condições antes de permitir a execução de uma operação. Os experimentos mostraram que o sistema sinalizava ao usuário a possibilidade de uma ação apenas quando todos os critérios eram atendidos, reduzindo atritos e o tempo total de execução. Além disso, a complexidade introduzida pela central de validações resultou em um aumento na latência do sistema em cenários com múltiplas validações simultâneas.Outra hipótese testada foi a de que a implementação de um sistema de inteligência artificial para análise de crédito poderia melhorar a precisão das decisões de crédito e reduzir o risco de inadimplência. Modelos de machine learning foram treinados utilizando um conjunto de dados históricos de crédito, e diversas técnicas de aprendizado supervisionado foram aplicadas para identificar padrões e prever a probabilidade de inadimplência.Os modelos foram validados utilizando um conjunto de dados de teste separado, e as métricas de desempenho, como precisão, recall e AUC-ROC, foram avaliadas. Os resultados mostraram uma melhoria na precisão das decisões de crédito em comparação com métodos tradicionais baseados em regras. No entanto, a complexidade dos modelos resultou em tempos de processamento mais longos, o que pode ser um problema em cenários de alto volume de transações. Além disso, a transparência das decisões tomadas pelos modelos de IA foi uma preocupação, sugerindo a necessidade de técnicas de explicabilidade para garantir a confiança dos usuários nas decisões automatizadas. DESAFIO TECNOLÓGICO: Observou-se um aumento na latência ocorreu devido à sobrecarga de inicialização das funções serverless e à limitação na capacidade de escalar instantaneamente para atender a picos de demanda. Além disso, a natureza stateless das funções serverless dificultou o gerenciamento eficiente de sessões de usuário e a manutenção de estado entre requisições consecutivas, contribuindo para a degradação do desempenho. A hipótese de solução foi adotar uma arquitetura híbrida que combinasse funções serverless com servidores dedicados. A ideia era utilizar funções serverless para lidar com cargas de trabalho variáveis e servidores dedicados para garantir a consistência e o desempenho em cenários de alta carga.Testes de intrusão revelaram vulnerabilidades críticas na interface de administração, que poderiam ser exploradas por atacantes para obter acesso não autorizado. Essas vulnerabilidades incluíram falhas na validação de entrada, autenticação inadequada e proteção insuficiente contra ataques de força bruta. A hipótese de solução foi implementar um sistema de defesa em profundidade, utilizando técnicas avançadas de detecção e mitigação de ataques. Isso incluiu a implementação de firewalls de aplicação web (WAF) para proteger contra ataques de injeção e cross-site scripting (XSS), sistemas de prevenção de intrusão (IPS) para detectar e bloquear atividades maliciosas, e autenticação multifator (MFA) para fortalecer a segurança de login. A integração dessas técnicas com a plataforma existente e a garantia de que todas as vulnerabilidades fossem mitigadas foram desafios significativos. A eficácia dessa solução precisou ser rigorosamente testada antes da externalização segura da plataforma.Além disso, o aumento na utilização de CPU e memória indicou que a infraestrutura existente poderia não suportar a carga adicional a longo prazo. O desafio foi encontrar um equilíbrio entre a otimização das consultas e a eficiência das operações de escrita, sem comprometer o desempenho geral do sistema. A hipótese de solução foi implementar técnicas de particionamento de dados (sharding) e replicação para distribuir a carga de leitura e escrita entre diferentes nós do banco de dados. Essa abordagem permitiria que diferentes partes do banco de dados fossem gerenciadas independentemente, melhorando a escalabilidade e o desempenho. A utilização de caches distribuídos, como Redis ou Memcached, para armazenar resultados de consultas frequentes também foi considerada.A centralização das validações significava que todas as requisições precisavam passar por um ponto único de verificação, criando um gargalo. Além disso, a necessidade de realizar múltiplas validações sequenciais aumentou o tempo de processamento total, impactando negativamente a experiência do usuário. A hipótese de solução foi implementar técnicas de paralelização e otimização de processos de validação para reduzir a latência. A utilização de filas de mensagens e processamento assíncrono para gerenciar operações de validação em paralelo foi considerada. Isso permitiria que múltiplas validações fossem processadas simultaneamente, reduzindo o tempo de espera. A eficácia dessa solução dependeria da capacidade de orquestrar eficientemente os processos assíncronos e garantir a integridade dos dados.A transparência das decisões foi uma preocupação, pois modelos de machine learning são frequentemente vistos como ""caixas-pretas"", dificultando a explicação das decisões tomadas aos usuários e reguladores. A hipótese de solução foi implementar técnicas de compressão de modelos e otimização de inferência para reduzir o tempo de processamento. Técnicas como quantização, poda de redes neurais e utilização de hardware especializado poderiam ter melhorado a eficiência dos modelos. Utilizar frameworks de explicabilidade, como LIME ou SHAP, para garantir a transparência das decisões automatizadas também foi considerada. METODOLOGIA: Para lidar com os desafios técnicos encontrados nesta linha de pesquisa, foram organizadas as seguintes atividades:1- Experimentos voltados para avaliar o impacto das APIs e funções serverless na redução dos tempos de resposta a solicitações de acesso e na melhoria da eficiência geral do sistema, simulando cenários de alta carga.Métricas:Tempo de Resposta: Medido em milissegundos, essa métrica avalia o tempo que o sistema leva para responder às solicitações dos usuários.Latência: Mediu o atraso introduzido pelo sistema no processamento de solicitações, indicando sobrecarga de rede e processamento.Taxa de Erro: Acompanhou a porcentagem de solicitações falhadas, destacando possíveis gargalos ou problemas no sistema.2- Experimentos voltados para identificar e abordar vulnerabilidades associadas à externalização da plataforma, com foco em autenticação, proteção contra ataques DDoS e resistência a intrusões. Métricas:Número de Ataques Bem-Sucedidos/Falhados: Mediu a eficácia das implementações de segurança, acompanhando a taxa de sucesso de ataques DDoS e intrusões simulados.Integridade dos Dados: Avaliou se os dados permaneceram consistentes e inalterados durante e após os testes de segurança.Tempo de Resposta Sob Ataque: Monitorou a capacidade de resposta do sistema durante ataques simulados para avaliar o impacto na performance.3- Experimentos exploraram o impacto de bancos de dados relacionais otimizados e serviços de busca na melhoria da performance geral da plataforma, com foco especial na velocidade das consultas e no tempo de processamento das transações.Métricas:Velocidade da Consulta: Medido em milissegundos, essa métrica avaliou o tempo necessário para recuperar dados do banco de dados.Tempo de Processamento de Transações: Avaliou a eficiência na conclusão de operações dentro do banco de dados.Uso de CPU e Memória: Monitorou a utilização de recursos do sistema de banco de dados para identificar possíveis gargalos. Tempo de Escrita: Mediu o tempo necessário para escrever dados no banco de dados, especialmente relevante após a implementação de índices complexos para otimização de consultas.4- Experimentos com o objetivo de determinar se um sistema de validação centralizado poderia reduzir atritos, minimizar erros e melhorar a satisfação do cliente ao agilizar e acelerar as operações dos usuários. Métricas:Tempo de Conclusão da Operação: Mediu o tempo total que os usuários levam para concluir tarefas, avaliando o impacto do processo de validação centralizado.Taxa de Erros dos Usuários: Acompanhou o número de erros cometidos pelos usuários durante as operações, avaliando a eficácia das validações na prevenção de ações incorretas. Latência: Mediu o atraso introduzido pelo processo de validação, especialmente em cenários com múltiplas validações simultâneas.5- Experimentos investigaram a viabilidade e eficácia da implementação de um sistema de IA para análise de crédito, com o objetivo de melhorar a precisão das decisões e reduzir o risco de inadimplência. Métricas:Precisão: Mediu a porcentagem de previsões corretas feitas pelo modelo de IA na avaliação da solvência de crédito.Recall: Avaliou a capacidade do modelo de identificar corretamente os casos positivos, indicando sua eficácia em reconhecer possíveis aprovações de empréstimos.AUC-ROC: Avaliou o desempenho geral do modelo medindo sua capacidade de distinguir entre casos positivos e negativos (solventes vs. não solventes)Tempo de Processamento: Mediu o tempo necessário para o modelo de IA processar solicitações de crédito, crucial para cenários de alto volume. INFORMAÇÃO COMPLEMENTAR: Este projeto, focado na integração de tecnologias avançadas na plataforma de processamento de uma instituição financeira, representa um investimento crucial no crescimento futuro e na competitividade. Ao adotar tecnologias como APIs, funções serverless e inteligência artificial, o banco pode melhorar significativamente sua eficiência operacional, fortalecer as medidas de segurança e tomar decisões de crédito mais informadas. A implementação bem-sucedida dessas tecnologias se traduz em benefícios tangíveis, incluindo tempos mais rápidos de processamento de transações, redução do risco de fraudes e violações de dados, e maior precisão na identificação de clientes com capacidade de crédito, tudo contribuindo para uma experiência aprimorada do cliente, maior lucratividade e uma vantagem competitiva mais forte no cenário financeiro em evolução. Testes rigorosos, análises e refinamentos garantem que o esforço de modernização seja baseado em uma abordagem prática e orientada para resultados, minimizando riscos. Ao enfrentar os desafios identificados por esse processo, o banco demonstra um compromisso tanto com a inovação quanto com a excelência operacional, posicionando-se como líder na adoção e integração de tecnologias de ponta no setor financeiro.Adarbah, H. Y., Al-Badi, A. H., Toosy, S. G., Jajarmi, H., Bocar, A., Ancheta, R., & Ordoubadi, M. S. (2023). Banking on the cloud: Insights into security and smooth operations. Journal of Business, Communication & Technology, 2(2), 1-14.Ghule, S., Chikhale, R., & Parmar, K. (2014). Cloud computing in banking services. International Journal of Scientific and Research Publications, 4(6), 1-8.Mushtaq, M. F., Akram, U., Khan, I., Khan, S. N., Shahzad, A., & Ullah, A. (2017). Cloud computing environment and security challenges: A review. International Journal of Advanced Computer Science and Applications, 8(10).Nguyen, D. S., & Sondano, J. (2023). Resilience and stability in organizations employing cloud computing in the financial services industry. Journal of Computer and Communications, 11(4), 103-148. RESULTADO ECONÔMICO: As otimizações e a precisão aprimorada nas análises de crédito impulsionaram a eficiência operacional e a satisfação dos clientes, contribuindo para um avanço econômico significativo. RESULTADO INOVAÇÃO: Melhoria substancial na eficiência do sistema de processamento de transações, demonstrando o impacto positivo da integração de recursos tecnológicos avançados na otimização de processos operacionais. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 3 ID ÚNICO: 109723 NOME: MODERNIZAÇÃO DE PLATAFORMAS TECNOLÓGICAS A PARTIR DA APLICAÇÃO DE TECNOLOGIAS EM NUVEM DESCRIÇÃO: O objetivo desta linha de pesquisa é demonstrar a viabilidade técnica da modernização de fluxos e processos associados a produtos relacionados às áreas de investimentos. A maior parte dos processos estavam baseados em infraestrutura mainframe ou em estágios iniciais da modernização para a nuvem. Dada a complexidade inerente da convivência de sistemas híbridos, foram organizadas atividades experimentais buscando proposições a serem validadas em protótipos.São projetos constituintes desta linha de pesquisa:- Desenvolvimento de infraestrutura em nuvem para basear a construção de aplicativo, - Desenvolvimento de soluções acopladas para gerir e controlar plataforma voltada a segmentos específicos, - Desenvolvimento de processo de migração, remodelagem e melhoria do processo de abertura dos ETFs para nuvem, - Desenvolvimento de processo de migração de siglas com foco na otimização de desempenho de aplicações, - Desenvolvimento de esteira onshore com aplicação de recursos da nuvem.Como resultado, espera-se validar processo de modernização para nuvem viabilizando o desenvolvimento de novas funcionalidades e a otimização de jornadas. A premissa considerada é que sistemas em nuvem viabilizam condições de disponibilidades e escalabilidades até então não possíveis a partir das restrições da configuração existente. Busca-se, ainda, alcançar maiores níveis de segurança, de excelência operacional, de confiabilidade e de desempenho dos serviços ofertados aos clientes.Vale pontuar que os desenvolvimentos possibilitam o fortalecimento de competências em tecnologias em nuvem com estudos, pesquisas e experimentos voltados à aplicação e solução de problemas complexos.Como marco crítico, considerou-se como ponto de atenção a transição de um processo central e sensível para a infraestrutura em nuvem, em que qualquer falha poderia ter um impacto direto e imediato nos serviços ofertados aos clientes. A complexidade técnica da migração, combinada com a necessidade de assegurar a integridade e o desempenho dos dados durante e após a transição, torna este um momento decisivo. Se comprovada tecnicamente, a criação da infraestrutura em nuvem seria capaz de suportar operações críticas, enquanto falhas podem comprometer a confiança no projeto de modernização como um todo.Por fim, vale pontuar que os desenvolvimentos trouxeram o desafio quanto a gerenciar transições complexas e resolver problemas em tempo real. A habilidade de identificar e mitigar riscos rapidamente durante o processo de migração e construção de infraestrutura modernizada era essencial para garantir a continuidade dos negócios e minimizar interrupções. Porém, havia o risco de não haver validação técnica dos processos ameaçando a modernização no tempo necessário. Dificuldades e falhas neste ponto poderiam levar a atrasos, custos adicionais e uma reavaliação completa da estratégia de migração para a nuvem. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Modernização,  mainframe,  nuvem,  latência,  volumetria,  microsserviços. NATUREZA: Processo ELEMENTO TECNOLÓGICO: O objetivo desta linha de pesquisa é demonstrar a viabilidade técnica da modernização de fluxos e processos associados a produtos relacionados às áreas de gestão financeira e de investimentos, corretora e tesouraria. O desenvolvimento das atividades experimentais estudadas a partir de protótipos possibilitou um conjunto de aprendizados.Foi possível a compreensão de nova arquitetura não mais baseada em estruturas estáticas para processamento configurando-se como elemento novo recursos instanciados com microprocessamentos. Estudos levaram ao entendimento de estratégia de correlação de informações geradas em diferentes camadas, sendo que testes integrados comprovaram tecnicamente as peças componentes desta solução.Experimentações feitas permitiram comprovações para construção de novos mecanismos no processo ETL para aplicação de arquitetura no padrão hexagonal. Também foi possível entender a aplicação de arquitetura de microsserviços, hospedados no PaaS, configurando-se como elemento novo a construção para consumir dados com diferentes origens.Houve comprovação técnica da criação de novos códigos em nuvem de forma a viabilizar melhor a segregação de rede com VPC endpoint. Estudos para aplicação de ferramentas de governança Turbot e Cloudcustodian permitiram compreender armazenamento, reutilização de aplicativos e implantação de arquiteturas sem servidores. Foram feitas experimentações com funções lambdas possibilitando a construção de proposições a serem estudadas.Estudos a partir de protótipos permitiram entender a estruturação dos mecanismos de segurança, de software e de infraestrutura, incluindo nestes pipelines de gates de validação de segurança, risco e vulnerabilidade. Configurou-se como elemento novo o entendimento da possibilidade de acréscimo de camada adicional de segurança, não utilizada até então, baseada em tokens dinâmicos.Foi possível a compreensão de processo para dimensionamento dos fluxos de entradas e saídas de requisições permitindo viabilizar entendimento de comportamento dos clientes e de parceiros. Estudos e experimentos possibilitaram comprovações técnicas para criação de integração configurando-se como elemento novo a arquitetura baseada em eventos e streaming realizados através de tópicos Kafka. Foi possível compreender a abstração do log como fluxo de mensagens resultando em processamento de baixa latência e fácil suporte para diferentes fontes e consumos de dados distribuídos.Por fim, a partir dos protótipos estudados, foi possível entender mecanismo de autenticação e de convivência entre sistemas legados e em nuvem garantindo fluxo e concatenação de dados, bem como o alcance de latências de microssegundos e determinísticas. DESAFIO TECNOLÓGICO: A criação de um processo de integração e migração entre fluxos baseados em mainframe e aqueles em estruturas de nuvem apresenta riscos como incompatibilidade de tecnologias, problemas de comunicação e latência. Devido às especificidades do mainframe e das áreas de negócio envolvidas, não existem soluções prontas para serem replicadas. Um desafio adicional é a impossibilidade de construir e testar a solução em ambiente produtivo, pois isso poderia causar indisponibilidade nos sistemas e afetar o Banco como um todo. Portanto, estudos e experimentos foram organizados para encontrar elementos que componham protótipos a serem validados experimentalmente.Replicou-se, como experimento, condições de crescimento orgânico e aumento de eventos extraordinários para as plataformas. Com isso, foi possível tratar problemas de represamento de dados com erros impedindo transações. Não era conhecida estratégia para tratar tal barreira sendo levadas a experimento diferentes hipóteses. Lidou-se com desafios de lentidão do sistema na implementação do ambiente em nuvem. Buscava-se compreender problemas relacionados à volumetria de dados com a hipótese de que haveria limites máximos. Testes de estresse foram propostos buscando entendimentos para proposição de nova arquitetura para diminuir latência, incluindo tráfego de dados e seus gargalos de tempo da comunicação.Hipóteses foram tratadas buscando composição de solução auto escalável para comprovação técnica de implementação de recursos em microsserviços. Estudos e experimentos permitiram a construção e experimentação de cenários via POCs,  foi considerada a tecnologia Athena procurando tratar as incertezas sobre limitações de escalabilidade e disponibilidade quando o Quicksight importava os dados direto no Bucket S3.Experimentos foram organizados para lidar com incertezas relacionadas a não existir processo de disponibilização de dados para áreas internas com riscos à integração entre dados da plataforma legada e a nova solução. Delineamentos experimentais foram propostos buscando estratégia de integração. Estudos também se voltaram a buscar elementos para composição para lidar com quantidade massiva de dados. As propostas foram consideradas em PoCs buscando validação técnica de fluxo de dados e eficiência de seu processamento.Hipóteses foram delineadas considerando utilização de dois fatores para entendimento de composição para mandates de segurança. Experimentos se voltaram a entender cenários com duas camadas de proteção para a troca de tokens de segurança das requisições. Buscava-se comprovação técnica de hash de sessão aberto em ambiente legado como forma de garantir que a requisição partiu de solicitação de cliente.Havia que se lidar com incertezas sobre o streaming de dados das plataformas existentes. Tratou-se a hipótese de que o uso do Apache Kafka poderia garantir a integração necessária, entendendo-se que se mostrava como alternativa mais viável para garantir que essas integrações fossem assíncronas e em tempo real. Buscava-se comprovação da substituição de integração com processos batch. METODOLOGIA: A fim de avaliar a viabilidade técnica da modernização de fluxos e processos, esta linha de pesquisa foi organizada para buscar entendimentos de novas arquiteturas, abordagens de microsserviços, e mecanismos de integração para serem validados via protótipos. Para lidar com os desafios técnicos encontrados, foram organizadas as seguintes atividades:1- Estudos e testes para desenvolvimento de protótipos de arquitetura de microsserviços em nuvem.Métricas:- Tempo de resposta das transações- Facilidade de integração com sistemas legados- Escalabilidade e elasticidade do sistema- Resiliência e recuperação,  tempo médio de recuperação2- Estudos e experimentos para validar a migração de processos de ETL para arquitetura hexagonal.Métricas:- Redução no tempo de processamento de dados- Eficiência no consumo de recursos- Facilidade de manutenção e modularidade da arquitetura3- Estudos para implementação de soluções de segurança baseadas em tokens dinâmicos e validação automatizada em pipelines de segurança.Métricas:- Taxa de detecção de vulnerabilidades- Tempo de validação4- Experimentos para criação de novos mecanismos de governança e segregação de rede com VPC Endpoint.Métricas:- Redução de falhas de segurança- Tempo de resposta das requisições em ambientes isolados- Facilidade de gestão e controle de acesso5- Testes de estresse e validação da arquitetura de streaming de dados utilizando Apache Kafka.Métricas:- Latência das transações em tempo real- Taxa de sucesso no processamento de eventos distribuídos6- Estudos para desenvolvimento de soluções auto escaláveis em microsserviços, validando o desempenho em cenários de alta demanda.Métricas:- Eficiência do auto escalonamento- Consumo de recursos durante picos de utilização- Tempo de resposta em cenários críticos de carga7- Testes de carga para avaliar o desempenho e latência na comunicação entre sistemas legados e a nuvem.Métricas:- Latência média das transações entre os ambientes- Taxa de sucesso nas integrações- Impacto da comunicação híbrida no desempenho geral do sistema INFORMAÇÃO COMPLEMENTAR: Estruturas centenárias do setor bancário-financeiro ainda enfrentam desafios ao migrar a atual estrutura para bases em nuvem. Entre as principais dificuldades estão questões de segurança e privacidade, já que a migração para a nuvem introduz novas vulnerabilidades e exige medidas robustas para prevenir violações de dados e cumprir com regulamentações. A convivência com sistemas legados configura-se com uma barreira técnica, pois muitos bancos dependem de sistemas antigos que podem não ser compatíveis com ambientes de nuvem modernos,  com isso, há riscos de perda de dados, corrupção e problemas de integração. Como mitigação destes riscos, há que se construir uma governança com foco na manutenção da integridade, da disponibilidade, usabilidade e segurança dos dados durante e após a migração, exigindo estratégias e processos robustos. A gestão de custos é outro desafio, uma vez que a migração inicial pode ser cara e, sem planejamento adequado, os custos contínuos podem aumentar rapidamente. Além disso, a falta de habilidades especializadas entre as equipes em infraestrutura tanto de mainframe quanto de nuvem aumenta a complexidade dos desenvolvimentos.Apesar destes desafios, a migração das estruturas bancárias para a nuvem é inevitável. A nuvem oferece agilidade e escalabilidade, permitindo que os bancos respondam rapidamente às demandas do mercado e às expectativas dos clientes, além de facilitar a implantação de novos produtos e serviços. A adoção de tecnologias avançadas é essencial para evolução de competências como análise de dados e aplicação de novos desenvolvimentos com inteligência artificial. Viabiliza, por isso, insights baseados em dados, experiências personalizadas para os clientes e uma gestão de riscos mais eficiente. A segurança e resiliência aprimoradas também são melhoradas, com ambientes de nuvem facilitando a recuperação de desastres e a continuidade dos negócios. Por fim, a nuvem melhora a experiência do cliente, permitindo interações mais amigáveis e acesso 24/7 aos serviços bancários a partir da digitalização de jornadas.Referências bibliográficasÇ- Megargel, A., Shankararaman, V., & Walker, D. K. (2020). Migrating from monoliths to cloud-based microservices: A banking industry example. Software engineering in the era of cloud computing, 85-108.- Merizzi, N., Dhal, P., Srinivas, V., & Hazuria, S. (2022). Accelerating digital transformation in banking and capital markets with industry clouds. Deloitte Insights.- Singh, M., Tanwar, K. S., & Srivastava, V. M. (2018). Cloud computing adoption challenges in the banking industry. In 2018 international conference on advances in big data, computing and data communication systems (icABCD) (pp. 1-5). IEEE. RESULTADO ECONÔMICO: Redução de custos associados a menor utilização da plataforma legada. Aumento de receita associada à disponibilização de novas funcionalidades dos produtos. RESULTADO INOVAÇÃO: Modernização de plataformas melhorando requisitos de latência, disponibilidade, escalabilidade e segurança. Desenvolvimento de novas funcionalidades ao usuário melhorando a experiência do cliente. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 4 ID ÚNICO: 109724 NOME: NOVOS MECANISMOS DE CONVIVÊNCIA E INTEGRAÇÃO ENTRE ESTRUTURAS MAINFRAME E NUVEM DESCRIÇÃO: Esta linha de pesquisa trata da criação de processos de integração e de comunicação entre plataformas mainframe e em nuvem.Quando um sistema move seu workload para a nuvem, os sistemas que ainda não passaram por essa migração e que utilizam dados produzidos por sistema legado precisam de um mecanismo de outbound no IMS (Information Management System), tecnologia de banco de dados e sistema de transação utilizada em sistemas mainframe. Até o momento, não foi possível criar solução de outbound que atenda critérios de atendimento, de segurança, de confiabilidade e de escalabilidade.Dividiu-se este trabalho em fase síncrona e assíncrona a partir da barreira de um desenvolvimento único que lide com 950 milhões de transações por dia. Não havia conhecimento de um processo pronto para isso que alocasse uma unidade de trabalho com processadores e memória que são recursos limitados. Assim, o objetivo tecnológico era entender como criar mecanismos de comunicação incluindo elementos que não existiam em ambiente mainframe – como comunicação HTTPS e monitoração para rastreabilidade do sistema.A linha de pesquisa também se dedicou ao desenvolvimento de nova capacidade técnica, em processo de outbound, considerando a integração de serviços já modernizados de forma que pudessem ser consumidos a partir do mainframe. Foram consideradas áreas críticas, como o sistema de conta corrente. O objetivo tecnológico é evoluir mecanismos de retry e observability, de processos de autenticação, entre outros. A solução existente não conseguia absorver o crescimento de serviços modernizados, A partir do entendimento de problemas de indisponibilidade de transações foi entendida a necessidade de um processo de monitoração das milhares de transação no ambiente IMS. A solução existente também apresentava limitações pois quando foi criada não existia a necessidade de transações on-line ficarem disponíveis 24x7. Outro problema enfrentado era que a atualização feita em finais de semana e feriados levava vários minutos em função do tamanho do ambiente. Com isso, o cliente era impactado pois podia acontecer indisponibilidade de alguns serviços. Houve a compreensão que fragilidades do mecanismo de monitoração das transações inviabilizavam a solução da indisponibilidade. Além de não haver solução pré-definida para lidar com isso, havia pouca documentação sobre o processo atual dificultando entendimentos para propor nova solução. Nesse contexto, o objetivo tecnológico foi, através de atividades experimentais, construir um sistema de gestão eficiente de SLA considerando a volumetria de transações IMS.Por fim, para viabilizar processos de conexões com sistemas internos (como IMS, MQ [Message Queuing] e DB2 [Database 2) em um processo de replataforma (isto é, transferir uma plataforma para outra da forma como está), o objetivo tecnológico foi entender a emulação do mainframe. Esse processo é sempre único uma vez que reflete um sistema altamente atrelado a códigos e regras de negócios específicos. Destacaram-se, aqui, estudos para definições de transações ativas via pipeline e de regiões de processamento.Um ponto crítico dos desenvolvimentos desta linha de pesquisa estava em superar restrições e limitações do mainframe. Ainda que haja literatura correspondente, como o mainframe é um sistema fortemente acoplado, caracterizado pela forte interdependência entre seus componentes de hardware e software, não é possível pré-definir seu comportamento e as variáveis envolvidas na comunicação com ambiente em nuvem. O inverso é verdadeiro: sistemas em nuvens têm que lidar com a incerteza quanto à interoperabilidade com sistemas tão específicos e rígidos como o mainframe. Garantir ou não essa interoperabilidade teria efeitos diretos em todo o restante do desenvolvimento. Entender como fazê-los, além de viabilizar processos de migração e disponibilidade 24x7, representa um marco dos desenvolvimentos aqui colocados e um avanço no estado da arte sobre sistemas mainframe. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Mainframe,  nuvem,  processo de replataforma,  processo de outbound,  comunicação síncrona e assíncrona. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Estudos e experimentos para superar os desafios enfrentados por esta linha de pesquisa viabilizaram um conjunto de aprendizados.Houve comprovação técnica de desenvolvimento de novo processo de comunicação entre sistema mainframe e ambientes modernizados. Foram realizados estudos considerando mecanismos de se enviar a requisição para aplicação externa de forma a viabilizar a criação de mecanismo padronizado. Estudos também se voltaram a entender formas de monitoramento e criar um sistema que possa reagir rapidamente a qualquer variação de comportamento.Aprendizados permitiram entender como desenvolver aplicação que realiza a integração com conector que abstrai o acesso à API modernizada. Também houve aprendizados para criar a capacidade já presente nos servidores, mas não dentro do mainframe, como comunicação com criptografia HTTPs, consumo de guarda segura de credenciais, gestão de token, trazendo ainda o elemento inédito de credencial em cache e gestão do tempo.A partir do entendimento das limitações do processo síncrono de outbound, estudos se voltaram a entender como criar um processo de orquestração de transações. Nele, a transação inicial que recebeu o input do cliente é iniciado dentro do IMS. O elemento de novidade é a criação de uma identificação única para essa transação, assim como são guardadas informações na memória para que a transação encerre, que possa enviar uma consulta à API externa por meio de toda da infraestrutura já criada. Depois é iniciada uma segunda transação capaz de recuperar as áreas de memória de forma a retomar o processamento. Ainda como novidade, de forma que esse fluxo funcione, houve aprendizados que levaram ao desenho de solução em que a transação, ao ser chamada, aciona uma sinalização no produto que faz o controle de manter, em memória, a conectividade com o cliente enquanto a transação é encerrada. Foi possível entender, ainda, elementos para criação de token para identificador único.Estudos permitiram a proposição de processo em que aplicações tradicionais em mainframe – como as escritas em COBOL e ASSEMBLER, pudessem acessar serviços modernos. Houve entendimento, como elemento novo, de protocolo para requisições. Com isso, foi possível desenvolver integração entre o mainframe e a arquitetura orientada a serviços e ainda ampliar as capacidades dessas plataformas para interagir com tecnologias contemporâneas sem a necessidade de reescrever aplicativos legado em outra plataforma fora do mainframe – um elemento de novidade do desenvolvimento.Houve entendimento de divisão das transações com SLA 24x7 fora dos arquivos de regras buscando comprovação técnica de maior eficiência no tratamento das regras, nas interfaces on-line, nas automações e nos processamentos batch. Estudos sobre o comportamento real de cada transação levaram a entendimentos para definir funções de inclusão, exclusão, alteração e consulta. A partir de protótipos, foi possível entender que os processos precisavam se basear, ao máximo, nas exceções diante do tamanho do ambiente. Foi criado processo inédito para estruturar os processos.Aprendizados para concepção de refatoração automatizada da plataforma mainframe de forma a garantir a conversão de código fonte de uma linguagem de programação antiga para uma moderna com o diferencial de manter a lógica de negócios intacta. DESAFIO TECNOLÓGICO: Os desenvolvimentos desta linha de pesquisa lidaram com um conjunto de desafios técnicos.Havia incertezas relacionadas à própria natureza do mainframe – com restrições quanto a desempenho e velocidade de processamento transacional. Havia riscos quanto à eficiência ao acessar os dados sem incorrer em latências ou ocupações excessivas de recursos.Lidou-se com desafios quanto à transição para modelos de mensageria em sistemas distribuídos uma vez que o mainframe manipula dados de maneira posicional. Como hipótese de solução, levou-se a experimento a conversão de dados em COBOL aproveitando funções nativas buscando comprovações técnicas para lidar com JSON de forma eficiente.Lidou-se com incertezas quanto ao comportamento das estruturas envolvidas por conta da natureza síncrona do sistema. Havia riscos de aumentar o consumo de recursos que alocam memória. Ainda é incerto se será possível criar um mecanismo de alocação assíncrona.Era incerto ser possível levantar os comandos de cada uma das milhares de transações de forma a determinar um SLA preciso, registrá-lo em arquivos de regras e manter o sistema atualizado. Não havia documentação suficiente do processo existente para servir como referências.Não se sabia, a priori, quais as funções necessárias para compor todo o ecossistema do SLA – por exemplo, classificação interna de dados, variáveis compostas e arrays. Também não se sabia a linguagem REXX (Restructured Extended Executor), utilizada em sistemas mainframe, para tais ações.Sem o mapeamento do sistema existente, havia incertezas quanto aos comandos e exceções que deveriam ser consideradas. Como hipótese de solução, foram propostas diferentes lógicas que liam 8 arquivos de logs concatenas e análises decorrentes. Era necessário proceder a inclusão da submissão de jobs e fazer análise de cada um. Houve entendimento de que não havia um padrão de comportamento nos comandas emitidos,  também havia resultados difíceis de interpretar por causa dos comandos com máscara. Isso reforçava incertezas quanto ao desenvolvimento da solução.Lidou-se, ainda, com desafios associados à grande complexidade das múltiplas dependências internas e externas que devem ser reproduzidas no processo de replataforma. Não está pré-definido o melhor modelo de migração de dados com desafios relacionados àqueles antigos e proprietários. METODOLOGIA: Para lidar com os desafios enfrentados, foram organizadas as seguintes atividades:1- Estudos buscando referências e elementos para criação de conector- Número de referências (como artigos e livros) mapeadas- Percentual de referências que foram relevantes e aplicáveis ao projeto2- Estudos e experimentos buscando elementos para proposições a serem testadas para construção de infraestrutura e de conexão de sistemas IMS- Percentual de proposições que passaram nos testes iniciais- Tempo médio gasto em cada teste de proposição3- Estudos de processo de outbound considerando estruturas mainframe IMS e estruturas em nuvem- Quantidade de processos de outbound estudados e analisados- Percentual de processos que foram compatíveis com a estrutura em nuvem- Tempo médio necessário para integrar um processo de outbound4- Estudos buscando proposições de estratégia para viabilizar integração entre mainframe e a arquitetura orientada a serviços- Análises comparativas entre estratégias considerando tempo médio necessário para implementação, impacto na disponibilidade do sistema, conformidade e segurança, desempenho em testes de carga e volumetria5- Estudos sobre conceitos para APIs, protocolos de segurança e estruturação de solicitações considerando ambiente moderno e buscando comprovação técnica de processo que atue no mainframe- Percentual de APIs que atendem aos protocolos de segurança estabelecidos- Tempo médio gasto no desenvolvimento de cada API6- Estudos buscando referências para proposição de modelos de mensageria buscando compatibilização entre sistemas distribuídos e dados posicionais- Percentual de modelos que foram compatíveis com os sistemas distribuídos e dados posicionais- Tempo médio necessário para validar cada modelo de mensageria7- Estudos e experimentos de proposições para validar desenvolvimento de componente para chamada assíncrona- Percentual de componentes que passaram nos testes de validação- Tempo médio gasto no desenvolvimento de cada componente8- Estudos da arquitetura atual do sistema mainframe identificando aplicações, bancos de dados, interfaces e dependências internas e externas buscando proposições para emular o ambiente em estrutura de nuvem. Estudos considerando as regras de negócios existentes- Percentual de elementos que foram emulados com sucesso na nuvem- Tempo médio gasto na identificação e mapeamento de cada elemento9- Desenvolvimento de comandos e campos a serem levados a experimento- Percentual de comandos e campos que passaram nos experimentos- Tempo médio gasto no desenvolvimento de cada comando e campo10- Elaboração e execução de PoCs das principais soluções propostas- Percentual de PoCs que foram bem-sucedidas11- Realização de testes funcionais e não funcionais para validar as soluções propostas- Percentual de soluções que passaram nos testes de validação- Tempo médio gasto em cada teste funcional e não funcional12- Testes unitários e regressivos da solução- Percentual de testes unitários que foram bem-sucedidos- Percentual de testes regressivos que foram bem-sucedidos- Tempo médio gasto em cada teste unitário e regressivo INFORMAÇÃO COMPLEMENTAR: A integração entre sistemas mainframe legados e soluções em nuvem lida com desafios técnicos significativos devido às diferenças estruturais destes ambientes. Os sistemas mainframe são caracterizados por uma arquitetura fortemente acoplada com foco em confiabilidade e segurança. Em contraste, as soluções em nuvem adotam uma abordagem de serviços distribuídos, com arquiteturas escaláveis e flexíveis, projetadas para facilitar a elasticidade de recursos. Esta diferença cria barreiras para a interoperabilidade direta, exigindo o uso de adaptadores ou middleware para facilitar a comunicação entre os dois ambientes. É necessário criar e desenvolver estratégias específicas que possam mediar esta integração de forma eficiente.As estratégias de convivência entre sistemas legados e soluções em nuvem são, por natureza, únicas para cada cenário de implementação. A abordagem comum de integração envolve a utilização de APIs. Elas criariam uma camada de abstração para viabilizar a comunicação entre os sistemas mainframe e as aplicações na nuvem – sem alterar a lógica de negócios subjacente do sistema legado. No entanto, a implementação dessas APIs requer um extenso entendimento tanto da lógica de negócios do sistema legado quanto das capacidades da plataforma em nuvem de forma a evitar a degradação do desempenho ou a perda de dados críticos. Além disso, aspectos como segurança, latência e governança de dados devem ser rigorosamente considerados, dada a sensibilidade e a importância dos dados gerenciados pelos sistemas mainframe. Há que se evoluir nas soluções baseadas em APIs pois nem sempre consegue-se cumprir tais condições.Assim, não existe uma solução única ou um caminho definido para a integração entre sistemas legados e plataformas em nuvem – ponto corroborado por literatura nacional e internacional relacionada. Dessa forma, cada organização deve avaliar suas próprias necessidades, infraestrutura existente e objetivos de negócios para desenvolver uma estratégia de convivência personalizada. A escolha da estratégia adequada depende de uma avaliação cuidadosa das capacidades técnicas, requisitos de negócios e objetivos estratégicos da organização. Grande parte desse desenvolvimento é feito a partir de projetos intensivos em atividades experimentais e com alto potencial de resultarem em inovação tecnológica.Entre os estudos sobre integração e migração de sistemas para estruturas em nuvem, especialmente sobre desafios enfrentados, destacam-se:- Cheng, M., Qu, Y., Jiang, C., & Zhao, C. (2022). Is cloud computing the digital solution to the future of banking? Journal of Financial Stability, 63, 101073.https://www.sciencedirect.com/science/article/pii/S1572308922000948#bib48.- Deloitte. (n.d.). Modernizing legacy systems in banking: How banks can succeed at core and app modernization. Deloitte.https://www2.deloitte.com/us/en/pages/financial-services/articles/modernizing-legacy-systems-in-banking.html- Niazmand, N. (2015). The impact of cloud computing in the banking industry. International Journal of Information, Security and System Management, 4(2), 436-440. RESULTADO ECONÔMICO: Redução de 47% de clientes impactados por indisponibilidades,  crescimento de 240 mil interações/dia para 3 milhões de interações/dia entre mainframe e nuvem. RESULTADO INOVAÇÃO: Mecanismo de outbound entre sistema mainframe e nuvem em condições síncronas e assíncronas. Simplificação de processo de integração entre estes sistemas. Novo processo de governança refletindo na redução da indisponibilidade dos sistemas. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 5 ID ÚNICO: 109725 NOME: NOVO PROCESSO PARA GOVERNANÇA, MOVIMENTAÇÃO E CONTROLE DE QUALIDADE DE DADOS DESCRIÇÃO: As atividades desta linha de pesquisa se dedicaram a aprendizados para construção de processos de nova plataforma de dados a partir de entendimento e comprovação técnica de orquestração inédita de tecnologias existentes. Não havia processo único relacionado aos diferentes dados – de diferentes formatos e tipos – considerando todo o escopo da empresa. As soluções usuais não atendiam ao contexto bancário-financeiro com alto volume de dados em tempo real e descentralizadas em diferentes áreas de negócios. Assim, foi necessário organizar atividades experimentais buscando soluções inéditas para composição da nova plataforma.Para criar o processo de governança, as atividades tinham como objetivo tecnológico entender processos de extração, armazenamento e consulta de dados de múltiplas fontes e sistemas satélites de forma a unificar o acesso e organizar os dados. Havia que se entender como criar um sistema de governança capaz de lidar com a complexidade inerente aos altos volumes de dados, sua diversidade e a necessidade de processamento rápido. Não havia solução existente que cuidasse da plataforma de dados do Banco considerando um sistema único,  o que havia era um conjunto de sistemas descentralizados. Ao final, esperava-se a criação de um sistema genérico compatível aos diferentes sistemas.Para construir processos de movimentação de dados em diferentes ambientes, o objeto tecnológico foi propor nova estratégia de trânsito das informações do ambiente on-premise para a nuvem pública facilitando a convivência de processos de análise e estatística. Buscava-se entendimentos para viabilizar novo processo de conversão de scripts de linguagem obsoleta para modernizada. Não havia solução existente para esta movimentação de dados uma vez que plataformas mainframe são únicas, com alto acoplamento, nem sempre sendo possível replicar sua lógica de forma a garantir interação com sistemas em nuvem.Para garantir a qualidade dos dados, o objetivo tecnológico foi buscar elementos para solução que permita aos próprios produtores e consumidores de dados da plataforma controlar, com autonomia, a qualidade dos dados dos quais são responsáveis. Além de não haver solução existente que permitisse o controle de qualidade, havia poucas referências sobre processos que garantissem que tanto os produtores quanto os consumidores validassem essa qualidade,  havia que se criar processo de governança para isso. Alguns times rascunharam soluções isoladas que não foram comprovadas tecnicamente.Assim, considerando a situação existente, havia uma situação ineficiente e propensa a erros,  também não era aproveitado o potencial dos dados existentes. Além de viabilizar maior confiabilidade na tomada de decisão baseada em dados confiáveis, os desenvolvimentos vão permitir o desenvolvimento de novos dados e indicadores sobre eles.A utilização de ferramentas do sistema em nuvem AWS acabou por se tornar um ponto crítico, pois, em vez de facilitarem, acabaram por representar barreiras técnicas.  A utilização do recurso Glue considerou a premissa ele aceitaria comandos genéricos para validações de qualidade,  a partir da transformação de dados, seria possível aplicar validações, limpezas e mapeamentos que endereçam questões de qualidade como formatos de dados, integridade, e consistência, e de validade dos dados de acordo com regras de negócio específicas. No entanto, não houve uma pronta comprovação técnica, e estudos foram organizados para encontrar alternativa de solução. Além disso, ainda que a estrutura em nuvem seja considerada flexível, essa característica não se mostrou suficiente para acolher a solução. Estudos tiveram que ser organizados para entender como superar a rigidez da estrutura. Por fim, o isolamento de recursos da AWS com contas em ambientes separados foi uma vantagem que se configurou como barreira,  lidou-se com o desafio de criar processo para a gestão de acesso entre elas para centralizar os processos em uma nova plataforma de dados. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Modelagem de dados,  big data,  modelo dimensional,  lineage de dados,  programação orientada a objetos,  compilador,  transpilador,  governança de dados,  democratização de dados. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Não havia um sistema único considerado todos os dados que circulavam na empresa. Estudos se voltaram a entender sistemas satélites que lidassem com a variedade de ferramentas, tecnologias e estratégias encontrada.Para ações de armazenamento, havia que se entender como criar um processo que centralizasse todos os dados e, então, como criar um sistema de consulta deste repositório de dados. Enfrentou-se o desafio de demora nas conclusões,  estudos permitiram entender, como elemento novo, a organização dos dados na forma de um modelo dimensional que seria formado por tabelas de dados com informações sobre uma ação específica. Houve entendimento da aplicação de SQL para executar uma sequência de ações, identificando qual sistema deveria ser acessado para aquela dúvida a partir de ferramentas Athena e Hive e, por fim, execução de uma query SQL. Como elemento novo, foi possível entendimento para criar uma camada comum em que as várias informações oriundas de várias ferramentas de consulta pudessem ser traduzidas, configurando um sistema genérico compatível aos diferentes sistemas.Foram organizados estudos sobre como basear a funcionalidade de movimentação de dados levando a entendimentos para desenvolvimento considerando arquitetura orientada a objetos para implementar a lógica necessária para integração entre os ambientes, a conversão do dado em sua origem e posterior movimentação para o destino. Trata-se de novidade uma vez que não havia referências de arquitetura que sediasse processo de transpilação entre linguagem obsoleta para uma modernizada. Buscou-se, em literatura sobre bibliotecas, entendimentos para aplicação de conceitos de compiladores e transpiladores de forma a viabilizar processo inédito de implementação de regras que identifiquem tokens em uma determinada linguagem e possibilite sua conversão. Entre as referências que discutem a aplicação destes conceitos para lidar com ambientes em diferentes maturidades tecnológicas destaca-se o trabalho de SCHNAPPINGER, M.,  STREIT, J (2021).Foram realizados estudos para completar a composição desse processo de movimentação. Inicialmente, houve desenvolvimento de etapa de transformação de dados dentro do Glue por ele aceitar comandos genéricos para validações de qualidade tanto para dados técnicos quanto de negócios. No entanto, não era conhecido como aplicar o Glue Data Quality para viabilizar o desenvolvimento necessário, então o processo não estava comprovado a priori. Foram usadas referências sobre os desafios de aplicar esta tecnologia, com destaque para SAXENA, M,  SOWELL, B,  ALAMGIR, D. et al. (2023). Estudos também se voltaram a entender como centralizar os dados de forma automática e assíncrona a partir da estrutura já utilizada. A rigidez da estrutura, que não podia ser alterada, levou à necessidade de se experimentar diferentes mecanismos de centralização considerando automação e assincronia.Foram necessários estudos sobre como desenvolver scripts em que os produtores executassem o processo de qualidade de dados. Partiu-se da construção de processos de registros de resultados em tabela NoSql para métricas de qualidade. Houve comprovação técnica de processo de SOR de qualidade de dados a partir da composição de ingestão nesta tabela, criação de partição que geram um evento para o tópico Kafka de notificação no data mesh, aprendizados para criação de componente de sincronização que captura esses eventos de criação de partição e os registra em um repositório de armazenamento bucket S3. Em seguida, estudos permitiram entender como desenvolver script em PySpark que captura esses eventos do repositório, faz sua validação e lista todas as novas aparições criadas em tabela de qualidade dos dados. O processo termina com a busca de resultados em todas as tabelas do data mesh e faz a centralização em tabela NoSql, completando o processo de execução do controle de qualidade via produtores e consumidores. DESAFIO TECNOLÓGICO: Como não havia solução prévia que centralizasse e tratasse integralmente os dados da empresa – considerando mecanismos de governança, de movimentação e de controle de qualidade – havia que se construir proposições, considerando tanto referências na literatura quanto criatividade, para serem levadas a teste. Havia riscos de o conhecimento técnico-científico ser insuficiente para comprovação técnica almejada.Lidou-se com a barreira de comunicação necessária entre todas as contas AWS para captura de resultado. Isso porque cada conta AWS é um ambiente separado e pode atender diferentes áreas ou negócios,  ou seja, há isolamento entre recursos, configurações e permissões que, naturalmente, constitui-se uma barreira. Assim, lidou-se com o desafio de criar processo para a gestão de acesso necessária à nova plataforma de dados.Não era certa a composição dos processos que garantissem a governança dos dados. Lidou-se com barreira associada à validação de completude da tabela local de qualidade dada questões de dinamismo dos dados e padrões de qualidade variáveis. A necessidade de se alcançar um processo near real time reforçava a complexidade de lidar com aquelas qualificações dos dados. Como hipótese de solução, foram testados processos de solicitação automática de acesso, de replicação de arquivos em tempo real e o desmembramento struct em colunas. Ainda assim, era incerto como fazer as configurações e integrações dos endpoints necessários com o frontend. Para proposição de hipótese de solução, buscou-se referências em API rest e monitoração de microsserviços.Como estratégias de convivência entre estruturas legadas e modernizadas são únicas, principalmente devido ao forte acoplamento da estrutura on-premisse e sua configuração específica e não padronizada, havia riscos de não ser possível criar processo de intersecção entre os ambientes que resultasse na movimentação de dados necessária.Era incerto ser possível alcançar processo de conversão de linguagem por se tratar de recursos oferecidos por uma linguagem na origem e não existir uma contrapartida direta para a linguagem de destino. Também havia riscos devido à alta complexidade do processo de transpilação de código. Isso porque era necessário mapear a linguagem de origem entender e saber implementar o mesmo comportamento na linguagem de destino. Não havia benchmark ou referências para seguir dada a especificidade da estrutura legada.A necessidade de criar um processo de qualidade de dados trouxe desafios quanto à composição de scripts que considerasse a participação e avaliação de produtores e consumidores. Foi levado a experimento a composição parcial deste processo, considerando: registros de resultados em tabela NoSql para métricas de qualidade, de SOR de qualidade de dados a partir da ingestão nesta tabela, criação de partição que geram um evento para o tópico Kafka de notificação no data mesh, criação de componente de sincronização que captura esses eventos de criação de partição e os registra em um repositório de armazenamento bucket S3. Uma vez comprovado tecnicamente esta composição, experimentos se voltaram a entender se o desenvolvimento do script em PySpark viabilizaria a captura desses eventos do repositório, a respectiva validação e lista todas as novas parições criadas em tabela de qualidade dos dados. Buscava-se comprovar que o processo resultaria na possibilidade de busca de resultados em todas as tabelas do data mesh e centralização em tabela NoSql. METODOLOGIA: Para superar as barreiras citadas nesta linha de pesquisa, foram organizadas as seguintes atividades de pesquisa e desenvolvimento:1- Estudos buscando referências para proposição de processos de extração de dados de sistemas satélites considerando variedade de ferramentas, tecnologias e estratégiasMétricas:- Tempo médio de extração de dados usando diferentes ferramentas e tecnologias2- Testes de volumetria buscando comprovação técnica de processo que lidasse com o crescimento orgânico do uso da plataformaMétricas:- Número máximo de usuários simultâneos suportados sem degradação de desempenho- Tempo médio de resposta da plataforma sob diferentes cargas de uso- Percentual de erros ou falhas durante testes de alta volumetria3- Estudos buscando referências para construção de processo de armazenamento com experimentos para comprovação técnica para ações de consultaMétricas:- Tempo médio para realização de consultas no banco de dados4- Estudos e experimentos de estratégias de convivência entre sistemas on-premisse e a nuvem buscando referências e elementos para composição de processo de movimentação de dados entre os ambientesMétricas:- Tempo médio para sincronizar dados entre ambientes on-premise e na nuvem- Velocidade média de transferência de dados entre os sistemas- Percentual de dados corretamente sincronizados e consistentes entre os ambientes5- Estudos de estratégias de compiladores e transpiladores com execução de testes relacionados a processos de conversão de dadosMétricas:- Percentual de dados corretamente convertidos- Tempo médio necessário para converter um conjunto de dados- Percentual de erros ou falhas durante o processo de conversão6- Estudos de infraestrutura, dos tipos de integrações necessárias bem como orquestrações e monitoramentos necessários com testes para comprovação de solução de qualidade de dadosMétricas:- Tempo médio necessário para integrar diferentes sistemas- Quantidade de integrações diferentes suportadas pela infraestrutura- Percentual de dados que atendem aos critérios de qualidade definidos7- Experimentos para entender condições de altura mínima de qualidade de dados, tabelas e partiçõesMétricas:- Percentual de dados que atendem aos critérios mínimos de qualidade- Tempo médio de acesso a dados particionados versus não particionados- Percentual de dados consistentes após particionamento8- Estudos e experimentos relacionados a mapeamento de fluxos entre produtores e consumidoresMétricas:- Tempo de latência entre a produção e o consumo dos dados- Percentual de dados entregues corretamente aos consumidores9- Estudos sobre tecnologias de democratização de dados de forma a compor proposições considerando arquitetura data mesh a partir do desenvolvimento de PoCs- Percentual de domínios que operam de forma autônoma- Número de acessos aos dados por diferentes equipes e usuários INFORMAÇÃO COMPLEMENTAR: É comum a visão de que as empresas mantêm todos os seus dados centralizados e facilmente acessíveis. No entanto, a centralização de dados representa um desafio significativo, especialmente em organizações de grande escala que operam em múltiplos domínios e utilizam uma variedade de sistemas de TI. De fato, essas organizações muitas vezes lidam com dados armazenados em silos, cada um com seu próprio esquema, formato e protocolos de acesso, dificultando a integração e análise unificada. A literatura em Ciência de Dados e Gestão de TI destaca a complexidade inerente à integração de dados de fontes heterogêneas, enfatizando a necessidade de abordagens sofisticadas que lidem com processos de governança, movimentação e qualidade de dados.Sobre esta última dimensão, lida-se com o desafio inerente de construir processos de validação e limpeza de dados que exigem extenso conhecimento das características específicas dos dados e dos contextos de negócios aos quais se aplicam. Considerando uma estrutura de empresa grande, construir tal processo não é fácil nem óbvio. Vale dizer que a qualidade dos dados é fundamental para a tomada de decisões baseada em dados para que não haja erros de interpretação e a decisões mal-informadas,  uma estrutura centralizada não pode existir sem processos de averiguação de qualidade. Nesta linha de pesquisa, a complexidade é aumentada pela proposição de incluir o controle tanto por parte de produtores quanto consumidores dos dados.Apesar dos desafios citados, a execução das atividades desta linha de pesquisa viabilizou avanços importantes no conhecimento técnico-científico relacionado. Foi possível estabelecer processos para uma plataforma inédita de dados a partir de novas abordagens, tecnologias e práticas.Referências bibliográficas:- J.P. MORGAN. (n.d.). Evolution of data mesh architecture can drive significant value in modern enterprise. J.P. Morgan.- SAXENA, M,  SOWELL, B,  ALAMGIR, D. et al. The story of AWS Glue.- SCHNAPPINGER, M.,  STREIT, J (2021). Efficient platform migration of a mainframe legacy system using custom transpilation. In: 2021 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 2021. p. 545-554.- WINTER, R., & HACKL, T. (2023). Data mesh at scale: Exploration of current practices in large organizations. University of St.Gallen, Institute of Information Management. RESULTADO ECONÔMICO: Melhor atendimento ao cliente (em qualidade e velocidade) devido à redução de silos de informação, diminuição do custo da operação a partir da redução da necessidade de reprocessamento de dados. RESULTADO INOVAÇÃO: Criação de sistema para tomada de decisão baseado na engenharia de dados. nova solução de controle autônomo de qualidade de dados, nova solução para democratização de dados da jornada dos usuários. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 6 ID ÚNICO: 109726 NOME: DESENVOLVIMENTO DE SOLUÇÕES DE TESTES PARA MONITORAMENTO ATIVO COMO ESTRATÉGIA PARA MINIMIZAR A INDISPONIBILIDADE DE SISTEMAS DESCRIÇÃO: Apesar da evolução tecnológica alcançada por estruturas de software, a indisponibilidade destes sistemas ainda é um problema inclusive em corporações com recursos e especialização técnica diferenciados. Em um ambiente bancário, este problema é crítico uma vez que envolve transações que devem ser processadas em tempo real.Há uma série de fatores que podem resultar nesta indisponibilidade. Sistemas bancários de software integram uma variedade de serviços e funcionalidades configurando estruturas complexas que aumentam os riscos de erros e falhas,  esse potencial é aumentado pelo fato de ainda haver parte dos sistemas em plataforma legada mainframe – que são únicos e rígidos. A necessidade de atualização, de manutenção e de escalabilidade, ainda que inerentes a sistemas de software, aumentam os riscos de indisponibilidade quando considerada a estrutura complexa de grandes organizações.Resolver este problema passa por um conjunto de ações. Esta linha de pesquisa se dedica à concepção e desenvolvimento relacionados a testes de forma a criar processo para detecção e falhas nas jornadas de clientes. Foi possível entender a aplicação de conceitos de monitoração sintética à automação de testes,  além de se configurar como uma abordagem ativa, foi possível alcançar um desenvolvimento compatível com diferentes programas e clientes (a partir de APIs) e integrável a outros testes existentes na corporação. Atividades experimentais também se voltaram a entender como lidar com ambientes em diferentes maturidades tecnológicas e garantir uma automação de testes que permitisse um monitoramento único, sem a necessidade de ser duplicado. Foram organizados estudos que considerassem uma nova orquestração de execuções com entendimentos para construção uma mesma pipeline com retornos apartados entre si permitindo o monitoramento necessário.Os desenvolvimentos dos processos de automação de testes lidaram com a complexidade existente associada a cenários, tecnologias e ambientes distintos. Em um contexto bancário, com milhões de usuários, funcionalidades e transações, havia riscos de não ser possível uma solução que abrangesse a diversidade e criticidade de situações. Ainda que exista no mercado uma gama de ferramentas para testes e monitoração ativa, não havia soluções que apresentassem o desempenho necessário a esse contexto ainda que ajustadas e personalizadas. Entre as tecnologias consideradas, destacam-se Dynatrace, New Relic Synthetics, Pingdom e Catchpoint.Estudos considerando o estado da arte científico indicaram que a aplicação de monitoração sintética em automação de testes para detectar falhas em tempo real é incipiente e não tem sido atualizada. Estudos existentes geralmente se concentram em ambientes de desenvolvimento homogêneos ou modernos, com lacunas considerando soluções que abordem a integração de testes automatizados em infraestruturas mistas (com coexistência entre sistemas legados e novos serviços em nuvem ou arquiteturas distribuídas). Também há poucas referências sobre como criar pipeline que integre diferentes tipos de testes (unitários, integrados, funcionais e não funcionais).Como resultado, a execução desta linha de pesquisa trouxe, como novidade, uma abordagem ativa e integrada de testes para monitoração que minimizaram situações de indisponibilidade.Caso o desenvolvimento não fosse compatível com os outros testes relacionados a desenvolvimentos específicos (vale lembrar que testes unitários, integrados, funcionais e não funcionais são aplicados a cada desenvolvimento feito na empresa), a automação não seria viável. Caso não fosse possível uma solução única para testes em ambientes tecnológicos distintos, havia riscos de aumentar as situações de indisponibilidade. Tais fatos impediriam a conclusão da linha de pesquisa configurando-se, portanto, como marco crítico. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Monitoração sintética,  modularização,  testes automatizados,  automação. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Na solução existente, a execução de testes seguia estratégia de monitoração passiva com realização de testes para monitoramento a partir da experiência real do usuário em cenários autênticos. Considerando sistemas bancários, era possível ter uma análise detalhada de cada transação e sua interação com o cliente. No entanto, isso não se refletia em sistemas mais disponíveis. Como esse tipo de monitoramento depende da análise de eventos que já ocorreram, a detecção de problemas acontece de forma tardia,  além disso, sua cobertura não é abrangente a ponto de considerar a variedade de componentes interconectados dos sistemas bancários. Tais fatores indicaram que a abordagem passiva para testes de monitoramento era insuficiente para evitar a indisponibilidade dos sistemas. Com isso, era necessário entender novas estratégias para automação de testes com estudos voltados a conceitos de monitoração ativa.As principais abordagens desta monitoração – discutidas por Shahin, Babar e Zhu (2017) – forneceram insights em termos de contribuições teóricas, mas careciam de casos e aplicações práticas. Houve comprovação da aplicação de conceitos de monitoração sintética (que monitora a partir de simulações) para a automação de testes almejada. Diferente de outras abordagens, os testes podem ser totalmente automatizados proporcionando uma monitoração contínua do sistema de forma a identificar problemas assim que eles ocorrem,  a monitoração sintética também conseguiu abranger uma ampla gama de funcionalidades e tecnologias que coexistem e precisam ser testados de maneira integrada.Para isso, era necessário entender como emular as interações do usuário e dos fluxos de transação considerando a complexidade das inúmeras variáveis, cenários, produtos e serviços. Levou-se a teste a premissa de que a abordagem da linguagem de programação a ser utilizada na plataforma poderia simplificar o cenário alvo. Foi possível comprovar a aplicação de DSLs (domain-specific language) para desenvolvimento destes cenários. Uma Domain-Specific Language (DSL) é uma linguagem de programação utilizada para um domínio particular de aplicação. Nesse caso, sua aplicação permitiu abstrair a complexidade citada ao mesmo tempo em que permitiu que os especialistas do domínio, que possuem conhecimento detalhado das operações bancárias, definissem cenários de teste sem necessidade de compreender todas as nuances técnicas de cada sistema. Diferente de outras abordagens, como Model-Based Testing (MBT), a DLS permitiu a flexibilidade para permitir a personalização dos testes para cenários específicos sem perder a generalidade necessária para ser aplicada a diferentes sistemas.Com a abordagem de automação de testes definida, estudos foram direcionados para integrar à esteira de desenvolvimento os diversos tipos de testes — unitários, integrados, funcionais e não funcionais — necessários no design, desenvolvimento e implantação dos produtos de software da empresa. A problemática de integrar diversos tipos de testes em uma esteira de desenvolvimento contínuo é amplamente abordada na literatura especializada em engenharia de software e práticas DevOps – Erich, Amrit e Daneva (2017),  Rahman, Helms, Williams, Parnin (2015),  Lwakatare, Kuvaja & Oivo (2016). Ainda assim, há lacunas no estado da arte sobre abordagens e ferramentas que facilitem a integração e execução automatizada desses diferentes tipos de testes em um fluxo contínuo,  consolidá-los em uma esteira maior que ofereça uma visão abrangente e coerente de todas as atividades de teste é dificultada pela falta de frameworks e metodologias que abordem a heterogeneidade dos ambientes de desenvolvimento. Dentre as hipóteses consideradas, houve comprovação de nova composição de pipeline, com abordagem modularizada, de forma a criar processo para o provisionamento de ambientes e testes dinâmicos e apartados. DESAFIO TECNOLÓGICO: Como hipótese de solução, buscou-se entendimentos em monitoração ativa de forma a ser possível testar uma composição para a automação de testes que diminuísse situações de indisponibilidades. Kim, G., Behr, K., Spafford, G. (2013) ofereceram insights sobre a importância de combinar automação de testes com monitoramento ativo como forma de resolver problemas de disponibilidade de sistemas. Adkins, H., Beyer, B., Blankinship, P., Lewandowski, P., Oprea, A., Stubblefield, A. (2020) reforçavam que o monitoramento passivo não seria suficiente para garantir a disponibilidade contínua dos sistemas sugerindo a adoção de práticas de monitoramento ativo e automação.Considerando o monitoramento ativo, entre as principais técnicas estava a análise de logs em tempo real,  porém, havia limites quanto a analisar grandes volumes de dados relacionados à alta demanda por capacidade de processamento e armazenamento. Outra hipótese considerada era a monitoração baseada em transações a partir da criação de scripts automatizados que simulam diferentes transações típicas do usuário ou do sistema. Esses scripts seriam executados periodicamente para verificar se as transações estão sendo processadas conforme o esperado, detectando falhas ou problemas de desempenho. A literatura especializada aponta limitações da abordagem, em especial a dificuldade de cobrir todos os caminhos de uso ou funcionalidades, deixando áreas críticas sem monitoramento efetivo.A criação e desenvolvimento de estrutura de monitoração ativa enfrentou barreiras associadas à insuficiência do conhecimento em fornecer soluções que equilibrem precisão, cobertura e desempenho dos testes, sem sobrecarregar os recursos do sistema – conforme Shahin, Babar e Zhu (2017). Era incerto ser possível superar a contradição técnica de minimizar a disponibilidade do sistema a partir de uma ferramenta que pode reforçar tais falhas.Outra barreira diz respeito à simulação de sistemas complexos e dinâmicos que possa abranger todas as variáveis, cenários, produtos e serviços envolvidos em sistemas bancários. Tal barreira é reforçada por existir uma lacuna no conhecimento técnico-científico sobre como desenvolver e manter testes sintéticos altamente personalizados e flexíveis que possam automaticamente ajustar-se às mudanças nos sistemas que monitoram.Delineamentos experimentais se voltaram a entender a viabilidade da monitoração sintética lidando com as barreiras da emulação do sistema. Como hipótese de solução, testou-se a aplicação de DSLs (Domain-Specific Languages) buscando comprovação de flexibilidade nos testes. A premissa considerada era que criar linguagens específicas de domínio que permitam aos desenvolvedores especificarem comportamentos de testes de forma mais natural e intuitiva poderia simplificar a criação e manutenção de testes sintéticos R. Gupta, S. Kranz, N. Regnat, B. Rumpe and A. Wortmann. (2021) fornecem um mapeamento sistemático do uso de DSLs em testes automatizados de forma a simplificar a definição de testes sintéticos especialmente considerando sistemas complexos.A integração almejada de todas as esteiras de teste em um fluxo contínuo de monitoramento lidou com incertezas sobre como harmonizar ferramentas e processos de teste heterogêneos em uma esteira de desenvolvimento integrada,  a literatura especializada vem abordando a falta de frameworks ou metodologias que possam abordar tal questão.Estudos mostravam a necessidade de mais pesquisas sobre a interoperabilidade entre as diferentes ferramentas de monitoração ativa (Forsgren, N.,  Humble, J.,  Kim, G. (2018)). Experimentos apontavam para dificuldades na configuração e manutenção das integrações entre as ferramentas de monitoração e pipelines de outros testes. Estudos considerando o estado da arte trataram da adaptação de ferramentas de monitoração para diferentes ambientes e necessidades específicas de negócios, mas indicaram a necessidade de suporte melhor para análise preditiva e detecção de anomalias. METODOLOGIA: Para superar os desafios relacionados a novos processos de automação de testes visando soluções para cenários de indisponibilidade, foram organizadas as seguintes atividades:1- Estudos teórico e prático sobre metodologias de monitoração ativa buscando comprovações técnicas para configuração de testes- Percentual de metodologias aplicadas com sucesso em testes práticos.2- Estudos sobre monitoração sintética para entendimento de aplicações buscando referências para composição de solução- Quantidade de aplicações de monitoração sintética analisadas.- Número de referências e casos de uso documentados.3- Delineamentos experimentais para guiar emulação de cenários em ambiente de testes de forma a entender viabilidade da abordagem de monitoração sintética- Percentual de cenários que replicaram com precisão os ambientes de produção.4- Elaboração e execução de PoCs para realização de testes com usuários reais de forma a comparar achados do sistema emulado- Percentual de PoCs que resultaram em insights aplicáveis para a melhoria do sistema.5- Delineamentos experimentais para mapear os sistemas a coexistirem, suas características e principais problemas que possam estar prejudicando a comunicação entre eles e gerando indisponibilidade- Número de sistemas mapeados e documentados.- Número de problemas de comunicação identificados e resolvidos.6- Experimentos considerando modularização levando a teste o provisionamento de ambientes de testes dinâmicos apartados, - Tempo médio de provisionamento dos ambientes de teste.7- Experimentos buscando proposição de execução de testes regressivos de forma apartada de testes integrados, - Percentual de falhas detectadas nos testes regressivos antes da integração.8- Estudos buscando elementos e referências para subsidiar nova proposição de orquestração automática do provisionamento de múltiplos ambientes de teste para múltiplos artefatos associados a uma única pipeline.- Número de elementos e referências identificados.- Número de proposições de orquestração automática validadas. INFORMAÇÃO COMPLEMENTAR: A concepção de pipeline modular foi uma abordagem diferenciada para a execução de testes. Apesar de inicialmente parecer óbvia, ao separar esteiras entre mainframe e nuvem, o envolvimento da execução de testes foi um complicador. Esta abordagem permitiu que diferentes módulos de teste operassem de forma independente, mas coordenada. Era incerto ser possível dividir o processo de teste em segmentos distintos, cada um focado em aspectos específicos da funcionalidade do sistema. Havia que se comprovar se esta maior flexibilidade na gestão dos testes, possibilitando ajustes dinâmicos e a reutilização de módulos em diferentes cenários de teste, seria possível sem a necessidade de reconfiguração completa da pipeline.Este desenvolvimento buscava, na modularização, comprovações da execução de múltiplas cadeias de testes simultaneamente reduzindo o tempo total necessário para testes abrangentes. Além disso, essa abordagem facilitaria a identificação e isolamento de falhas entre ambientes mainframe e em nuvem, uma vez que cada módulo poderia ser avaliado individualmente.Para a implementação de testes integrados automáticos, foram adotadas ferramentas que permitem a simulação de interações complexas entre diferentes sistemas. Utilizou-se uma combinação de frameworks de automação de teste e linguagens de script específicas que, juntas, oferecem a flexibilidade necessária para modelar o comportamento do usuário e as transações entre sistemas heterogêneos. Essa abordagem permite a criação de cenários de teste altamente personalizáveis e adaptáveis capazes de simular uma vasta gama de operações bancárias em diferentes plataformas. A capacidade de automatizar esses testes integrados é fundamental para avaliar a interoperabilidade e o desempenho do sistema como um todo proporcionando insights valiosos sobre a robustez e a resiliência das integrações implementadas.Vale pontuar o papel das PoCs e os experimentos voltados a soluções de automação até então inexistentes. Isso permitiu identificar os principais desafios técnicos especialmente relacionados à integração de sistemas legados com plataformas em nuvem, o que exigiu uma abordagem criativa para superar as barreiras de compatibilidade e comunicação. Como resultado, houve refino das estratégias e soluções viabilizando entendimentos para a automação sem afetar questões de disponibilidade. Como resultado, foi possível estabelecer novos padrões para a realização de testes em sistemas bancários complexos e heterogêneos.Referências bibliográficas:- Adkins, H., Beyer, B., Blankinship, P., Lewandowski, P., Oprea, A., & Stubblefield, A. (2020). Building secure and reliable systems: best practices for designing, implementing, and maintaining systems. O'Reilly Media.- Erich, F. M., Amrit, C., & Daneva, M. (2017). A qualitative study of DevOps usage in practice. Journal of software: Evolution and Process, 29(6), e1885.- Forsgren, N., Humble, J., & Kim, G. (2018). Accelerate: The science of lean software and devops: Building and scaling high performing technology organizations. IT Revolution.- Gupta, R., Kranz, S., Regnat, N., Rumpe, B., & Wortmann, A. (2021). Towards a systematic engineering of industrial domain-specific languages. In 2021 IEEE/ACM 8th International Workshop on Software Engineering Research and Industrial Practice (SER&IP) (pp. 49-56). IEEE.- Kim, G., Behr, K., & Spafford, G. (2013). The phoenix project: A novel about IT. DevOps, and helping your business win, 345.- Lwakatare, L. E., Kuvaja, P., & Oivo, M. (2016). An exploratory study of devops extending the dimensions of devops with practices. Icsea, 104.- Rahman, A. A. U., Helms, E., Williams, L., & Parnin, C. (2015). Synthesizing continuous deployment practices used in software development. In 2015 Agile Conference (pp. 1-10). IEEE.- Shahin, M., Babar, M. A., & Zhu, L. (2017). Continuous integration, delivery and deployment: a systematic review on approaches, tools, challenges and practices. IEEE access, 5, 3909-3943. RESULTADO ECONÔMICO: Possibilidade de monitoração proativa de jornadas de clientes com redução de custos associada a menor indisponibilidade dos serviços. RESULTADO INOVAÇÃO: Processo de monitoração sintética para jornadas e APIs,  Novo processo para testes integrados considerando ambiente em nuvem e mainframe. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;"NÚMERO: 7 ID ÚNICO: 109727 NOME: DESENVOLVIMENTO DE SOLUÇÕES AVANÇADAS PARA DADOS TRANSACIONAIS E DE CLIENTES DESCRIÇÃO: Esta linha de pesquisa está associada a atividades experimentais para a criação e o desenvolvimento de novas soluções relacionadas a dados transacionais e de clientes.Para criar um motor de ingestão de dados, o objetivo tecnológico era entender processo de integração de dados,  ao considerar o reuso de peças de software existentes, implicou em restrições que aumentaram a complexidade do desenvolvimento. A modelagem se refletiu em um pipeline criando roles genéricas – usualmente, o que se tem é um role específico para cada tabela. Ao final, esperava-se ter solução para lidar com dados transacionais e, ao viabilizar a democratização dos dados, poder criar uma inteligência de dados para traçar perfis de clientes usuários de determinados produtos.Atividades também foram organizadas com o objetivo tecnológico de criar processo de consumo de dados de forma transacional – algo inexistente. Havia que se entender como desenvolver APIs transacionais de forma que recebesse requisição e se comunicasse com o data mesh. O processo conhecido fazia migrações de dados para o data mesh (modelo de armazenamento de dados descentralizado) em uma característica warehouse (modelo de armazenamento de dados centralizado). Era necessário evoluir esta solução.Para a criação de modelagem de uso de canal com cliente de forma a consolidar entendimento de suas jornadas, o objetivo tecnológico era desenvolver um sistema capaz de analisar a navegação (de todos os clientes e por longos períodos) para entender, de forma mais acertada, possíveis melhorias da experiência do cliente. Tentativas usuais de solução têm limitações uma vez que são menos analíticas e possuem um caráter mais opinativo. Também se baseiam em estudos a partir de entrevistas com clientes e, por isso, têm um escopo limitado. Ao fazer uso de tecnologia, espera-se cobrir uma gama maior de situações.Como resultado, espera-se viabilizar a criação de portais de análise de dados de forma a fornecer uma abordagem data driven nos diferentes canais em que o cliente atua, com criação de análises de sequências mais prováveis de utilização de serviços. Como diferencial tem-se uma nova forma de tratamento de dados a partir de novos algoritmos.Ao mesmo tempo que considerar o reuso de peças de software existentes diminuiria a complexidade do desenvolvimento, na prática tornou-se um ponto crítico por envolver a necessidade de generalizar pipelines que eram, por definição, específicas. Não compatibilizar peças existentes com as novas poderia inviabilizar o desenvolvimento. Apesar de se tratar de uma empresa que lida, basicamente, com transações, não havia solução relacionada a esse tipo de dados. Assim, configurou-se como marco crítico deste desenvolvimento transformar tais dados em elementos analíticos. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Ingestão de dados,  consumo de dados,  democratização de dados,  ambiente transacional,  cadeia de Markov,  aprendizagem de máquina,  modelos de agrupamento,  ciência de dados,  banco de dados relacional,  banco de dados não-relacional. NATUREZA: Processo ELEMENTO TECNOLÓGICO: A partir de estudos sobre a solução atual, com entendimento dos fundamentos e das redundâncias desnecessárias, foi possível propor estratégia para processo de ingestão de dados genéricos, via pipeline. Houve entendimentos para saber quais recursos de software criar e quais manter, uma vez que se buscava o reuso e não novas implantações. Houve compreensão de novo uso para o step function, cuja aplicação nesta situação, não era óbvia.Como elemento novo, houve entendimento para criar forma para consumo de dados, a transacional, em oposição à usual abordagem data warehousing. Houve comprovação técnica da aplicação do Athena para que as consultas não concorressem – não havia sido criado, ainda, uma forma de comunicação considerando esta tecnologia. Partiu-se da premissa que o Athena realiza consultas analíticas que não são concorrentes e não estão em um formato cliente servidor. Isso viabilizou, como elemento novo, utilizá-la para realizar consultas transacionais – isto é, aquelas que buscam por um único grupo de informação específica e de forma ágil. Não existia outra solução de consumo de dados considerando o ambiente transacional.Houve aprendizados que viabilizaram nova modelagem de análise da jornada do cliente a partir da sua navegação on-line. Houve entendimento de novo processo de identificação automática das atividades dos clientes considerando navegação internética. As soluções existentes funcionam em um contexto em que o componente principal de uma página é um produto. Aqui, tratava-se de uma situação em que os websites hospedam serviços e eles podem ser utilizados em conjunto fazendo com que não seja óbvio o mecanismo de análise de páginas da internet. O diferencial foi o desenvolvimento de novo algoritmo capaz de identificar as atividades realizadas pelo cliente dentro do canal.Aprendizados também viabilizaram a adaptação da estrutura de dados e criação de um algoritmo capaz de realizar o treinamento e o deploy de modelos baseados em cadeia de Markov de múltiplas ordens e diretamente em um banco de dados. As soluções existentes para treinamento de modelos geralmente são feitas carregando os dados em memória para então realizar o treinamento. No modelo aqui almejado, a análise deve ser feita simultaneamente considerando todos os dados de todos os clientes de um determinado segmento e por um longo período. Assim, há quantidade de dados analisados que não conseguem ser armazenados na memória o que limita o treinamento com ferramentas de mercado. Como elemento novo, houve aprendizados para criação de uma biblioteca para treinamento e implantação dos modelos baseados em cadeia de Markov diretamente no banco de dados.Cadeias de Markov são modelos estatísticos que utilizam probabilidade condicional para determinar a chance de transição de um estado para outro. Por essas características, construiu-se a hipótese de que eles conseguiriam modelar a navegação do cliente sendo capazes de predizer o comportamento dos usuários daquele canal. Porém, normalmente este modelo fica restrito a predizer qual a próxima página ou serviço que o cliente irá acessar. Houve aprendizados para utilizar a estrutura interna do modelo de comportamento de navegação para criar uma visualização em forma de grafo capaz de mostrar como os clientes acessam os serviços. Treinamentos associados à cadeia de Markov utilizam sequências de eventos que, ao final, são armazenadas na estrutura de dados interna do modelo em forma de probabilidades condicionais. Portanto, quando o evento observado na etapa de treinamento é informado, o modelo é capaz de predizer qual ou quais serão os próximos eventos. Como elemento novo, foram criadas operações de filtragem de público diretamente na estrutura de dados interna do modelo. A novidade é porque usualmente esse tipo de modelo a filtragem implica na seleção de público ou tipo de evento feito nos dados antes da etapa de treinamento por causa de uma limitação do próprio algoritmo do modelo. DESAFIO TECNOLÓGICO: Lidou-se com desafio associado a processos de integração visando criar um motor de ingestão de dados. Não era conhecido como criar processo para ingerir tabelas e criar roles de forma a se obter um recurso para todas as tabelas e consolidar uma solução genérica. Também não se sabia como encapsular a ingestão de dados, considerando outros sistemas de origem, sem replicá-los nestas respectivas origens. Buscava-se comprovação de um motor que facilitasse o acesso, a análise e o compartilhamento desses dados. Era incerto qual arquitetura poderia basear o desenvolvimento,  houve entendimento que o processo usual não resolveria buscando-se entender como fazer a solução com grafo de integração.Era incerto se o Athena seria capaz de unir diversas bases de dados, com permissionamentos e localizações diferentes, em um único JOIN. Havia que experimentar se ele possibilitaria o retorno da resposta em tempo viável para operações transacionais como seria em uma consulta analítica. Não havia muitas referências e casos de uso para serem utilizados como inspiração.Lidou-se com desafio sobre a forma de organização dos dados pois não se sabia como transformar a navegação dos clientes em atividades. Havia a barreira quanto à análise das interações dos clientes através de sequências de páginas carregadas e eventos observados. Esta forma de organização dos dados dificultou a identificação de padrões de navegação e colocou riscos à criação de processos de modelagem. O processo de análise dos resultados mostrou-se confuso, pois com a disponibilização de dados tal como dada era difícil identificar os serviços utilizados pelos clientes e correlacioná-los. Houve a compreensão de que a análise do comportamento dos clientes pode ser prejudicada pelo uso de atalhos criados e pela interrupção no uso dos serviços, dificultando a modelagem. Como hipótese, foi considerado que as interações dos clientes com o canal deveriam ser consideradas uma sequência de atividades.O desconhecimento de padrões de navegação, e até o desconhecimento da existência deles, constituiu uma barreira nas fases iniciais de estudo. Buscou-se em modelos baseados em aprendizado de máquina possíveis soluções. Havia dificuldades relacionadas a não se conseguir dados representativos em termos estatísticos de forma a obter a capacidade preditiva necessária. Para aplicação desta tecnologia havia que identificar padrões nas navegações dos clientes e agrupar os serviços disponibilizados de acordo com o seu uso. Assim, havia que correlacionar a navegação com o trabalho em que o cliente está, e depois agrupar serviços de acordo com o trabalho a ser executado.A partir da solução identificada, lidou-se com o desafio com a inferência na etapa em que são analisados novos dados. Experimentou-se executar tanto o treinamento quanto a implantação do modelo diretamente nas estruturas de dados. Foi criada uma biblioteca capaz de treinar um modelo analisando diretamente as bases de dados de navegação e armazenar sua estrutura interna. Houve comprovação de que, com essa biblioteca, a quantidade de dados analisados durante o treinamento de inferência fica limitada apenas pela tecnologia do banco de dados utilizada.Lidou-se com desafios quanto a cadeias de Markov, pois não há soluções disponíveis no mercado capazes de lidar com cadeias de ordem superior e grandes volumes de dados. Como hipótese, foi testado a possibilidade de o armazenamento das estruturas de dados internas de o modelo resolver os problemas de desempenho associados a grandes volumes.Durante o desenvolvimento da solução, havia a necessidade de fazer algumas análises que guiam os processos e elas deveriam ser feitas por público específico. As soluções disponíveis no mercado focam no caráter preditivo do modelo que possui etapas definidas de treinamento e inferência. Considerando o algoritmo original, toda filtragem de dados deve ser feita antes do treinamento, o que representava uma barreira ao seu uso. METODOLOGIA: De forma a lidar com os desafios encontrados nos desenvolvimentos desta linha de pesquisa, foram organizadas as seguintes atividades:1- Estudos para proposição de processo de integração de dados para dados transacionais- Tempo médio de integração de dados- Percentual de integrações bem-sucedidas)- Redução de redundâncias e inconsistências nos dados após a integração2- Experimentos buscando possíveis processos de criação de roles genéricas- Tempo médio para a criação de cada role- Redução no tempo de configuração de novos usuários3- Estudos para proposições de pipelines para criação de processo de consumo de dados transacionais com experimentos considerando Step Function- Latência média dos pipelines (a partir do tempo de processamento)- Taxa de sucesso na execução das Step Functions- Escalabilidade dos pipelines (considerando a capacidade de lidar com volumes crescentes de dados)4- Estudos buscando elementos para definição de arquitetura buscando soluções de grafos de integração- Tempo médio de consulta e integração de dados utilizando a arquitetura de grafos- Redução de redundâncias e inconsistências nos dados5- Experimentos considerando o Athena buscando proposições que levassem à comprovação técnica de união de bases de dados e retorno em tempo viável- Tempo médio de resposta das consultas no Athena- Percentual de consultas bem-sucedidas)6- Estudos para subsidiar experimentações com modelos avançados como cadeias de Markov, árvore de predição compacta, redes neurais profundas buscando mapear complexidades e padrões nos dados que não seriam perceptíveis em métodos tradicionais- Comparação de desempenho dos respectivos modelos com métodos tradicionais- Redução de erros de previsão7- Estudos sobre arquiteturas buscando referências de componentes e soluções com maior aderência e integração com o canal- Grau de aderência e integração dos componentes com o canal (medido por testes de integração)8- Estudos e experimentos buscando elementos para proposição de desenvolvimento de biblioteca- Taxa de adoção da biblioteca pelos desenvolvedores9- Experimentos a partir de ferramentas de Big Data, Apache Spark e Glue buscando estruturação de dados que facilitasse criação de análises visuais- Tempo médio de processamento e estruturação dos dados- Taxa de sucesso na criação de análises visuais10- Mapeamento dos fluxos de canais e tecnologias envolvidos no desenvolvimento buscando proposições de padronização- Grau de padronização alcançado (medido por conformidade com as diretrizes de padronização)- Redução de variabilidade nos processos de desenvolvimento11- Testes funcionais e não funcionais das soluções propostas- Percentual de testes bem-sucedidos- Identificação e resolução de bugs e problemas de desempenho12- Testes unitários e integrados buscando validação de desempenho das soluções propostas- Percentual de testes bem-sucedidos INFORMAÇÃO COMPLEMENTAR: Apesar de uma instituição financeira lidar com uma vasta quantidade de dados transacionais diariamente, não havia processo eficiente para tratar esses dados para fins analíticos. Tradicionalmente, os dados transacionais são utilizados para o processamento de transações do dia a dia, como pagamentos ou transferências,  sem tratamento, não havia competência de extrair insights valiosos que poderiam ser utilizados na tomada de decisões. A necessidade de transformar dados transacionais brutos em informações analíticas úteis justifica a organização de atividades experimentais que poderiam resultar em inovação tecnológica. O objetivo era superar limitações dos sistemas tradicionais que se concentram primariamente na função transacional sem aproveitar o potencial analítico dos dados.Os padrões de navegação dos clientes, quando adequadamente analisados, podem oferecer insights profundos sobre o comportamento do usuário, preferências e necessidades não atendidas. No entanto, a falta de ferramentas adequadas para processar e analisar esses dados de navegação limita a capacidade de entender e melhorar a experiência do cliente. É preciso encontrar uma forma de aprimorar a jornada on-line do cliente. O entendimento das restrições das abordagens tradicionais traz a necessidade de estudos para desenvolvimento de novos algoritmos e modelos de dados, como cadeias de Markov, para capturar e interpretar a complexidade da navegação do cliente.Por fim, os desenvolvimentos alvo desta linha de pesquisa permitem que a instituição saia de uma situação da existência de dados que não se comunicam entre si para uma organização que permita que se eles reflitam em uma maior capacidade analítica.Esta linha de pesquisa abordou diferentes formas e abordagens de armazenamento e tratamento de dados utilizando trabalhos como:- Winter, R., & Hackl, T. (2023, February). Data mesh at scale: Exploration of currentpractices in large organizations. University of St.Gallen, Institute of InformationManagement.Para subsidiar as ações relacionadas a Cadeias de Markov, foram utilizados os seguintes trabalhos:- M. Kaur, ""Why Stochastic Models That Are So Famous, Become Infamous In Reliability Engineering,"" 2020 Annual Reliability and Maintainability Symposium (RAMS), Palm Springs, CA, USA, 2020, pp. 1-8.- Schweitzer, P. J. (2021). A survey of aggregation-disaggregation in large Markov chains. Numerical solution of Markov chains, 63-88.- Vermeer, S., & Trilling, D. (2020). Toward a Better Understanding of News User Journeys: A Markov Chain Approach. Journalism Studies, 21(7), 879–894. RESULTADO ECONÔMICO: Redução de tempo de análise de dados por produto. Entendimento melhor do cliente a partir da identificação de que serviços são mais importante e como o cliente os utiliza. RESULTADO INOVAÇÃO: Novas estratégias para produtos mais específicos a cada cliente. Novo modelo capaz de simplificar a experiência de navegação. Contribuição à cultura data driven e analítica da empresa. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;"NÚMERO: 8 ID ÚNICO: 109728 NOME: SOLUÇÃO INTEGRADA PARA INGESTÃO, ANÁLISE, CLUSTERIZAÇÃO, MODELAGEM E DEMOCRATIZAÇÃO DE DADOS DESCRIÇÃO: Esta linha de pesquisa envolve processos para ingestão, análise, clusterização, modelagem e democratização de dados.Apesar de serem dimensões usuais, as soluções existentes não suportavam a quantidade e a variedade de dados gerados, seu crescimento exponencial, e a flexibilidade para integração de novos tipos de dados. Sistemas anteriores dependiam de processos de análise mais tradicionais e fragmentados,  tais soluções geralmente envolviam sistemas de banco de dados relacionais para armazenar e gerenciar dados com limitações quanto à escalabilidade e ao processamento de grandes volumes de dados,  muitas vezes exigiam processos manuais para a integração de dados de diferentes fontes, e processos manuais de ingestão e limpeza de dados que dependiam de scripts personalizados e rotinas de ETL com manutenção e ajustes contínuos. Ferramentas e plataformas operavam em silos, dificultando a visão unificada dos dados e a sua análise integrada.Com o objetivo tecnológico de entender como acelerar processo de ingestão de dados que alimentem motores de processamento, buscava-se comprovação de solução genérica para processos de integração de dados em qualquer linha de negócio. Para superar restrições relacionadas ao processamento de altas volumetrias de dados, o objetivo tecnológico era entender processo que permita a comparação de uma mesma base de dado em datas ou até mesmo versões diferentes de uma mesma base de forma a consolidar volumes. Havia que se tratar a questão de que, na solução nativa, os elevados volumes resultavam em reprocessamento de dados pois não se identificava o último registro e ele não era salvo,  isso resultava em perdas de dados no caso de alguma interrupção. Assim, o objetivo tecnológico foi entender como controlar o último registro executado superando desafios de heterogeneidade de dados, de grandes volumes, também buscando criar um sistema de recuperação automática após interrupções.Para desenvolver clusterizações de clientes de forma a guiar conversas e ofertas de produtos mais contextualizadas, o objetivo tecnológico foi criar processo de captura e centralização de dados. Entre os desafios destacam-se o desconhecimento de como absorver diferentes regras de negócio, de unificar as marcações de modo a manter a explicabilidade da regra, principalmente considerando origens de dados baseadas em diferentes infraestruturas.A ideia de desenvolver uma plataforma gamificada que dê transparência ao cliente sobre sua situação em determinados produtos levou ao objetivo tecnológico de lidar com dados de diferentes fontes, tratá-los e democratizar esses dados para diferentes áreas. O objetivo tecnológico era entender como construir solução para disponibilizar consumo de grande volume de dados que são gerados e armazenados na estrutura da plataforma de modelagem. Plataformas de gamification vêm ganhando espaço como ferramenta de engajamento,  porém, como é um desenvolvimento relativamente novo e carece de boas práticas a serem seguidas – o que se reflete em vários desafios tecnológicos a serem superados via atividades experimentais Changiz Hosseini et. Al. (2022).Ao final, almejava-se a criação de processos automatizados de explicitar e tratar os dados do cliente sem que o engenheiro de dados precisasse desenvolver novas rotinas e códigos complexos. Com isso, será possível construir plataformas de ingestão e integração de dados diminuindo o tempo de disponibilização das informações para a tomada de decisão.Aqui, um ponto crítico foi lidar com a volumetria e a flexibilidade associadas a dados. As tecnologias envolvidas não atendiam, tal como estavam, ao escopo do desenvolvimento. Apesar de causar estranheza, a princípio, uma vez que tais dimensões são inerentes ao desenvolvimento de software, houve a compreensão da necessidade de extrapolar conhecimentos existentes buscando avançar nos respectivos estados da arte. Se não fossem superadas tais barreiras, seria difícil viabilizar os desenvolvimentos almejados. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Gestão de dados,  integração de dados,  base de dados,  clusterização,  democratização de dados,  advanced analytics,  mineração de dados,  gamificação. NATUREZA: Processo ELEMENTO TECNOLÓGICO: O desenvolvimento das atividades desta linha de pesquisa resultou em um conjunto de aprendizados.A solução conhecida para processo de ingestão de dados envolvia o banco de dados Cassandra (sistema de gerenciamento distribuído projetado para lidar com grandes volumes de dados) em NoSQL (sistema que suporta grande variedade de modelos de dados), mas ela não atendia às funcionalidades necessárias. Porém, a partir desta base foi possível viabilizar processo de integração de dados entre camadas de diferentes origens e destinos, resultando em novo uso para as tecnologias citadas. Aprendizados levaram à proposição de monitoramento de notificação de alterações nos dados de origem: a notificação acontece no CDP (plataforma de dados do cliente) de forma que, ao ocorrer modificações estruturais em tabelas neste ambiente – como criação, remoção e atualização de tabelas – uma notificação é disparada através de um tópico Kafka.Além disso, experimentos consideraram o Step Function e sua combinação com Lambda de forma a viabilizar fluxos da aplicação, algo que ainda não havia sido utilizado no contexto de integração de dados. Com isso, houve comprovação de processo de orquestração dos serviços de integração para que as pipelines de ingestão possam ser executadas de forma assíncrona.Aprendizados para desenvolver processo para não duplicação dos dados levou à criação de ferramenta genérica que faz as comparações entre as versões das bases – em um Bucket S3 no formato *parquet para orquestrar um conjunto de Glue Jobs utilizando Spark. Apesar das tecnologias não serem inovadoras por si só, trata-se de novidade ao aplicá-la para este novo contexto refletindo em avanços no respectivo estado da arte.Para superar o desafio de tempo de ingresso dos dados à plataforma única – na solução existente era cerca de 400 horas – estudos permitiram viabilizar construção de peça que eliminava a necessidade de diversos microsserviços,  a solução usual seria usar a compartimentação de responsabilidade dos microsserviços para agilizar tal ingresso, algo que não se comprovou tecnicamente. Para construir as clusterizações, estudos se voltaram a técnicas de árvore de decisão, DBScan, KMeans e KNN, com entendimento do algoritmo baseado em árvore de decisão.Para a plataforma de gamificação, não era conhecido processo para obtenção de dados oriundos de diversas fontes devido às características específicas de interatividade em tempo real e personalização dinâmica das experiências dos usuários. O principal desafio era garantir a integridade dos dados durante a interação do usuário com o jogo. Estudos permitiram comprovar a utilização de dados democratizados no mesh através de consultas via glue e obtendo os dados fechados em D-1 de forma que não sofressem alterações enquanto o cliente estivesse no game. Assim, foi desenvolvido processo para obtenção de dados de diversas fontes sem demandar controles adicionais da parte consumidora.Diante dos desafios quanto à fragmentação das informações, houve entendimentos de lógica para lidar com a grande quantidade de arquivos pequenos sendo consolidado em módulo automático de mapeamento e concatenação de small files. Tais tipos de arquivos podem representar um gargalo significativo ao desempenho de sistemas distribuídos. Aprendizados levaram à criação de 2 lambdas, Mapper e Concat, utilizando python, para a geração de arquivos parquets aptos para consumo. A solução usual seria usar o glue,  porém, experimentos permitiram entender que a lógica de funcionamento criada a partir de lambdas teriam melhor tempo de processamento.Em suma, os aprendizados possíveis a partir de tecnologias existentes aplicadas a novo contexto resultaram em respectivos avanços no estado da arte das áreas de processos para ingestão, análise, clusterização, modelagem e democratização de dados. DESAFIO TECNOLÓGICO: A criação de processo de ingestão de dados lidou com o desafio de equilibrar o tempo de processamento com a volumetria apresentada – era necessário diminuir o tempo de ingestão utilizando a mesma volumetria da solução anterior. Havia que se superar restrições quanto à criação de scripts HQL, ShellScripts, entre outros processos convencionais, que resultavam em cerca de 400 horas para ingestão do banco de dados. Lidou-se com desafio de considerar grande quantidade de tabelas e em estruturas diferentes – sistemas legados e em nuvem. Não era factível criar um modelo de processamento para cada uma delas, sendo necessário organizar experimentos para compatibilizar as diferenças. Como hipótese, levou-se a teste a estratégia de convivência com a disponibilização da base produtiva de dados liberando o histórico para estudos a partir da conta consumer e seguindo a arquitetura data mesh.Apesar de problemas de volumetria serem dimensões comuns associadas a dados, havia que se lidar com problemas de interrupção que levavam ao reprocessamento dos dados. Algumas hipóteses foram consideradas – como uma biblioteca python desenvolvida por cientistas de dados com a premissa de disponibilizar bases através de processos que não ocupam memória. Como o mesmo usuário pode se encaixar em agrupamentos diferentes, havia riscos quanto à unificação da marcação do público. Como hipótese, experimentou-se a criação de um critério de priorização para eleger a categoria predominante desse usuário de modo e não comprometer a distribuição da população no estudo.Para a plataforma de gamificação, não era conhecido processo para obtenção de dados de diversas fontes. Algumas hipóteses de solução foram testadas. A primeira tentativa foi utilizar a resposta on-line de APIs síncronas e houve comprovação de retorno exato da foto do cliente naquele momento,  no entanto, ele não atendia ao outro requisito necessário de ter as informações consolidadas em tempo real não sendo factível esperar até o fim do dia. A segunda hipótese considerada foi utilizar a resposta assíncrona por eventos do tópico Kafka que trouxe o retorno sobre a movimentação da conta do cliente,  não houve comprovação a partir do entendimento da necessidade de um controle duplicado desses dados. Percebeu-se que se configurava um fluxo longo colocando complexidade devido à quantidade de clientes. Houve comprovação da terceira hipótese: utilizar dados democratizados no data mesh através de consultas via glue e obtendo os dados fechados em D-1 de forma que não sofresse alterações enquanto o cliente estivesse no game. Com isso, foi possível criar processo inédito para obtenção de dados de diversas fontes sem demandar controles adicionais da parte consumidora.Lidou-se com restrições associadas à abordagem de fragmentação dos dados. O fato de serem arquivos muito pequenos levava a um desempenho não otimizados nas consultas. Isso principalmente devido ao Athena precisar listar todos os locais de partição, processo que demanda tempo. Como hipótese, partiu-se da premissa que a manipulação de arquivos de maior tamanho, e hospedados em S3, poderiam resultar em operação mais ágil em comparação com os mesmos registros distribuídos em múltiplos arquivos menores. A hipótese de concatenar os arquivos menores contou com desafios de lidar com tabelas de partições de grande volume. Não era factível utilizar os recursos AWS para isso pois eles não atendidam à necessidade de uma instância com elevada capacidade de memória para efetuar as concatenações de maneira eficiente e no menor tempo possível. Experimentos considerando o Glue indicaram que ele executa essas operações de forma sequencial, em uma única instância, impondo limitações ao processo de otimização do armazenamento e acesso aos dados,  por isso, havia que se testar outras hipóteses. METODOLOGIA: Para superar desafios técnicos, foram organizadas as seguintes atividades:1- Experimentos para validar as proposições de peças da plataforma considerando tecnologias como Cassandra, Aurora, lambdas, step function, entre outrosMétricas:- Tempo médio de resposta das operações realizadas- Percentual de operações bem-sucedidas em relação ao total de operações- Capacidade do sistema de lidar com aumentos na carga de trabalho sem degradação de desempenho2- Experimentos para normalização e compatibilização de forma a consolidar ferramenta genéricaMétricas:- Percentual de dados que foram normalizados corretamente- Percentual de fontes de dados diferentes que foram integradas com sucesso- Tempo médio necessário para normalizar um conjunto de dados- Avaliação da precisão e integridade dos dados após normalização3- Estudos considerando a orquestração entre lambda e step function com experimentos e testes buscando comprovação técnica de processo orientado a eventosMétricas:- Avaliação de latência a partir do tempo médio de execução das funções Lambda orquestradas por Step Functions- Capacidade do sistema de escalar automaticamente com aumento de eventos4- Estudos sobre ferramentas Glue com experimentos para ferramenta genérica para comparação entre bases de dados, Métricas:- Tempo médio necessário para completar processos de ETL- Percentual de comparações corretas entre diferentes bases de dados5- Execução e avaliação de PoCs considerando cenários de: grande quantidade de tabelas, tipos de estruturas de tabelas diferentes, tipos variados de tecnologias de banco de dadosMétricas:- Percentual de tecnologias de banco de dados diferentes que foram integradas com sucesso- Avaliação de desempenho do sistema em cenários com grande quantidade de tabelas e estruturas variadas6- Experimentos considerando proposições com Cassandra embarcado para entendimento de refinos necessários na infraestrutura buscando comprovação técnica de processo sem lentidão ou perda de dadosMétricas:- Latência das operações a partir do tempo médio de execução das operações no Cassandra embarcado- Capacidade do Cassandra embarcado de lidar com aumentos na carga de trabalho7- Experimentos buscando estabelecer priorização a partir de processo de unificação da marcação do público com testes para validação de volumetria dos clusters, coerência com a realidade e possíveis impactos para o negócio em termos de comunicação, crédito, engajamento e experiência do clienteMétricas:- Avaliação qualitativa e quantitativa dos impactos nas áreas de comunicação, crédito, engajamento e experiência do cliente- Avaliação da volumetria dos clusters a partir do tamanho médio dos clusters gerados e sua coerência com a realidade do negócio8- Estudos e experimentos de hipóteses para obtenção de dados de outras fontes. Experimentos a partir da resposta on-line de APIs síncronas e de respostas assíncronas por eventos do tópico Kafka, de uso da estrutura data mesh através de consultas via glueMétricas:- Tempo médio de resposta das APIs síncronas- Latência dos eventos Kafka a partir do tempo médio de processamento- Taxa de sucesso das consultas glue- Avaliação da precisão dos dados obtidos de diferentes fontes9- Experimentos considerando lambdas buscando comprovação técnica de abordagem para lidar com grande quantidade de arquivos pequenosMétricas:- Tempo médio necessário para processar os arquivos pequenos- Percentual de operações bem-sucedidas em relação ao total de operações- Capacidade do sistema de escalar automaticamente com aumento na quantidade de arquivos10- Estudos e experimentos buscando elementos para composição de estratégia de mitigação para concatenar arquivos menores e, ao mesmo tempo, com tabelas de partições de grande volumeMétricas:- Avaliação do desempenho das tabelas de partições após a concatenação- Percentual de operações de concatenação bem-sucedidas- Percentual de utilização de CPU, memória e I/O durante o processo de concatenação. INFORMAÇÃO COMPLEMENTAR: Questões de volumetria e escalabilidade são inerentes ao desenvolvimento de sistemas de software projetados para manipular grandes conjuntos de dados e atender a um número crescente de usuários ou transações. A princípio, essas questões podem ser percebidas como desafios convencionais de engenharia em que a aplicação de conhecimentos e práticas estabelecidas poderia levar à identificação e implementação de soluções eficazes. Isso acontece principalmente porque esses problemas geralmente envolvem a otimização do uso de recursos computacionais, como memória e poder de processamento, e o gerenciamento eficiente de grandes volumes de dados.No entanto, a distinção entre um problema convencional de engenharia ou de um desafio técnico que exige desenvolvimentos experimentais é mais complexo. Deve ser feita considerando se as variáveis envolvidas são conhecidas e se há restrições de conhecimento para resolver o desafio. Ou seja, a distinção é definida pela não solução, e não apenas pelo problema, das questões envolvidas. Isso porque, em muitos desses casos, as soluções existentes e as práticas de engenharia de software tradicionais não são suficientes para resolver problemas de volumetria e escalabilidade de maneira eficaz. É possível que o estado da arte das tecnologias envolvidas não ofereça as capacidades necessárias para lidar com a escala e a complexidade dos dados ou do sistema em questão. Essa possível lacuna entre as necessidades do projeto e as soluções disponíveis destaca a natureza única desses desafios. É o caso encontrado por esta linha de pesquisa.Em contextos de desenvolvimento de software de alta complexidade, novos tipos de dados, padrões de acesso e requisitos de desempenho podem surgir estabelecendo condições diferentes de aplicação. Isso pode resultar em situações em que soluções usuais não atendam, ainda que o problema seja considerado comum. Essa situação pode se configurar uma incerteza ao desenvolvimento ao estabelecer lacunas de conhecimento técnico-científico.Portanto, enquanto questões de volumetria e escalabilidade podem se assemelhar a problemas de engenharia tradicionais à primeira vista, sua complexidade, a falta de soluções óbvias e a natureza dinâmica do ambiente de desenvolvimento as distinguem como desafios técnicos que podem, potencialmente, levar à inovação tecnológica, a partir de atividades experimentais e abordagens criativas.Citam-se exemplos de trabalhos acadêmicos que abordam questões de volumetria que não têm solução óbvia:- Grolinger, K., Higashino, W. A., Tiwari, A., & Capretz, M. A. M. (2013). ""Data Management in Cloud Environments: NoSQL and NewSQL Data Stores"". Journal of Cloud Computing: Advances, Systems and Applications, 2(1), 22.- Chen, L., et al. (2013). ""Big Data Challenge: A Data Management Perspective"". Frontiers of Information Technology & Electronic Engineering, 19(12), 1586-1597.Como base para entendimentos sobre plataforma gamificada, tem-se a seguinte referência:- Changiz Hosseini, Oda Humlung, Asle Fagerstrøm, Moutaz Haddara (2022), An experimental study on the effects of gamification on task performance, Procedia Computer Science, Volume 196, Pages 999-1006, ISSN 1877-0509. RESULTADO ECONÔMICO: Aumento da velocidade da disponibilização de dados, redução do tempo de processamento de dados a partir de automatização da comparação entre bases, redução de tempo de ingresso de jornadas na plataforma única de 400 horas para 4 horas. RESULTADO INOVAÇÃO: Novo processo genérico de ingestão de dados, novo processo de clusterização com uso intensivo de dados, novo processo gamificado a partir de dados de fontes diferentes DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 9 ID ÚNICO: 109729 NOME: PROCESSAMENTO DE DADOS PARA GESTÃO DE AUTENTICAÇÕES E CONSENTIMENTOS MULTIBANCÁRIOS DESCRIÇÃO: Esta linha de pesquisa tem como objetivo obter novas compreensões a fim de aperfeiçoar a eficiência e segurança nos processos de gestão de autenticações e consentimentos dentro de um ecossistema de compartilhamento de dados bancários entre várias instituições financeiras. Para alcançar tal propósito, propõe-se a adoção de uma arquitetura serverless, com experimentações de funções Lambda, visando assegurar uma maior escalabilidade do sistema e a redução significativa dos custos operacionais. Foi proposta a experimentação de uma arquitetura serveless que o sistema se ajuste automaticamente às demandas variáveis, sem a necessidade de gerenciamento direto de servidoresAlém disso, foram realizados experimentos com bases de dados não relacionais, com o intuito de melhorar o armazenamento e a recuperação de informações complexas, visando não apenas a alta performance, mas também a agilidade do sistema em responder às solicitações. Essa abordagem é complementada pelo desenvolvimento de mecanismos voltados à segurança dos dados, incluindo o gerenciamento de concorrência e a aplicação de técnicas avançadas de controle de versão e transações, fundamentais para assegurar a integridade e a confiabilidade dos dados manipulados. A serialização eficiente de dados, por meio do uso de bibliotecas especializadas que reduzem o tamanho dos dados transmitidos, visa otimizar ainda mais a performance do sistema, garantindo uma transferência de dados rápida e segura entre as diferentes entidades.Espera-se elevar o nível de eficiência e segurança na gestão de autenticações e consentimentos nos ambientes de compartilhamento de dados. O desenvolvimento da arquitetura serverless e das bases de dados não relacionais deve proporcionar um sistema mais agilidade e escalabilidade, além de prover recursos mais robustos em termos de segurança da informação.Vale destacar ainda um ponto crítico no desenvolvimento do projeto, sendo o desenvolvimento e a operacionalização corretos de uma arquitetura serverless robusta, combinada com o uso de bases de dados não relacionais, para aprimorar a eficiência e segurança nos processos de gestão de autenticações e consentimentos dentro do ecossistema de compartilhamento de dados bancários. Esse avanço foi fundamental, pois demonstrou a viabilidade de o sistema projetado para ser altamente escalável, que se ajusta automaticamente às demandas variáveis de tráfego, sem a necessidade de gerenciamento direto de servidores, atingindo assim uma redução significativa dos custos operacionais.A integração bem-sucedida de funções Lambda e a adoção de bases de dados não relacionais não foram pontos triviais, tendo a necessidade de evoluir substancialmente o armazenamento e a recuperação de informações complexas, proporcionando alta performance e agilidade no sistema para responder às solicitações de maneira eficaz. A complementação dessa abordagem com o desenvolvimento de mecanismos avançados de segurança de dados, incluindo gestão de concorrência, controle de versão sofisticado e técnicas de transação, foi crítica para assegurar a integridade e a confiabilidade dos dados manipulados em um ambiente compartilhado. Além disso, a serialização eficiente de dados, por meio do uso de bibliotecas especializadas que minimizam o tamanho dos dados transmitidos, otimizou ainda mais a performance do sistema, garantindo uma transferência de dados rápida e segura entre a empresa e demais instituições financeiras. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Relacional,  dados,  IAM,  serviços de autenticação,  consentimento,  modelo,  serialização,  descentralizada,  pesquisa. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Buscou-se a comprovação científica de que um sistema de funções distribuídas poderia ser utilizado para criar um sistema de processamento de eventos em tempo real, altamente distribuído e resiliente, utilizando arquiteturas de microsserviços com comunicação entre regiões geográficas. Tal hipótese foi levada a experimentações. Configurou-se um sistema distribuído de funções em múltiplas regiões, com replicação de eventos de autenticação e consentimento entre regiões. Utilizou-se um sistema de orquestração de workflows complexos entre essas funções, com failover automático em caso de falha de uma região. O sistema obteve tempos de failover inferiores a 1 segundo e latência média de 250 ms por função.Buscou-se comprovar a viabilidade da aplicação de técnicas avançadas de caching e comunicação, como cache distribuído e comunicação assíncrona, para melhorar a performance do sistema de autenticação e consentimento. Assim, implementou-se um cache distribuído e configurou-se comunicação assíncrona entre funções e o sistema de armazenamento de dados. Foram realizados testes de carga com 5000 conexões simultâneas e mediu-se a latência de acesso a dados em cache. O cache distribuído reduziu a latência de acesso aos dados em até 86%, com um tempo médio de 3 ms para dados em cache. A comunicação assíncrona demonstrou alta eficiência, com latências médias de 15 ms por mensagem.Foi testada ainda a ideia de que o uso de pooling de conexões com protocolos avançados poderia melhorar a performance e a eficiência de comunicação no sistema distribuído. Assim, criou-se um pooling de conexões utilizando um protocolo avançado com multiplexação de streams, suportando até 10.000 conexões simultâneas. Foram executados testes de carga para medir a latência e a eficiência de comunicação. O pooling de conexões resultou em uma melhora de performance de até 64%, com redução do tempo de estabelecimento de novas conexões para 6 ms e latência média de comunicação de 18 ms.A consistência eventual e transações foram exploradas. A hipótese de que a consistência eventual poderia ser combinada com técnicas de aprendizado de máquina para prever e otimizar operações de leitura em um sistema distribuído foi testada por meio da criação de um modelo de consistência eventual para operações de leitura, combinado com um algoritmo de aprendizado de máquina para prever padrões de acesso e otimizar a distribuição de dados. Realizaram-se testes com 200.000 transações simultâneas, medindo a latência e a precisão das previsões. A combinação de consistência eventual e aprendizado de máquina resultou em uma melhoria de performance de 46%, com latências médias de 16 ms e precisão de previsão de 94%.A exploração de blockchain para gestão descentralizada de identidade e acesso também foi realizada. A hipótese era que blockchain poderia ser utilizado para criar um sistema de gestão de identidade e acesso com contratos inteligentes para automação de autenticações e consentimentos. Implementaram-se contratos inteligentes em plataformas de blockchain para automatizar processos de autenticação e consentimento. Realizaram-se testes de carga com 10000 transações simultâneas, medindo a latência e a eficiência dos contratos inteligentes. Os contratos inteligentes demonstraram alta eficiência, com latências médias de 6 segundos em uma plataforma e 0.9 segundos em outra, validando a viabilidade de automação de processos de autenticação e consentimento. Além disso, a hipótese era que blockchains privadas com mecanismos de consenso avançados poderiam fornecer maior controle sobre a performance e a segurança das transações foi testada com a implementação de uma blockchain privada com mecanismos de consenso avançados, realizando testes de carga com 2000 transações simultâneas e medindo a latência e a resiliência do sistema. Os mecanismos de consenso avançados demonstraram alta resiliência e baixa latência, com tempos médios de 1 segundo por transação. DESAFIO TECNOLÓGICO: Durante a execução dos experimentos para sincronização de estado em tempo real, a equipe observou que, sob condições de alta variabilidade de latência, que podia oscilar entre 50 ms e 1200 ms, o algoritmo de consenso modificado falhava em manter a consistência de estado em até 30% das transações sob teste. A hipótese de solução envolveu a introdução de um sistema de buffer adaptativo que armazenaria temporariamente as transações durante picos de alta latência, com o algoritmo ajustando dinamicamente o tamanho do buffer com base na latência média observada nos últimos minutos. No entanto, a implementação desse sistema introduziu complexidades adicionais, como a necessidade de um mecanismo eficiente de resolução de conflitos para transações que eram armazenadas no buffer por períodos prolongados.Para o sistema de failover automático, experimentos iniciais mostraram que o tempo de detecção e resposta a falhas era inaceitavelmente alto, com média de 500ms, muito acima do objetivo de 100ms. A equipe propôs a utilização de uma rede de sensores de falha distribuídos, que poderia aumentar a precisão da detecção de falhas através de um modelo de votação ponderada. Cada sensor avaliaria a saúde do sistema com base em um conjunto de métricas pré-definidas, e um algoritmo de agregação ponderada consolidaria essas avaliações para determinar a necessidade de failover. A complexidade da calibração dos pesos atribuídos a cada sensor, no entanto, revelou-se um desafio significativo, exigindo múltiplas iterações de ajuste fino para alcançar a sensibilidade desejada.Na otimização de caching, a volatilidade das cargas de trabalho resultou em taxas de acerto de cache abaixo de 60%, significativamente menor do que a meta de 90%. A equipe testou um modelo de aprendizado de máquina que utilizava redes neurais recorrentes (RNN) para prever a demanda baseando-se em padrões de acesso históricos. Embora os modelos iniciais tenham mostrado promessa, ajustando-se a padrões de acesso com uma precisão de até 78%, a latência introduzida pelo tempo de inferência do modelo, que poderia chegar a 200 ms, comprometeu a eficácia da solução em um ambiente de produção.O desafio do pooling de conexões sob cargas extremas revelou que o novo protocolo de comunicação experimental atingia sua capacidade máxima a aproximadamente 10.000 conexões simultâneas, abaixo do objetivo de 50.000. A equipe propôs a implementação de uma camada de abstração que permitiria a multiplexação de conexões virtuais sobre um número limitado de conexões físicas, utilizando um esquema de priorização baseado na criticidade da tarefa. No entanto, a complexidade de implementar um sistema de priorização eficaz sem introduzir latência adicional foi um desafio notável.Para a previsão de padrões de acesso utilizando aprendizado de máquina, a adaptação rápida dos modelos a mudanças quase em tempo real em padrões de acesso provou ser problemática. Os modelos demoravam cerca de 5 minutos para se adaptar a novos padrões, um atraso considerável em um ambiente onde os padrões de acesso podem mudar em questão de segundos. A equipe explorou a utilização de modelos de aprendizado de máquina mais leves e técnicas de treinamento incremental, que permitiriam aos modelos ajustarem-se mais rapidamente. Ainda assim, encontrar o equilíbrio certo entre velocidade de adaptação e precisão do modelo permaneceu um desafio.Além disso, a segurança dos dados em um ambiente distribuído apresentou desafios significativos, especialmente na implementação de criptografia de ponta a ponta e gerenciamento de chaves em escala. A equipe propôs uma solução de gerenciamento de chaves descentralizado, que utilizava blockchain para manter um registro imutável e seguro das chaves de criptografia. Contudo, a latência adicional introduzida pela verificação de chaves via blockchain, especialmente em regiões com alta latência de rede, exigiram soluções criativas, como o caching local seguro de chaves frequentemente acessadas. METODOLOGIA: Para superar os desafios técnicos identificados, adotou-se uma metodologia de desenvolvimento experimental centrada na iteratividade, validação contínua e ajuste fino das soluções propostas. Inicialmente, realizou-se uma análise detalhada das condições de rede e padrões de uso para identificar as principais variáveis que afetam a sincronização de estado e a latência de rede.Avaliou-se também o desempenho atual do sistema de failover e caching sob diferentes cargas de trabalho para estabelecer uma linha de base, juntamente com uma análise dos protocolos de comunicação e mecanismos de segurança para identificar gargalos e vulnerabilidades.Com base nessa análise preliminar, formularam-se hipóteses para a melhoria da sincronização de estado, incluindo a implementação de um sistema de buffer adaptativo e um algoritmo de consenso modificado. Desenvolveram-se modelos de aprendizado de máquina para a previsão de demanda de caching e adaptação rápida de modelos a mudanças de padrões de acesso, além de propor um novo protocolo de comunicação experimental e um sistema descentralizado de gerenciamento de chaves.A fase seguinte envolveu a implementação de protótipos para cada uma das soluções propostas, utilizando dados simulados em ambientes controlados para avaliar a viabilidade técnica e identificar possíveis ajustes. Executaram-se testes de carga e stress para avaliar a robustez dos sistemas de failover e pooling de conexões sob condições extremas. Estes experimentos em ambiente controlado permitiram a realização de testes das soluções de sincronização de estado e caching com cargas de trabalho variáveis, ajustando os modelos de aprendizado de máquina com base nos resultados dos testes para refinar a capacidade de previsão e adaptação dos modelos.A fase de validação e ajuste fino consistiu na validação das soluções em um ambiente de produção limitado, monitorando o desempenho e coletando feedback para ajustes finos. O ajuste fino dos algoritmos e sistemas com base nos dados coletados focou na otimização de performance e segurança. Por fim, a avaliação compreensiva dos resultados obtidos em comparação com os objetivos definidos, utilizando métricas de desempenho específicas para cada desafio, permitiu uma iterativa refinamento das soluções e abordagens conforme necessário.Especificamente, para cada desafio, delinearam-se experimentos específicos, acompanhados de testes para confirmação das hipóteses. Para a sincronização de estado, testes de latência variável com simulação de diferentes condições de rede foram utilizados para validar a eficácia do buffer adaptativo e do algoritmo de consenso.No sistema de failover, simularam-se falhas em componentes críticos do sistema sob diferentes condições para avaliar a precisão e tempo de resposta do sistema de monitoramento baseado em quóruns dinâmicos. A otimização de caching foi avaliada através de testes de carga para verificar a eficácia do modelo de aprendizado de máquina na previsão de demanda e ajuste das políticas de caching.Para o pooling de conexões e protocolo de comunicação, testes de estresse avaliaram a capacidade máxima de conexões simultâneas e a eficiência do protocolo de comunicação sob cargas extremas. Na segurança de dados, a robustez do sistema de gerenciamento de chaves descentralizado e a eficácia da criptografia de ponta a ponta foram testadas através da simulação de ataques. INFORMAÇÃO COMPLEMENTAR: Os desenvolvimentos realizados demonstraram uma melhoria significativa na eficiência operacional e na gestão de recursos, graças à implementação de funções Lambda para o processamento em tempo real de eventos de autenticação e consentimento. A utilização de um sistema de banco de dados não relacional, em conjunto com estratégias de serialização e deserialização de dados, reduziu drasticamente o overhead de comunicação, resultando em uma diminuição considerável da latência de acesso aos dados. Além disso, a adoção de técnicas de caching e o pooling de conexões contribuíram para uma melhora de até 50% na performance em cenários de alta demanda, destacando a eficácia das soluções implementadas para otimizar a escalabilidade e a eficiência operacional.Além disso, a exploração de uma solução de Gestão de Identidade e Acesso (IAM) descentralizada, baseada em tecnologia blockchain e redes de confiança, apresentou resultados promissores em termos de segurança e performance. Apesar dos desafios associados à latência e aos custos das transações em blockchains públicas, a combinação de blockchains privadas com mecanismos de consenso eficientes mostrou-se uma abordagem viável. Além disso, o desenvolvimento de protocolos de criptografia de ponta a ponta e técnicas de anonimização garantiu a proteção das informações compartilhadas na rede, cumprindo com as regulamentações de privacidade de dados para a gestão de consentimentos em tempo real. RESULTADO ECONÔMICO: A integração de funções Lambda e bancos de dados não relacionais reduziu o overhead de comunicação e melhorou a eficiência operacional, diminuindo custos operacionais em cenários de alta demanda. RESULTADO INOVAÇÃO: A inovação trouxe entendimentos valiosos de como desenvolver um sistema IAM descentralizado que une blockchain e redes de confiança, elevando a segurança e eficiência na gestão de consentimentos. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;"NÚMERO: 10 ID ÚNICO: 109730 NOME: SOLUÇÕES AVANÇADAS DE INTELIGENCIA ARTIFICIAL PARA SISTEMAS BANCÁRIOS DESCRIÇÃO: Esta linha de pesquisa trata de evoluções relacionadas a técnicas de inteligência artificial de forma a viabilizar a análise de dados bancários e novos sistemas avançados de autenticação de usuários.Visando construir uma solução para analisar, processar e interpretar grandes volumes de dados bancários, foram organizadas atividades com objetivo tecnológico de construir um modelo de Machine Learning que lidasse com análises preditivas e comportamentais extraídas de dados históricos e transacionais. Tal modelo deveria absorver demandas de segurança e privacidade dos dados,  foram organizadas atividades com objetivo tecnológico de desenvolver práticas avançadas de criptografia, incluindo criptografia de ponta a ponta e técnicas de anonimização de dados, assegurando a conformidade com regulamentações globais de proteção de dados.Com o advento da tecnologia, novas formas de aplicar golpes, processos/transações fraudulentas surgiram e suas ocorrências cresceram consideravelmente. Aza, I., Munir, K., & Almutairi, M. (2022). Dado o problema, a presente linha de pesquisa busca abordar também a experimentação de tecnologias baseadas em IA Generativa para a ideação e desenvolvimento de soluções que diminuam as ocorrências por fraudes em transações/processos bancários, sendo então realizados estudos e a fim de obter novos conhecimentos capazes de viabilizar a construção dessas soluções. Vorobyev, I., & Krivitskaya, A. (2022).O desafio de processar eficientemente grandes conjuntos de dados em tempo real para a detecção de fraudes era elevado, dada a arquitetura subotimizada das redes neurais e por implementações de código que não maximizavam a performance. A integração de modelos de aprendizado de máquina em sistemas legados acabava por gerar problemas como serialização e desserialização decorrentes de incompatibilidades entre as versões das bibliotecas utilizadas nos ambientes de desenvolvimento e produção. Essas dificuldades não apenas retardavam o processo de treinamento e implementação dos modelos, mas também colocavam em risco a integridade e a segurança dos dados dos clientes.Para o desenvolvimento de um sistema avançado de autenticação e identificação de usuários, o objetivo tecnológico era entender algoritmos de reconhecimento facial. Este sistema foi pensado para aprimorar a segurança das transações e acessos aos serviços bancários. As atividades experimentais se voltaram a entender técnicas de aprendizado profundo buscando comprovações de sistema capaz de analisar características faciais únicas de cada usuário com alta precisão, mesmo em condições variáveis de iluminação e ângulos de face.Espera-se que os desenvolvimentos presentes nesta linha de pesquisa resultem em um aumento considerável da eficácia da detecção de fraudes e na proteção de dados. A otimização da arquitetura da rede neural, juntamente com a implementação eficiente do código, visa obter uma capacidade aprimorada de processamento de dados em tempo real, resultando em modelos mais precisos e rápidos.Ademais, um marco crítico no desenvolvimento deste projeto foi a superação dos desafios associados à integração efetiva de tecnologias de inteligência artificial, especificamente um modelo de Machine Learning avançado, dentro do ecossistema de sistemas bancários legados. A capacidade de desenvolver e validar um modelo de aprendizado de máquina capaz de realizar análises preditivas e comportamentais, processando volumes massivos de dados bancários em tempo real para a detecção de fraudes, constituiu um avanço significativo.Além disso, o desenvolvimento bem sucedido e a validação de práticas inovadoras de criptografia e anonimização de dados, adaptadas para trabalhar em conjunto com o modelo de IA, marcaram outro ponto de virada. Este avanço foi crítico ao demonstrar que é possível fortalecer a segurança dos dados sensíveis dos clientes e, ao mesmo tempo, utilizar a inteligência artificial para melhorar a autenticação de usuários e a detecção de fraudes. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Inteligência artificial,  biometria facial,  IA generativa,  ciência de dados,  machine learning,  visão computacional. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Os desenvolvimentos desta linha de pesquisa viabilizaram um conjunto de aprendizados.Foram organizados estudos buscando entendimentos para viabilizar construção de modelos de machine learning (ML) e ciência de dados para processar e analisar informações de clientes advindas de diversas fontes de dados. O intuito era de gerar insights personalizados e recomendações de produtos e serviços. Como elemento novo, entendeu-se a integração a partir do AWS Glue para orquestração dos dados.Foi possível comprovar que a implementação de modelos de ML avançados, combinados com técnicas de ciência de dados (multi-estrutural) e o poder de processamento do AWS Glue, poderia permitir uma análise detalhada e em tempo real das informações dos clientes, resultando em recomendações personalizadas, análises de perfil comportamental, definição de métricas e cálculos para definição de estratégias.Atividades experimentais permitiram entender a codificação de algoritmos de clustering, como K-means, para segmentar os clientes com base em comportamentos e preferências,  criação de modelos de regressão e árvores de decisão para prever comportamentos futuros dos clientes, como a probabilidade de churn ou a propensão para adquirir novos produtos,  desenvolvimento de mecanismos de recomendação utilizando técnicas de filtragem colaborativa e aprendizado profundo para sugerir produtos e serviços adaptados às necessidades individuais dos clientes.Com isso, houve um conjunto de constatações. Os modelos de regressão logística e árvores de decisão foram treinados para prever comportamentos futuros dos clientes, como a probabilidade de churn. O modelo de árvore de decisão alcançou uma precisão de 85% e uma pontuação F1 de 0.82, enquanto o modelo de regressão logística apresentou uma precisão de 80% e uma pontuação F1 de 0.78.Foram organizados experimentos visando comprovação de viabilidade dos recursos de IA para biometria facial. Houve a compreensão de mecanismos de autenticação biométrica multifatorial dinâmica que integrasse reconhecimento facial utilizando Redes Neurais Convolucionais (CNNs), juntamente com outros indicadores biométricos e comportamentais. Os entendimentos possibilitaram a construção de uma solução altamente adaptativa para diversos canais do banco, sendo agnóstica a todo o ecossistema da empresa.Estudos levaram ao entendimento de que a combinação de múltiplas formas de autenticação biométrica e comportamental, ajustadas dinamicamente de acordo com o contexto da transação e o perfil de risco do usuário, poderia oferecer uma segurança significativamente melhorada para transações bancárias, reduzindo a incidência de fraudes e melhorando a experiência do usuário. Para isso, foram organizados estudos que permitiram aprendizados para desenvolvimento de algoritmos para normalizar as entradas biométricas de diferentes canais. Para o treinamento de modelos, houve a compreensão da aplicação de um conjunto de dados diversificado para treinar modelos de CNN para reconhecimento facial, juntamente com modelos para outras modalidades biométricas. Além disso, como elemento novo, foi criado um sistema de pontuação de risco para avaliar o contexto da transação e ajustar dinamicamente os requisitos de autenticação.Foi possível constatar que a precisão combinada das modalidades biométricas supera as soluções de fator único ou duplo, atualmente em uso. Além disso, foi possível observar nos experimentos que a capacidade do sistema de ajustar os requisitos de autenticação com base no risco demonstra uma melhoria na segurança sem comprometer a conveniência.Experimentos levaram ao entendimento de como redistribuir valores de intensidade dos pixels de forma que o histograma da imagem resultante seja uniformemente distribuído, e como simular a incidência de luz sobre o rosto usando modelos físicos. Isso envolveu a aplicação de técnicas de renderização que ajustam artificialmente a iluminação da imagem, tentando replicar as condições de iluminação ideais. DESAFIO TECNOLÓGICO: Os principais desafios enfrentados pelos times de pesquisadores foram:Integrar dados heterogêneos, que variam em formato (estruturados, semi-estruturados, não estruturados) e fonte (transações on-line, interações de call center, logs de dispositivos móveis), em um formato unificado para análise. Foi considerada a hipótese de desenvolver uma integração de dados heterogêneos através de um middleware de integração e estruturando um gerenciador de APIs. Essa hipótese trazia o fato de que estruturar APIs sem configurações de segurança robustas, como encriptação de dados em trânsito e em repouso, autenticação forte e autorização com base em roles, poderia expor dados sensíveis a ataques de interceptação, Man-in-the-Middle (MitM) ou brechas de segurança em APIs públicas. Era preciso mensurar os riscos e definir todos os pontos de incerteza para que se chegasse em uma solução viável.Superar ataques de apresentação avançados (spoofing) e deepfakes: Os deepfakes utilizam redes neurais generativas adversariais (GANs) para criar imagens de faces extremamente realistas que poderiam enganar sistemas de biometria facial. Além disso, técnicas de spoofing como máscaras 3D, fotos de alta resolução e vídeos são usadas para tentar burlar os sistemas de segurança.Foi levantada a hipótese de abordar a detecção de ""vivacidade"" (liveness detection) que analisa micro-movimentos faciais e padrões de piscar, impossíveis de replicar em fotos ou vídeos. Isso poderia ser realizado através de algoritmos que capturariam e analisariam sequências de imagens em tempo real, com o intuito de identificar sutis movimentos naturais da face e da respiração. Foi pensada a hipótese de aplicar redes neurais convolucionais (CNNs) especialmente treinadas para distinguir entre faces reais e imagens sintéticas geradas por GANs. Essa hipótese envolveria o treinamento dessas redes com vastos conjuntos de dados compostos por imagens reais e deepfakes para aprimorar sua capacidade de detecção.Desafio de desenvolver um sistema de identificação capaz de analisar e processar imagens em baixa quantidade de luz. O time pensou na hipótese de codificar técnicas como a normalização de histograma, que poderia ajustar o contraste da imagem, e a transformação de iluminação baseada em modelos físicos que simulariam a incidência de luz sobre o rosto. Essas técnicas, em tese, poderiam ajudar a garantir que o sistema de reconhecimento facial possa operar eficientemente sob diferentes condições de iluminação.Levantou-se também a hipótese de levantar redes neurais siamesas que são treinadas não apenas para reconhecer as características faciais, mas também para entender e adaptar-se às variações na aparência de um mesmo indivíduo. Koshy, R., & Mahmood, A. (2020).Modelagem preditiva com dados esparsos. O desafio aqui era melhorar a precisão dos modelos de ML em face de dados esparsos, especialmente em segmentos de clientes com poucas interações ou transações. Uma possível solução seria explorar técnicas de aprendizado profundo, como redes neurais autoencoders, para identificar padrões latentes em dados esparsos. A combinação dessas técnicas com métodos de imputação sofisticados poderia melhorar a densidade dos dados. METODOLOGIA: Para superar as barreiras, foram organizadas as seguintes atividades experimentais:Estudo sobre Middleware de Integração e Gerenciamento de APIs: Uma investigação profunda foi realizada para desenvolver um middleware de integração capaz de unificar dados de formatos e fontes variadas. Além disso, foi realizado um estudo sobre a estruturação de um gerenciador de APIs que enfatizasse configurações de segurança robustas. Essas pesquisas abordaram riscos potenciais, como ataques de interceptação e brechas de segurança, propondo soluções para mitigá-los.Pesquisa em Detecção de ""Vivacidade"": Foi conduzida uma análise sobre o uso de algoritmos para capturar e analisar sequências de imagens, visando identificar micro-movimentos faciais e padrões de piscar. Este estudo explorou a viabilidade de implementar tais técnicas para fortalecer a segurança contra spoofing e deepfakes.Estudo sobre Redes Neurais Convolucionais (CNNs): Uma pesquisa extensiva foi realizada sobre o treinamento de CNNs com vastos conjuntos de dados, incluindo imagens reais e deepfakes, para aprimorar a capacidade de distinguir entre faces autênticas e sintéticas.Pesquisa em Técnicas de Ajuste de Contraste e Modelos de Iluminação: Estudos foram realizados sobre a aplicação de normalização de histograma e transformação de iluminação para melhorar a qualidade de imagens capturadas em condições de baixa luz, visando a eficácia do reconhecimento facial.Estudo sobre Redes Neurais Siamesas: Uma pesquisa abordou o desenvolvimento de redes neurais siamesas que reconhecem as características faciais e adaptam-se às variações na aparência, mesmo sob diferentes condições de iluminação.Pesquisa em Aprendizado Profundo e Técnicas de Imputação: Um estudo foi dedicado à exploração de redes neurais autoencoders e métodos sofisticados de imputação para identificar padrões latentes em dados esparsos. Essa pesquisa visou melhorar a densidade dos dados e, consequentemente, a precisão dos modelos de machine learning. INFORMAÇÃO COMPLEMENTAR: O desenvolvimento de mecanismos da biometria facial envolveu a coleta e processamento de grandes conjuntos de dados de imagens faciais, respeitando estritamente as regulamentações de privacidade e proteção de dados. O sistema foi treinado para distinguir efetivamente entre imagens autênticas e tentativas de spoofing, utilizando técnicas de detecção de vivacidade para prevenir acessos não autorizados através de fotos, vídeos ou máscaras.A infraestrutura de processamento de dados foi desenhada para suportar o processamento em tempo real, utilizando tecnologias como Amazon Kinesis e Apache Kafka, permitindo que os modelos de Machine Learning operassem com latência mínima. Isso possibilitou a entrega de recomendações e análises personalizadas quase instantaneamente, melhorando a experiência do usuário ao interagir com os serviços bancários. Além disso, a solução foi projetada para ser altamente escalável, utilizando orquestração de contêineres com Kubernetes e AWS ECS, facilitando a gestão de microserviços e a implementação de atualizações e melhorias contínuas.Para validar a eficácia e a robustez dos modelos de machine learning e ciência de dados desenvolvidos, foram realizadas extensas etapas de testes e prototipagem, envolvendo diversas equipes da empresa. Inicialmente, equipes de ciência de dados e engenharia de software colaboraram para criar protótipos dos algoritmos de clustering, regressão e árvores de decisão. Esses protótipos foram submetidos a rigorosos testes de desempenho e precisão, onde se constatou, por exemplo, que o modelo de árvore de decisão alcançou uma precisão de 85% e uma pontuação F1 de 0.82.Em seguida, equipes trabalharam em conjunto para testar e validar serviços de orquestração e processamento dos dados, garantindo que a integração dos diversos fluxos fosse eficiente e segura. Paralelamente, a equipe de segurança da informação conduziu experimentos para testar a viabilidade dos mecanismos de autenticação biométrica multifatorial, utilizando Redes Neurais Convolucionais (CNNs) e outros indicadores biométricos e comportamentais. Esses testes envolveram a participação de usuários internos que simularam diversas condições de uso, permitindo ajustar e refinar os algoritmos de reconhecimento facial e outras modalidades biométricas. A colaboração entre essas equipes foi crucial para identificar e resolver desafios técnicos, garantindo que os modelos fossem não apenas precisos, mas também seguros e escaláveis.REFERÊNCIAS BIBLIOGRÁFICAS:Vorobyev, I., & Krivitskaya, A. (2022). Reducing false positives in bank anti-fraud systems basedon rule induction in distributed tree-based models. Computers & Security, 120, 102786.Koshy, R., & Mahmood, A. (2020). Enhanced deep learning architectures for face liveness detection forstatic and video sequences. Entropy, 22(10), 1186.Aza, I., Munir, K., & Almutairi, M. (2022). Deepfake: A threat to cybersecurity and community safety.Applied Sciences, 12(19), 9820. RESULTADO ECONÔMICO: As atividades de inovação apresentadas resultaram na otimização dos processos de detecção de fraude diminuiu o tempo de análise em aproximadamente 40%, aumentando a eficiência e reduzindo a necessidade de recursos humanos e técnicos. RESULTADO INOVAÇÃO: Obteve-se avanços notáveis na segurança de dados, com a integração de criptografia ponta a ponta e gestão de tokens, melhora na proteção contra fraudes e evolução no processamento de dados sensíveis. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 11 ID ÚNICO: 109731 NOME: NOVAS TECNOLOGIAS PARA ESTRUTURAÇÃO DE COMPONENTES DE ATENDIMENTO E SEGURANÇA DOS DADOS DE CLIENTES DESCRIÇÃO: A presente linha de pesquisa busca a obtenção de novos conhecimentos quanto à segurança de dados e a melhoria da experiência de atendimento ao cliente para a empresa, focando no desenvolvimento de tecnologias de ponta como sistemas de autenticação multifatorial (MFA) e criptografia de ponta a ponta (E2EE). Além disso, visa integrar chatbots alimentados por inteligência artificial (IA) em ambiente cloud, com o objetivo de tornar a interação com o cliente mais eficiente e personalizada. Essas tecnologias prometem elevar a proteção contra ameaças digitais e aprimorar a qualidade do serviço ao cliente, atendendo às suas necessidades de maneira mais intuitiva e precisa.Para alcançar os objetivos propostos, o projeto delineia uma série de atividades experimentais, incluindo o desenvolvimento e teste de protótipos de MFA, a implementação de algoritmos de criptografia E2EE, e a criação e o treinamento de modelos de IA para chatbots, utilizando técnicas de processamento de linguagem natural (PLN). Essas atividades são essenciais para validar as soluções propostas, assegurando sua eficácia e segurança em proteger dados sensíveis e melhorar a interação com o usuário. Através desses esforços, o projeto busca gerar conhecimento avançado sobre a aplicação dessas tecnologias no contexto bancário, enfatizando a importância da inovação contínua para enfrentar desafios de segurança e atendimento ao cliente. Moepi, G. L., & Mathonsi, T. E. (2021, December)O resultado esperado deste projeto é a formulação de respostas precisas e personalizadas às consultas dos usuários, melhorando significativamente a proteção de dados sensíveis e a experiência do cliente na empresa. Espera-se que as inovações implementadas, particularmente os avanços em MFA, criptografia E2EE e a integração de chatbots de IA, configurem um ambiente bancário mais seguro contra violações de dados, e proporcionem um serviço de atendimento ao cliente mais intuitivo e eficiente.Um ponto crítico neste projeto de inovação tecnológica foi a implementação bem-sucedida e a integração operacional de um sistema de autenticação multifatorial (MFA) robusto, combinado com a criptografia de ponta a ponta (E2EE), em ambiente de produção da empresa. Esse marco foi crítico, pois demonstrou a viabilidade de elevar significativamente a segurança dos dados dos clientes, ao mesmo tempo em que se mantém uma experiência de usuário fluida e sem interrupções. A superação dos desafios técnicos e operacionais para harmonizar essas tecnologias de segurança com os sistemas legados do banco, sem comprometer a usabilidade ou a performance, representou um avanço notável.Além disso, a criação, o treinamento e a integração bem-sucedidos de chatbots alimentados por inteligência artificial (IA) em ambiente cloud, utilizando técnicas avançadas de processamento de linguagem natural (PLN), constituiu outro marco crítico deste projeto. Este avanço foi crítico ao transformar a interação do banco com seus clientes, tornando-a mais eficiente, personalizada e intuitiva. A capacidade dos chatbots de IA de entender e responder às consultas dos usuários em tempo real, com respostas precisas e personalizadas, enquanto garantem a proteção de dados sensíveis, possibilitou um avanço significativa na experiência de atendimento ao cliente. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Autenticação,  integração,  experimentos,  chaves,  dados,  chatbots,  multifatorial,  infraestrutura,  resiliência. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: São elementos de novidade nesta linha de pesquisa: A compreensão de que era possível integrar chamadas de API para serviços de verificação de identidade, utilizando SDKs de autenticação biométrica para dispositivos móveis por meio da construção de um módulo de Autenticação Multifatorial (MFA). Isso foi possível por meio da execução de experimentos, que até constataram uma ligeira latência adicional no processo de autenticação devido às múltiplas etapas de verificação, mas sem comprometer significativamente a experiência do usuário.Foram simuladas 10.000 tentativas de login simultâneas para medir latência e desempenho, resultando em tempos médios de resposta de 100ms para verificação de credenciais, 150ms para impressão digital e 220ms para reconhecimento facial, com uma taxa de sucesso de 98% dentro de 1 segundo, constatando que o sistema suportava alta carga com eficiência. Foram comparadas ainda 20.000 imagens faciais e 20.000 impressões digitais para avaliar precisão e confiabilidade, obtendo-se taxas de falsos positivos de 0.5% para reconhecimento facial e 0.3% para impressão digital, e taxas de falsos negativos de 1% e 0.7%, respectivamente, demonstrando alta precisão e confiabilidade. Testes de penetração e simulações de falhas demonstraram que o sistema bloqueou tentativas excessivas de login após 5 tentativas falhas, nenhuma tentativa de spoofing foi bem-sucedida, e o sistema manteve funcionalidade reduzida durante falhas de componentes, graças à arquitetura distribuída e redundância, evidenciando robustez e resiliência.Comprovação de viabilidade do desenvolvimento de E2EE nas comunicações entre o cliente e os servidores do banco usando protocolos TLS (Transport Layer Security) para a camada de transporte e algoritmos AES (Advanced Encryption Standard) com chaves de 256 bits para criptografar mensagens no lado do cliente antes de serem enviadas. Isso possibilita o desenvolvimento de mecanismos de troca de chaves segura, empregando o protocolo Diffie-Hellman para a geração de chaves compartilhadas sem a necessidade de um canal seguro prévio.A execução de experimentos com o EventBridge para orquestrar eventos entre serviços na cloud trouxe o entendimento de que é possível estruturar um modelo de comunicação assíncrona e acoplamento frouxo entre os componentes do sistema de atendimento, por meio da configuração de regras de eventos que disparam funções Lambda. Essas por sua vez, executam lógicas de negócios específicas, como a iniciação de sessões de chatbot.Para melhorar a precisão na identificação de intenções dos usuários por Chatbots de IA, realizou-se um experimento utilizando tecnologias avançadas de aprendizado profundo e Processamento de Linguagem Natural (NLP). Inicialmente, coletou-se um vasto conjunto de dados de interações reais com usuários, abrangendo diversas expressões linguísticas e contextos culturais. Após a coleta, os dados foram pré-processados para remover ruídos e padronizar os formatos de texto.Modelos de transformadores BERT (Bidirectional Encoder Representations from Transformers) e GPT (Generative Pre-trained Transformer), foram treinados devido à sua capacidade de compreender contextos bidirecionais e gerar texto coerente. Adicionalmente, Redes Neurais Recorrentes (RNNs) com LSTMs (Long Short-Term Memory) foram aplicadas para analisar sequências de texto e prever intenções baseadas em contextos anteriores.Foram desenvolvidas técnicas de análise de sentimento utilizando ferramentas como VADER (Valence Aware Dictionary and sEntiment Reasoner) para entender as emoções subjacentes às mensagens dos usuários. Algoritmos de clusterização foram utilizados para agrupar mensagens semelhantes, facilitando a identificação de padrões de intenção. Os resultados preliminares mostraram uma melhoria significativa na precisão da identificação de intenções, com BERT e GPT demonstrando excelente desempenho na compreensão de contextos complexos e geração de respostas relevantes. DESAFIO TECNOLÓGICO: Durante o desenvolvimento dos experimentos com o MFA, os pesquisadores enfrentaram dois grandes desafios. O primeiro foi a latência na comunicação entre os componentes distribuídos do sistema, especialmente durante a verificação de dados biométricos em tempo real. A hipótese era que a configuração de uma rede de baixa latência e o uso de técnicas de otimização de rede, como caching distribuído, poderiam reduzir significativamente os atrasos. No entanto, a complexidade de criar e manter essas otimizações em um ambiente de nuvem pública representava um risco considerável, pois exigiria um monitoramento contínuo e ajustes dinâmicos.O segundo desafio foi a precisão do reconhecimento facial em condições de baixa iluminação e variações faciais, como uso de acessórios ou mudanças na expressão. A hipótese para resolver esse problema envolvia o treinamento de modelos de reconhecimento facial mais robustos utilizando redes neurais convolucionais (CNNs) e um conjunto de dados diversificado. No entanto, essa abordagem exigia um nível avançado de engenharia de dados, incluindo a coleta e rotulação de um grande volume de dados variáveis, além de ajustes finos nos hiperparâmetros dos modelos. A dificuldade técnica em garantir a qualidade e a diversidade dos dados de treinamento, bem como a necessidade de ajustar continuamente os modelos para manter a precisão, representava um risco significativo para a viabilidade técnica da solução.Configurou-se um grande desafio gerenciar chaves criptográficas de forma segura, especialmente no ambiente distribuído. A troca de chaves segura sem comprometer a facilidade de uso para o usuário final ou a segurança dos dados também era um ponto de incerteza, dado a complexidade técnica.Para esse problema, o time pensou na hipótese de adotar um sistema de gerenciamento de chaves (KMS) que suportasse rotação automática de chaves e armazenamento seguro. Foi pensada também a alternativa de desenvolvimento do protocolo Diffie-Hellman para a troca de chaves, sendo necessário explorar mais esse ponto para ter certeza, dado que reside uma complexidade em manter a infraestrutura segura contra ataques de intermediários.A ambiguidade linguística, com palavras e frases possuindo múltiplos significados contextuais, dificultou a precisão dos modelos de NLP, especialmente em diálogos curtos. As nuances culturais, incluindo expressões idiomáticas e gírias, variaram amplamente, complicando a interpretação correta pelos modelos de IA. Nah, F. F.-H., Zheng, R., Cai, J., Siau, K., & Chen, L. (2023).Além disso, as limitações computacionais representaram um desafio significativo, dado que o treinamento de modelos avançados de NLP é intensivo em recursos e requer hardware especializado.Uma possível hipótese de solução poderia envolver a integração de técnicas de aprendizado por reforço, ajustando os modelos com base no feedback dos usuários em tempo real. Outra abordagem seria a utilização de modelos híbridos que combinam transformadores com técnicas baseadas em regras culturais e linguísticas específicas. No entanto, essas soluções apresentam incertezas em termos de viabilidade computacional e precisão, exigindo rigorosos testes e investimentos significativos.Manter a arquitetura baseada em eventos escalável e resiliente diante de picos de demanda inesperados representou um desafio técnico significativo. A complexidade aumenta com a integração de múltiplos serviços AWS e a necessidade de garantir a entrega de eventos em tempo real sem perda de dados.Dado o problema, foi pensada a hipótese de desenvolver serviços na cloud como o SQS para filas de mensagens e o a AWS SNS para notificações, que poderia facilitar o gerenciamento de picos de carga. A criação de padrões de design como circuit breaker poderia ajudar a aumentar a resiliência. Ainda assim, a configuração ideal que garanta escalabilidade e resiliência sob todas as condições possíveis demandava contínua experimentação para mais comprovações. METODOLOGIA: APIs internas foram expostas e integradas com funções serverless para permitir comunicação bidirecional entre os diferentes componentes do sistema. Desenvolveu-se um pipeline de captura de imagens faciais utilizando câmeras de alta resolução, e os sensores de impressão digital foram integrados com estações de trabalho para capturar dados biométricos.Tanto as imagens faciais quanto os dados de impressões digitais foram enviados para funções serverless que processaram e compararam esses dados com os armazenados no banco de dados. O fluxo de autenticação multifatorial foi gerenciado por serviços de identidade na nuvem, com verificação de credenciais, impressão digital e reconhecimento facial em tempo real.Estudo e experimentação com algoritmos de cache seguros para tokens de autenticação: este trabalho envolveu o desenvolvimento de algoritmos de cache que garantem a segurança dos tokens de autenticação. Os experimentos realizados incluíram testes de penetração para avaliar a resistência desses algoritmos contra ataques, bem como análises de performance para verificar a eficácia do cache em acelerar o processo de autenticação sem comprometer a segurança.Para abordar a complexidade na compreensão de intenções pelos Chatbots de IA, dados coletados foram submetidos a um rigoroso processo de pré-processamento para remover ruídos, como erros tipográficos e informações irrelevantes, e para padronizar os formatos de texto, garantindo consistência. Em seguida, treinou-se os modelos utilizando transformadores, como BERT e GPT, devido à sua capacidade de captar nuances contextuais e semânticas da linguagem. Adicionalmente, aplicaram-se Redes Neurais Recorrentes (RNNs) com LSTMs para analisar sequências de texto e prever intenções baseadas em contextos anteriores.Implementaram-se técnicas de análise de sentimento para entender as emoções subjacentes às mensagens dos usuários e algoritmos de clusterização para agrupar mensagens semelhantes. Para avaliar a precisão dos modelos, utilizaram-se técnicas de validação cruzada, ajustando-se os hiperparâmetros conforme necessário. Um sistema de feedback contínuo foi implementado, onde as respostas dos chatbots foram monitoradas e avaliadas pelos usuários, permitindo ajustes e refinamentos constantes nos modelos. Periodicamente, os modelos foram atualizados com novos dados coletados e feedback dos usuários para garantir que o chatbot permanecesse relevante e eficaz.Foram realizados também estudos de caso práticos, incluindo a implementação de circuit breakers e bulkheads em sistemas de teste, para avaliar o impacto desses padrões na resiliência e escalabilidade de sistemas distribuídos sob alta demanda.Estudos para otimização de algoritmos de reconhecimento biométrico, que teve por objetivo reduzir a latência induzida por sistemas de autenticação multifator (MFA) com foco em métodos biométricos.Isso envolveu a criação e teste de novos algoritmos de reconhecimento biométrico focados em eficiência e velocidade,  comparação dos tempos de resposta dos novos algoritmos com os antigos em diversos dispositivos e condições de uso,  testes de cache para tokens de autenticação, avaliando o impacto na velocidade e segurança, com o intuito de obter uma redução significativa no tempo de processamento da autenticação biométrica, mantendo ou aumentando os níveis de segurança.Estudos e experimentações do protocolo Diffie-Hellman Modificado. Isso envolveu a exploração da viabilidade de um protocolo Diffie-Hellman modificado para a troca segura de chaves em um ambiente distribuído.Inicialmente foi realizada a modificação do protocolo Diffie-Hellman para aumentar a segurança contra ataques de intermediários,  depois a execução de simulações de ataque para testar a robustez do protocolo modificado,  em seguida foi feita a aplicação do protocolo em um ambiente de teste para avaliar a usabilidade e segurança, com o intuito de melhorar a eficiência e resistência a ataques de intermediários. INFORMAÇÃO COMPLEMENTAR: Com base nos estudos e experimentos realizados pelos pesquisadores envolvidos, evidenciou-se que a integração das tecnologias mencionadas, juntamente com a implementação de estratégias de autenticação avançadas como Autenticação Multifatorial (MFA) e autenticação de ponta a ponta, oferece à empresa uma infraestrutura robusta e segura, capaz de lidar com as exigências operacionais e de segurança do ramo bancário.Observa-se que a experimentação do API Gateway como ponto de entrada para as solicitações dos chatbots de IA permite um gerenciamento eficaz e seguro dessas interações, garantindo que apenas solicitações autenticadas sejam processadas, enquanto o AWS Lambda, operando em uma arquitetura sem servidor, assegura uma resposta dinâmica às variações de demanda sem a necessidade de intervenção manual para escalonamento de recursos. Esta abordagem não apenas maximiza a eficiência operacional, mas também minimiza os custos relacionados à manutenção de infraestrutura física.Além disso, a adoção de MFA e autenticação de ponta a ponta introduz camadas adicionais de proteção, essenciais para a salvaguarda de transações e dados sensíveis. A MFA adiciona uma barreira significativa contra acessos não autorizados, exigindo a verificação de identidade por meio de múltiplos fatores independentes, enquanto a autenticação de ponta a ponta garante que todas as comunicações entre o cliente e o sistema sejam criptografadas, protegendo-as contra interceptações maliciosas.O desenvolvimento do CloudWatch para o monitoramento contínuo do desempenho dos chatbots proporciona uma capacidade de observação e ajuste em tempo real, facilitando a detecção e correção de problemas operacionais de forma ágil, o que é crucial para a manutenção da estabilidade e confiabilidade do sistema em um ambiente tão crítico quanto o bancário.REFERÊNCIAS BIBLIOGRÁFICAS:Nah, F. F.-H., Zheng, R., Cai, J., Siau, K., & Chen, L. (2023). Generative AI and ChatGPT:Applications, challenges, and AI-human collaboration. Journal of Organizational Computingand Electronic Commerce, 33(4), 277-304.Moepi, G. L., & Mathonsi, T. E. (2021, December). Multi-Factor Authentication Method for Online Banking Services in South Africa. In 2021 International Conference on Electrical, Computer and Energy Technologies (ICECET) (pp. 1-5). IEEE. RESULTADO ECONÔMICO: Maiores níveis de satisfação do cliente, alinhado ao aprimoramento do atendimento, o que se converte em retenção de clientes e captação de mais oportunidades e core para o banco. RESULTADO INOVAÇÃO: Obtenção de novos conhecimentos a respeito de tecnologias como ML e PLN,  evolução de soluções de atendimento com obtenção de novos aprendizados em segurança da informação e dados sensíveis de clientes. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;"NÚMERO: 12 ID ÚNICO: 109732 NOME: SOLUÇÕES PARA PREVENÇÃO A FRAUDE E SISTEMAS DE AUTENTICAÇÃO PARA PLATAFORMA BANCÁRIA AGNÓSTICA AO BANCO DESCRIÇÃO: O projeto teve como objetivo principal avaliar a eficácia de um sistema de autenticação multifatorial (MFA) avançado, integrando tecnologias de reconhecimento biométrico, OTP via SMS, tokens de hardware e algoritmos de inteligência artificial (IA) para a detecção de comportamentos fraudulentos, visando aumentar a segurança nas transações e operações de login em plataformas bancárias. Antes da implementação deste sistema, as plataformas dependiam de métodos de autenticação menos sofisticados, o que deixava margem para uma maior ocorrência de fraudes e acessos não autorizados. Com a integração do MFA e algoritmos de IA, esperava-se uma redução significativa nessas incidências, melhorando assim a segurança e a confiança nas operações bancárias online. Sinigaglia, F., Carbone, R., Costa, G., & Zannone, N. (2020).Durante o desenvolvimento do projeto, foram realizadas análises comparativas entre usuários de um sistema de autenticação de fator único e aqueles utilizando o sistema MFA proposto. Os resultados obtidos mostraram uma diminuição considerável nas tentativas bem-sucedidas de fraude e nos acessos não autorizados no grupo que utilizou o MFA, confirmando a eficácia da abordagem multifatorial. Ajustes foram feitos nos algoritmos de IA para otimizar a detecção de fraudes e minimizar falsos positivos, melhorando a experiência do usuário sem comprometer a segurança. Além disso, foram realizadas adaptações nos métodos de autenticação e na interface de usuário, visando um melhor equilíbrio entre segurança e usabilidade.Após os desenvolvimentos e ajustes, o projeto demonstrou a viabilidade e os benefícios de integrar um sistema de autenticação multifatorial avançado em plataformas bancárias. A implementação de tecnologias biométricas, juntamente com a aplicação de algoritmos de IA para análise comportamental, provou ser uma estratégia eficaz na prevenção de fraudes e acessos não autorizados.A demonstração bem-sucedida da eficácia do sistema de autenticação multifatorial (MFA) avançado, que integra tecnologias de reconhecimento biométrico, OTP (One-Time Password) via SMS, tokens de hardware e algoritmos de inteligência artificial para a detecção de comportamentos fraudulentos em plataformas bancárias pode ser considerado um ponto chave nas atividades de desenvolvimento deste projeto. Esse marco foi alcançado após a conclusão de um estudo comparativo abrangente, que revelou uma diminuição significativa nas incidências de fraudes e acessos não autorizados entre os usuários que adotaram o sistema MFA em comparação com aqueles que dependiam de métodos de autenticação menos sofisticados.A realização de ajustes finos nos algoritmos de IA para aprimorar a detecção de atividades fraudulentas e a redução de falsos positivos, juntamente com as adaptações na interface de usuário para facilitar a usabilidade, representou um avanço significativo. Esse resultado comprovou a viabilidade do desenvolvimento do sistema MFA proposto sobre métodos de autenticação, além de fortalecer a segurança e conveniência para as operações bancárias online. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Autenticação,  MFA (Autenticação Multifatorial),  IA,  fraude,  segurança,  algoritmos,  reconhecimento biométrico,  OTP via SMS,  tokens de hardware. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: São elementos tecnologicamente inovadores deste projeto:Experimentos foram conduzidos com o objetivo de obter insumos para a criação de uma solução de middleware eficiente, que fosse capaz de garantir a segurança cibernética. Um dos experimentos envolveu a simulação de ataques de repetição e man-in-the-middle contra diversas configurações de autenticação. Observou-se que a vulnerabilidade diminuía significativamente ao utilizar-se uma combinação inédita de TLS (Transport Layer Security) com autenticação baseada em tokens de hardware. Esta combinação possibilitou a obtenção de novos conhecimentos: observou-se uma redução substancial nas vulnerabilidades a ataques de repetição e man-in-the-middle (MitM). No caso dos ataques de repetição, os tokens de hardware, que incorporam elementos temporais ou de uso único, impediram eficazmente a reutilização de mensagens interceptadas. Em relação aos ataques MitM, a integração de TLS com tokens de hardware tornou extremamente difícil a inserção de atacantes na comunicação, garantindo a detecção imediata de qualquer tentativa de interceptação ou alteração.Além disso, esses conhecimentos possibilitaram o desenvolvimento de um protocolo de comunicação personalizado. Este protocolo integra criptografia de ponta a ponta, assegurando que os dados permaneçam criptografados durante todo o trajeto, do emissor ao receptor. Também implementa um sistema de autenticação baseado em desafio-resposta, onde um desafio é enviado e deve ser corretamente respondido, aumentando a segurança em relação ao uso de senhas estáticas.Além disso, no desenvolvimento do sistema de pontuação de risco dinâmico, empregou-se uma abordagem customizada na análise de dados para extrair padrões associados a transações fraudulentas. Utilizando uma combinação avançada de técnicas de aprendizado de máquina, como redes neurais convolucionais (CNNs) e máquinas de vetores de suporte (SVMs), analisaram-se milhões de transações. Um dos avanços significativos foi a compreensão de como realizar a aplicação de Análise de Componentes Principais (PCA) para a redução da dimensionalidade dos dados. Esse entendimento permitiu descobrir características latentes fortemente indicativas de fraude. Essa descoberta foi fundamental para o desenvolvimento de um modelo híbrido, que combinou CNNs para detecção de padrões complexos e SVMs para classificação baseada nas características latentes, permitindo a identificação e a prevenção proativa de fraudes.Ao aplicar técnicas de explicabilidade em modelos de ML complexos, foram realizados experimentos para avaliar a eficácia de diversos métodos na identificação de características importantes. Um desses experimentos envolveu o emprego de SHAP (SHapley Additive exPlanations) para decompor as previsões de uma rede neural profunda treinada para detectar fraudes, comparando os resultados com aqueles obtidos por meio do uso de LIME (Local Interpretable Model-agnostic Explanations). Observou-se que SHAP fornecia uma visão mais coerente e abrangente das contribuições das características, facilitando o ajuste preciso do modelo para reduzir a taxa de falsos positivos sem comprometer a eficácia na detecção de fraudes. Baesens, B., Höppner, S., & Verdonck, T. (2021).Em relação às abordagens híbridas de modelagem, foram realizados experimentos comparando o desempenho de modelos complexos, como redes neurais profundas, com modelos mais simples, como árvores de decisão, em conjuntos de dados de transações bancárias. Um dos experimentos mais significativos envolveu a implementação de um sistema de votação ensemble, onde as previsões de múltiplos modelos simples eram combinadas para gerar uma previsão final. Esse sistema foi comparado com o desempenho de um único modelo complexo, revelando que, embora o modelo complexo apresentasse uma precisão ligeiramente superior, o sistema de votação ensemble oferecia melhor interpretabilidade, facilidade de manutenção e maior resiliência a mudanças nos padrões de dados. DESAFIO TECNOLÓGICO: Um dos principais obstáculos da linha desta linha de pesquisa foi a integração harmoniosa de tecnologias de reconhecimento biométrico, OTP via SMS e tokens de hardware em um ambiente bancário agnóstico. Esse desafio exigiu uma complexa coordenação de sistemas e protocolos de segurança para assegurar a interoperabilidade e eficácia na autenticação. Além disso, a calibração dos algoritmos de IA, a fim de identificar comportamentos fraudulentos com precisão, sem gerar um número excessivo de falsos positivos, representou um desafio técnico significativo, afetando diretamente a experiência do usuário e a eficiência operacional do sistema.Levantou-se a hipótese de adotar frameworks de desenvolvimento e APIs de segurança padronizadas que facilitassem a integração de diferentes tecnologias de autenticação. No entanto, a eficácia dessa abordagem permaneceu incerta, dada a diversidade de dispositivos e sistemas operacionais utilizados pelos clientes. Paralelamente, a codificação de técnicas de aprendizado de máquina mais sofisticadas e a alimentação constante dos algoritmos de IA com novos dados de transações poderiam, em teoria, melhorar a detecção de fraudes, minimizando os falsos positivos.Um dos obstáculos mais notáveis foi encontrar o equilíbrio certo entre segurança aprimorada e usabilidade. Embora o sistema MFA tenha demonstrado eficácia na redução de acessos não autorizados e tentativas de fraude, a complexidade adicional na experiência de autenticação gerou feedback negativo dos usuários, indicando uma barreira potencial à adoção plena do sistema. Além disso, a implementação eficiente de múltiplas formas de verificação sem causar atrasos significativos no acesso dos usuários legítimos apresentou-se como um desafio, dada a necessidade de processar e validar cada forma de autenticação de maneira rápida e segura.Para a autenticação multifatorial, foi considerada a possibilidade de integrar frameworks e serviços especializados em segurança e autenticação, visando simplificar a implementação de tecnologias biométricas ao mesmo tempo em que se mantinha a conformidade com regulamentações de privacidade. A ideia de utilizar Inteligência Artificial e Machine Learning, por meio do SageMaker, para análise comportamental em tempo real e identificação de padrões de uso suspeitos foi vista como uma potencial adição de segurança.A experimentação de modelos de ML complexos e ""caixas-pretas"", como redes neurais profundas, pode oferecer um alto desempenho na detecção de fraudes, mas muitas vezes à custa da interpretabilidade. Isso representa um desafio significativo quando é necessário explicar a base de uma determinada decisão a clientes ou a reguladores, especialmente em casos de disputa. Além disso, a conformidade com regulamentações que exigem transparência nas decisões automatizadas, impõe a necessidade de modelos que possam fornecer insights claros sobre seus processos de tomada de decisão. Baesens, B., Höppner, S., & Verdonck, T. (2021).Para abordar essa complexidade, a equipe ponderou a adoção de técnicas de interpretabilidade e explicabilidade de modelos, como LIME (Local Interpretable Model-agnostic Explanations) e SHAP (SHapley Additive exPlanations). Essas técnicas visam decompor as previsões de modelos complexos em contribuições compreensíveis de cada característica, proporcionando insights sobre como diferentes atributos influenciam as decisões do modelo.Além disso, foi considerada a possibilidade de implementar abordagens híbridas, como a combinação de CNN + árvore de decisão, cruzando modelos de alta precisão, mas de difícil interpretação, com modelos mais simples e explicáveis, utilizando, por exemplo, árvores de decisão ou modelos lineares para as camadas de decisão finais. Contudo, observou-se que a integração dessas técnicas e abordagens poderia apresentar desafios próprios, incluindo a complexidade adicional no treinamento e na manutenção dos modelos. METODOLOGIA: Pesquisa sobre Frameworks de Desenvolvimento e APIs de Segurança: Foram estudados e avaliados frameworks e APIs que facilitaram a integração de diversas tecnologias de autenticação, como reconhecimento biométrico, OTP via SMS e tokens de hardware, em um ambiente bancário.Experimentação com Técnicas de Aprendizado de Máquina Avançadas: Técnicas de machine learning sofisticadas foram implementadas e testadas, alimentando constantemente os algoritmos de IA com novos dados de transações para aprimorar a detecção de fraudes e minimizar falsos positivos.Avaliação de Usabilidade versus Segurança: Estudos de usuário foram realizados para encontrar o equilíbrio certo entre segurança aprimorada e usabilidade, considerando a complexidade na experiência de autenticação e o feedback dos usuários.Desenvolvimento de Métodos de Verificação Rápida e Segura: Foram pesquisados e testados métodos para processar e validar múltiplas formas de autenticação de maneira rápida e segura, sem causar atrasos significativos no acesso dos usuários legítimos.A integração de tecnologias biométricas, mantendo a conformidade com as regulamentações de privacidade, foi explorada por meio de frameworks e serviços especializados em segurança e autenticação.Uso de Inteligência Artificial para Análise Comportamental: O uso de IA e machine learning, por meio de ferramentas como SageMaker, para análise comportamental em tempo real e identificação de padrões de uso suspeitos foi considerado como uma adição de segurança.Exploração de Técnicas de Interpretabilidade e Explicabilidade de Modelos: Técnicas como LIME e SHAP foram estudadas e aplicadas para tornar as decisões de modelos de ML complexos mais interpretáveis e explicáveis, proporcionando insights sobre como diferentes atributos influenciam as decisões do modelo.Implementação de Abordagens Híbridas para Modelagem: A combinação de modelos de alta precisão, mas de difícil interpretação, com modelos mais simples e explicáveis, como árvores de decisão ou modelos lineares, para as camadas de decisão finais foi testada.Avaliação da Complexidade no Treinamento e Manutenção dos Modelos: Os desafios relacionados à complexidade adicional no treinamento e na manutenção dos modelos decorrentes da integração de técnicas de interpretabilidade e explicabilidade foram analisados.Estudo sobre a Aceitação de Explicações por Reguladores e Usuários Finais: Foi investigado como as explicações geradas pelos modelos foram recebidas por reguladores e usuários finais, considerando as incertezas quanto à aceitação dessas explicações. INFORMAÇÃO COMPLEMENTAR: O desenvolvimento de um sistema de autenticação multifatorial avançado, aliado à inteligência artificial, trouxe benefícios tangíveis não apenas para a segurança das operações em plataformas bancárias, mas também para a experiência geral do usuário. A utilização de reconhecimento biométrico, OTP via SMS e tokens de hardware, combinada com a capacidade analítica dos algoritmos de IA, resultou em um processo de autenticação mais robusto e personalizado. Isso não só dificultou ações fraudulentas, mas também aprimorou a capacidade dos sistemas em oferecer uma experiência de uso fluida e segura, ajustando-se automaticamente às necessidades e comportamentos dos usuários. Tal desenvolvimento tecnológico representa um avanço significativo na forma como as instituições financeiras abordam a segurança e a usabilidade, promovendo um ambiente online mais seguro e confiável.Além disso, a otimização dos processos de autenticação e a eficiência na detecção de tentativas de fraude têm um impacto positivo direto na operacionalidade das instituições financeiras. A redução de custos associados a fraudes e a melhoria na eficiência operacional são claros indicadores do valor agregado por essas tecnologias. Adicionalmente, a adaptação e integração de serviços de cloud computing, como AWS Cognito e Lambda, na gestão de identidades e autenticações, e o uso de SageMaker para o desenvolvimento de modelos de machine learning, ressaltam a importância da flexibilidade e da escalabilidade dos sistemas de segurança.Além disso, nos experimentos realizados para aprimorar a segurança das transações através de autenticação multifatorial, foi explorada a eficácia da biometria comportamental combinada com a autenticação de dois fatores (2FA). Este estudo envolveu a análise de padrões comportamentais, que são únicos para cada indivíduo. A integração desses dados comportamentais com métodos tradicionais de 2FA, como SMS ou tokens de autenticação de aplicativos, resultou em um sistema de autenticação robusto que diminuiu ainda mais a possibilidade de acesso não autorizado.É importante salientar também que diversas equipes conduziram experimentos e testagens para validar a eficácia e robustez dos sistemas propostos. A equipe de segurança cibernética simulou ataques de repetição e man-in-the-middle, descobrindo que a combinação de TLS com autenticação baseada em tokens de hardware reduzia vulnerabilidades em 70%, resultando em um protocolo de comunicação personalizado com criptografia de ponta a ponta e autenticação baseada em desafio-resposta, que melhorou o tempo de resposta a ameaças em 40%.A equipe de ciência de dados desenvolveu um sistema de pontuação de risco dinâmico utilizando redes neurais convolucionais (CNNs) e máquinas de vetores de suporte (SVMs). A aplicação de PCA reduziu a dimensionalidade dos dados em 50%, aumentando a precisão da detecção de fraudes em 15%, com uma taxa de detecção de 92% e redução de falsos positivos para 5%. Técnicas de explicabilidade como SHAP e LIME foram empregadas para ajustar modelos de ML complexos, com SHAP reduzindo a taxa de falsos positivos em 10% sem comprometer a eficácia.A equipe de desenvolvimento comparou sistemas de votação ensemble com modelos complexos, constatando que o ensemble oferecia uma precisão de 88% e maior resiliência de manutenção em comparação com a precisão de 90% do modelo complexo. Adicionalmente, a equipe de autenticação explorou a biometria comportamental combinada com autenticação de dois fatores (2FA), resultando em um sistema que reduziu a possibilidade de acesso não autorizado em 25%.REFERÊNCIAS BIBLIOGRÁGICAS:Sinigaglia, F., Carbone, R., Costa, G., & Zannone, N. (2020). A survey on multi-factorauthentication for online banking in the wild. Computers & Security, 95, 101745.https://doi.org/10.1016/j.cose.2020.101745Baesens, B., Höppner, S., & Verdonck, T. (2021). Data engineering for fraud detection. DecisionSupport Systems, 142, 113492. https://doi.org/10.1016/j.dss.2021.113492 RESULTADO ECONÔMICO: A redução significativa de fraudes e acessos não autorizados diminuiu prejuízos financeiros, elevando a confiança dos usuários e potencializando a economia operacional. RESULTADO INOVAÇÃO: A integração eficaz de autenticação multifatorial e IA aprimorou a detecção de comportamentos fraudulentos, elevando padrões de segurança e usabilidade das soluções. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;"NÚMERO: 13 ID ÚNICO: 109733 NOME: EXPERIMENTAÇÃO DE JORNADA AGNÓSTICA PARA UNIFICAÇÃO DE SERVIÇOS BANCÁRIOS EM UMA PLATAFORMA ÚNICA DESCRIÇÃO: Menos de um terço dos maiores bancos do mundo estão investindo em ecossistemas financeiros digitais de uma “maneira significativa”. Uma pesquisa diz que cerca de 25% dessas organizações estão investindo em testes piloto de sistemas bancários centrais. (MIT Technology Review Insights, & Boston Consulting Group, (2023).Diante disso, a presente linha de pesquisa propõe o desenvolvimento de mecanismos que viabilizem a modernização de serviços estruturais do banco, com ênfase na integração eficaz de sistemas mainframe antigos com as tecnologias de cloud computing modernas para evolução na entrega de serviços ao cliente. Especificamente, procurou-se desenvolver um robusto mecanismo de serialização e deserialização para facilitar a comunicação entre os sistemas legados e as plataformas modernas, otimizar o armazenamento e a performance na cloud, reforçar a segurança dos dados e gerenciar eficientemente as transações e falhas entre microserviços.Foi trabalhada uma abordagem experimental para testar a viabilidade técnica de alguns componentes a serem aplicados futuramente na infraestrutura da empresa. Foram desenvolvidos e testados protótipos de algoritmos de serialização e deserialização personalizados para converter dados de EBCDIC para UTF-8, utilizando técnicas de mapeamento de caracteres e normalização de dados. Estruturas aninhadas em JSON/XML foram prototipadas para representar campos compostos e registros hierárquicos dos mainframes utilizando esquemas JSON Schema e XML Schema para validação. Esses protótipos foram validados através de testes com conjuntos de dados representativos para verificar a integridade e precisão dos dados convertidos.Explorou-se o uso de Elastic Block Store (EBS) para otimizar o armazenamento na cloud e a performance, criando protótipos que foram submetidos a testes de carga e desempenho. Além disso, tecnologias de emulação de terminais foram experimentadas para garantir a compatibilidade com aplicações COBOL, avaliando a eficácia dessas soluções em ambientes controlados.A segurança foi significativamente reforçada através de experimentações com AWS IAM e AWS KMS para o gerenciamento de identidades, acessos e chaves de criptografia. Foram realizados testes de segurança utilizando ferramentas como OpenVAS e Nessus para identificar possíveis vetores de ataque. A viabilidade da atualização para os protocolos TLS 1.2 e 1.3 foi testada através de protótipos que passaram por rigorosos testes de desempenho e segurança com as ferramentas como JMeter, Gatling e SSL Labs, garantindo que a configuração mitigava vulnerabilidades sem introduzir novos vetores de ataque.Para gerenciar transações e falhas entre microserviços, foi desenvolvido um protótipo de sistema de ""sagas"". Este protótipo foi testado em condições normais e de falha utilizando ferramentas de simulação como Chaos Monkey, assegurando que as sagas mantinham a integridade dos dados mesmo em operações complexas e distribuídas. A coordenação entre microserviços foi experimentada com orquestradores de sagas como Netflix Conductor e Axon Framework.O marco crítico deste projeto foi a necessidade de desenvolver e testar protótipos de mecanismos de serialização e deserialização, junto com a introdução de APIs agnósticas para a integração entre sistemas mainframe legados e tecnologias de cloud computing. Esses desenvolvimentos experimentais possibilitaram uma comunicação eficaz entre as diferentes arquiteturas tecnológicas, superando os desafios de incompatibilidade e limitações de agilidade impostas pelos sistemas antigos. Além disso, a adoção de Elastic Block Store (EBS) melhorou o armazenamento e a performance na cloud.Espera-se que o banco tenha uma infraestrutura de TI mais ágil, segura e capaz de responder rapidamente às demandas do mercado e às expectativas dos clientes. A integração eficiente dos sistemas legados com as novas tecnologias deve permitir a introdução de serviços digitais inovadores com uma experiência de usuário aprimorada. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: API,  agnóstica,  arquitetura,  acesso,  Restful,  integração,  distribuídas,  dados,  criptografia. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para superar o desafio de integrar sistemas que utilizavam chamadas de procedimento remoto (RPC) e formatos de dados proprietários, foi necessário realizar uma série de experimentações técnicas focadas nos mecanismos de serialização/deserialização de dados e nos protocolos de autenticação segura. Durante os testes, foi implementado um mecanismo robusto de serialização/deserialização que converteu os tipos de dados proprietários dos mainframes em formatos JSON ou XML, utilizados pelas APIs RESTful. Os experimentos demonstraram que a conversão de dados poderia ser realizada com uma taxa de sucesso de 98%, garantindo a integridade e a precisão dos dados.No re-hosting, a compreensão sobre a arquitetura de armazenamento da cloud e as capacidades de I/O levou à experimentação de soluções de armazenamento de alto desempenho. Configurou-se o ambiente para maximizar a taxa de transferência e a IOPS (Input/Output Operations Per Second). Os testes de desempenho indicaram que a configuração otimizada resultou em uma melhoria de 40% na taxa de transferência e um aumento de 35% na IOPS comparado com as soluções de armazenamento tradicionais. Exploraram-se tecnologias de emulação de terminais avançadas, integrando soluções que ofereciam latência reduzida e maior compatibilidade com as aplicações COBOL existentes. Os resultados mostraram que a latência foi reduzida em 25%, garantindo uma experiência de usuário final ininterrupta e eficiente.A segurança foi reforçada através do desenvolvimento de um sistema de gerenciamento de identidade e acesso, juntamente com uma solução personalizada para gestão de chaves de criptografia. Estudos realizados apontaram que a ativação do TLS não era suficiente. Foi necessário entender as versões do protocolo TLS, as cifras suportadas e a importância da ordem de preferência dessas cifras. Constatou-se que versões anteriores do TLS (1.0 e 1.1) eram vulneráveis a ataques, como o POODLE, e que cifras mais robustas, como ECDHE (Elliptic Curve Diffie-Hellman Ephemeral) para o intercâmbio de chaves, ofereciam mais segurança e melhor desempenho. Com essas compreensões, configurou-se o TLS 1.2 (e posteriormente o 1.3) como padrão para todas as comunicações entre microserviços, especificando uma lista de cifras que excluía algoritmos conhecidos por suas vulnerabilidades. Os testes de segurança indicaram que a implementação do TLS 1.2 e 1.3 reduziu a vulnerabilidade a ataques em 95%.Para cada operação de negócio que exigia várias transações entre microserviços, delineou-se uma série de passos (sagas), identificando claramente as operações de compensação para cada passo em caso de falhas. Isso exigiu uma análise profunda dos fluxos de negócios e uma modelagem cuidadosa para garantir que todas as situações de falha fossem adequadamente tratadas, mantendo a integridade dos dados. Os experimentos de simulação de falhas indicaram que a implementação de sagas reduziu a perda de dados em 98% e melhorou a resiliência do sistema em 45%.Compreendeu-se que a criptografia AES-256, por si só, não garantia segurança completa sem uma gestão de chaves eficaz. Desenvolveu-se um serviço personalizado para gerenciamento de chaves que oferecesse não apenas armazenamento seguro, mas também políticas de rotação de chaves e controles de acesso granulares para as chaves de criptografia. Além de adotar o AES-256 para criptografia de dados em repouso e em trânsito, configurou-se a rotação automática de chaves, garantindo que as chaves antigas fossem substituídas regularmente sem interromper os serviços. Isso foi complementado por políticas de acesso restritas às chaves, onde apenas serviços e indivíduos autorizados poderiam requisitar o uso de chaves para criptografia e descriptografia. Os testes de segurança e eficiência mostraram que a rotação automática de chaves reduziu o risco de comprometimento das chaves em 90% e assegurou a continuidade dos serviços sem interrupções. DESAFIO TECNOLÓGICO: Configurou-se um grande desafio prover a integração de sistemas com formatos de dados proprietários dos mainframes. A principal complexidade técnica estava na incompatibilidade de tipos de dados. Mainframes frequentemente utilizavam formatos binários compactos, como EBCDIC, enquanto JSON e XML utilizam UTF-8. Além disso, os dados dos mainframes incluíam campos compostos e registros hierárquicos, sem correspondentes diretos em JSON ou XML, exigindo estruturas aninhadas e algoritmos de serialização/deserialização personalizados. A preservação da integridade dos dados era crucial, pois qualquer erro poderia comprometer a precisão dos dados financeiros e de transações críticas. Propôs-se um mecanismo robusto de serialização/deserialização, incluindo mapeamentos personalizados, validações rigorosas e técnicas de otimização de código. No entanto, havia incerteza sobre se essa abordagem poderia lidar com todas as variações dos dados dos mainframes e se os testes confirmariam uma taxa de sucesso suficientemente alta na conversão.Outro desafio significativo foi garantir a segurança nas comunicações entre microserviços, especialmente devido às vulnerabilidades em versões antigas do protocolo TLS. As versões anteriores do TLS, como 1.0 e 1.1, eram suscetíveis a ataques POODLE e BEAST, que exploravam fraquezas na implementação do protocolo para interceptar e decifrar dados sensíveis. A complexidade técnica aumentou devido à necessidade de garantir compatibilidade com cifras seguras sem comprometer o desempenho. Além disso, a configuração inadequada de cifras poderia resultar em vetores de ataque adicionais, como ataques de downgrade, onde um atacante força a utilização de uma versão ou cifra mais fraca do protocolo. Considerou-se a configuração do ambiente para utilizar versões mais seguras do TLS (1.2 e 1.3) e a seleção de cifras robustas, priorizando aquelas que ofereciam um equilíbrio entre segurança e desempenho. A implementação exigiria configuração detalhada de servidores e clientes para garantir que apenas cifras seguras fossem utilizadas, além de definir uma política de preferência de cifras. Contudo, havia incerteza sobre se essa abordagem poderia realmente mitigar os riscos sem introduzir novos problemas de desempenho ou compatibilidade.Manter a integridade dos dados em transações complexas entre microserviços foi outro desafio crítico. A principal dificuldade técnica envolveu a gestão de transações que exigiam múltiplas operações em diferentes microserviços, onde falhas parciais poderiam deixar o sistema em um estado inconsistente. Se uma dessas operações falhasse, era necessário garantir que todas as operações anteriores fossem revertidas para manter a integridade dos dados. Uma das soluções propostas foi a implementação de operações de compensação para cada passo da transação, mas isso se mostrou complexo, pois exigia a definição de lógicas de reversão específicas para cada tipo de operação. A abordagem baseada em sagas foi considerada, onde cada transação seria dividida em uma série de passos com operações de compensação definidas. Entretanto, permanecia a dúvida sobre se essa abordagem poderia realmente garantir a integridade dos dados em todas as possíveis condições de falha.A gestão eficaz de chaves de criptografia representou outro desafio técnico complexo. A criptografia AES-256, embora robusta, exigia uma gestão eficaz de chaves para garantir a segurança completa dos dados. A principal dificuldade envolveu a implementação de um sistema de gerenciamento de chaves que pudesse realizar a rotação automática de chaves sem interromper os serviços em execução. Delineou-se a hipótese de criar um serviço personalizado para gerenciamento de chaves que incluísse técnicas avançadas de criptografia e hardware seguro para armazenar as chaves e controles de acesso granulares para garantir que apenas entidades autorizadas pudessem utilizar as chaves. METODOLOGIA: A metodologia de desenvolvimento seguiu uma sequência estruturada para abordar as hipóteses e superar os desafios tecnológicos. Para a integração de sistemas com formatos de dados proprietários dos mainframes, iniciou-se com a análise dos formatos de dados, identificando e documentando todos os formatos proprietários utilizados, incluindo EBCDIC, formatos binários compactos e estruturas de registros hierárquicos. Foram analisadas as diferenças entre EBCDIC e UTF-8, bem como os impactos na conversão de dados numéricos e de datas. Desenvolveram-se algoritmos de serialização e deserialização personalizados para converter dados de EBCDIC para UTF-8, utilizando técnicas de mapeamento de caracteres e normalização de dados. Estruturas aninhadas em JSON/XML foram projetadas para representar campos compostos e registros hierárquicos dos mainframes, utilizando esquemas JSON Schema e XML Schema para validação. Testes de validação foram realizados utilizando conjuntos de dados de teste representativos, verificando a integridade e precisão dos dados convertidos. Testes automatizados com frameworks como JUnit e TestNG garantiram a robustez dos algoritmos de conversão.Na segurança das comunicações entre microserviços, a análise de vulnerabilidades das versões antigas do TLS foi o primeiro passo, seguida pela configuração do ambiente para utilizar TLS 1.2 e 1.3 com cifras robustas como ECDHE, AES-GCM e ChaCha20-Poly1305. Ferramentas de análise de segurança como OpenVAS e Nessus foram utilizadas para identificar possíveis vetores de ataque. Configurou-se o ambiente para utilizar TLS 1.2 e 1.3, priorizando cifras robustas. Realizaram-se testes de desempenho utilizando ferramentas como Apache JMeter e Gatling para avaliar o impacto das novas cifras. Testes de segurança com SSL Labs e Wireshark garantiram que a configuração mitigava vulnerabilidades sem introduzir novos vetores de ataque. A compatibilidade da nova configuração de TLS com todos os microserviços e sistemas legados foi testada em ambientes isolados, ajustando configurações conforme necessário.Para manter a integridade dos dados em transações complexas entre microserviços, analisaram-se as transações, identificando dependências e pontos de falha potenciais. Utilizaram-se diagramas de sequência e fluxogramas para visualizar o fluxo de transações. Desenvolveram-se operações de compensação específicas para cada tipo de operação, utilizando padrões de design como Command e Memento. Implementaram-se operações de compensação com frameworks de transações distribuídas como Apache Camel e Spring Boot. Transações complexas foram divididas em passos menores com operações de compensação definidas, utilizando o padrão de saga. A coordenação entre microserviços foi implementada com orquestradores de sagas como Netflix Conductor e Axon Framework. Testes em condições normais e de falha garantiram que as sagas mantinham a integridade dos dados, utilizando ferramentas de simulação de falhas como Chaos Monkey.Na gestão eficaz de chaves de criptografia, analisaram-se os requisitos de criptografia e desenvolveu-se um serviço personalizado para gerenciamento de chaves, incluindo técnicas avançadas de criptografia e hardware seguro. Identificaram-se e documentaram-se todos os requisitos de criptografia, incluindo tipos de dados a serem protegidos e políticas de acesso. Um serviço personalizado para gerenciamento de chaves foi experimentado, utilizando técnicas avançadas de criptografia. Políticas de rotação automática de chaves foram definidas utilizando frameworks como HashiCorp Vault e AWS KMS, garantindo a substituição regular das chaves sem interrupções. Implementaram-se controles de acesso granulares utilizando RBAC e ABAC para garantir que apenas serviços e indivíduos autorizados possam utilizar as chaves de criptografia. O acesso às chaves foi monitorado e auditado utilizando ferramentas de SIEM como Splunk e ELK Stack. INFORMAÇÃO COMPLEMENTAR: A execução de estudos, pesquisas e experimentos aqui apresentados trouxeram uma série de benefícios significativos para o time de pesquisadores e, por extensão, para o projeto de modernização dos serviços bancários como um todo. Estes benefícios abrangeram diversas áreas, desde a melhoria da eficiência operacional até a garantia de segurança e compliance. A seguir são apresentados alguns benefícios adquiridos:Ao estudar e testar a interoperabilidade entre sistemas mainframe e cloud, a equipe conseguiu desenvolver soluções que permitiram uma integração eficaz entre essas plataformas. Como consequência, foi possível perceber uma maior a flexibilidade operacional, permitindo que sistemas legados e modernos trabalhassem conjuntamente.Otimização de Processos de Modernização: A análise detalhada das estratégias de modernização de mainframe permitiu uma transição mais suave e eficiente para arquiteturas modernas, reduzindo riscos e custos associados à migração, além de minimizar interrupções operacionais.REFERÊNCIAS BIBLIOGRÁFICAS:MIT Technology Review Insights, & Boston Consulting Group. (2023, October 23).Seeking a successful path to core modernization. MIT Technology Review.https://www.technologyreview.com/2023/10/23/1082061/seeking-a-successful-path-to-core-modernization/ RESULTADO ECONÔMICO: Retenção e captação de novos clientes,  apresentação de estruturas modernizadas,  ganhos de produtividade e elevação de competitividade dada a modernização de serviços. RESULTADO INOVAÇÃO: Estruturação de novas arquiteturas que trouxeram jornadas modernizadas, ganhos de escalabilidade,  obtenção de novos conhecimentos para estruturação de aplicações em ambiente cloud. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 14 ID ÚNICO: 109734 NOME: SOLUÇÕES PARA ESTRUTURAÇÃO DE COMPONENTES DE SEGURANÇA EM LOGIN UNIFICADO DESCRIÇÃO: A presente linha de pesquisa propõe o desenvolvimento de um sistema de login unificado, empregando a tecnologia OpenID Connect e soluções da AWS. Este sistema tem como meta fornecer um processo de autenticação seguro e eficiente para os clientes, mitigando riscos associados a ameaças cibernéticas.Para atingir esse objetivo, foram estudados e experimentados componentes para prover a integração de sistemas legados com novas arquiteturas, incluindo eventos e microsserviços, e a implementação de métodos de autenticação avançados. Destacam-se a autenticação multifatorial e a detecção de anomalias com base em inteligência artificial, visando a escalabilidade, alta disponibilidade e conformidade com normas de segurança de dados.Espera-se que, com a implementação deste sistema de login unificado, a empresa aprimore a segurança e a eficiência do processo de autenticação. A experimentação de OpenID Connect, soluções da AWS, autenticação multifatorial e detecção de anomalias por IA deve criar uma plataforma segura e adaptável às necessidades dos clientes, contribuindo para a proteção contra ameaças cibernéticas e melhorando a experiência do usuário.O marco crítico desta linha de pesquisa foi a implementação bem-sucedida de um sistema de login unificado que integra OpenID Connect e soluções específicas da AWS para oferecer autenticação segura e eficiente. Esse sistema incorporou autenticação multifatorial e utilizou inteligência artificial para a detecção de anomalias, abordando diretamente as vulnerabilidades associadas a ameaças cibernéticas.A integração efetiva de sistemas legados com arquiteturas modernas foi realizada por meio de eventos e microsserviços, assegurando a escalabilidade e alta disponibilidade do sistema. Esse processo atendeu estritamente às normas de segurança de dados vigentes. O desenvolvimento desse sistema de login unificado visa fortalecer a segurança contra ameaças cibernéticas e melhorar a experiência do usuário ao simplificar o processo de autenticação. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: OpenID,  unificado,  integrar,  login,  segurança,  arquitetura,  autenticação,  sistema,  criptografia NATUREZA: Serviço ELEMENTO TECNOLÓGICO: A hipótese central desta linha de pesquisa foi a de que a criação de um sistema de login unificado, utilizando OpenID Connect em uma infraestrutura de cloud pública, poderia aprimorar significativamente o processo de autenticação para usuários de variados canais bancários. Este estudo visou comprovar a viabilidade técnica das soluções propostas, superando as deficiências inerentes aos sistemas de autenticação fragmentados. Para isso, foram conduzidos experimentos rigorosos e detalhados, com foco em diversas inovações tecnológicas.Foram realizados experimentos para desenvolver algoritmos de criptografia customizados, com o objetivo de proporcionar uma segurança superior em comparação às soluções tradicionais. A pesquisa inicial envolveu uma análise detalhada de algoritmos de criptografia existentes, como AES, RSA e criptografia homomórfica, identificando suas limitações e vulnerabilidades em cenários de autenticação. Utilizando Python e bibliotecas criptográficas, foi criado um novo algoritmo que combinava criptografia simétrica e assimétrica para garantir a segurança dos tokens JWT, incorporando técnicas de criptografia homomórfica para permitir a análise segura dos dados. A eficácia do novo algoritmo foi avaliada por meio de testes de penetração e análise de vulnerabilidades, que demonstraram resistência a ataques de força bruta, ataques de replay e outras ameaças comuns. Os resultados dos testes indicaram uma redução de 30% no tempo de criptografia e descriptografia, além de um aumento de 50% na resistência a ataques.Um método em Python foi desenvolvido e implementado como uma função Lambda, permitindo a rotação automática de chaves e a aplicação de políticas de acesso dinâmicas. A avaliação do desempenho incluiu a medição da latência de requisição e a rotatividade de chaves, resultando em uma latência média de 200 ms e uma rotação de chaves sem interrupção do serviço.A integração da biblioteca scikit-learn permitiu o desenvolvimento de um modelo de aprendizado profundo para analisar os dados comportamentais em tempo real. Este modelo foi treinado utilizando um conjunto de dados históricos de autenticação, identificando padrões normais e anômalos. Testes de desempenho garantiram que as funções Lambda pudessem escalar automaticamente e lidar com um grande volume de requisições, proporcionando uma validação eficiente das credenciais e uma análise comportamental precisa. A precisão do modelo foi avaliada em 95%, com uma capacidade de processamento de até 1.000 requisições por segundo.Para complementar, foi desenvolvido um sistema de logging que incorporava análise comportamental baseada em inteligência artificial. O sistema de logging coletava e armazenava logs detalhados das tentativas de autenticação. Técnicas de processamento de linguagem natural (NLP) foram implementadas para permitir a análise dos logs e a identificação de padrões suspeitos. Testes de análise, utilizando um conjunto de logs históricos e simulados, avaliaram a precisão do sistema na identificação de padrões suspeitos, resultando em uma detecção precisa de atividades suspeitas. O sistema de logging reduziu a taxa de falsos positivos para 2% e aumentou a detecção de atividades anômalas em 40%.Experimentos foram executados para comprovar a viabilidade da criação de um Gateway, que serviria para rotear todas as requisições de autenticação para as funções Lambda apropriadas, configurando políticas de acesso detalhadas para garantir a segurança das requisições. Um sistema de controle de acesso dinâmico foi desenvolvido para ajustar as políticas IAM em tempo real com base nas análises comportamentais e na avaliação contínua de risco. Testes de segurança garantiram que o Gateway e as políticas de acesso fossem configurados corretamente, proporcionando uma segurança aprimorada e uma experiência de usuário melhorada. O Gateway reduziu as tentativas de acesso não autorizado em 30% e melhorou o tempo de resposta em 20%. DESAFIO TECNOLÓGICO: Ao desenvolver algoritmos de criptografia customizados, o desafio técnico foi garantir que essas operações fossem executadas com latência mínima, mesmo em cenários de alta carga. A complexidade incluiu o gerenciamento de chaves, onde se buscou garantir a segurança das chaves enquanto se minimizava o tempo de acesso. A paralelização também foi um ponto crítico, exigindo a divisão das operações de criptografia para serem processadas em paralelo sem introduzir vulnerabilidades. Além disso, a integração com hardware, utilizando aceleração por hardware como AES-NI, foi considerada para melhorar a performance. A hipótese de solução envolveu a implementação de criptografia simétrica utilizando AES-GCM com aceleração por hardware através de AES-NI. Paralelamente, desenvolveu-se um módulo em Rust para operações críticas de criptografia, utilizando bindings com Python através de FFI (Foreign Function Interface). A ideia foi reduzir a latência ao máximo possível. No entanto, a integração segura entre Rust e Python, garantindo que não houvesse vazamento de memória ou vulnerabilidades de segurança, foi um desafio significativo e requereria testes rigorosos.A rotação automática de chaves e a aplicação de políticas dinâmicas no AWS KMS apresentaram problemas específicos, como a consistência das chaves, onde se buscou garantir que todas as instâncias de serviço utilizassem a chave correta após a rotação. A sincronização de políticas também foi um desafio, exigindo a aplicação de políticas de acesso dinâmicas em tempo real sem causar inconsistências. A latência de atualização foi outro ponto crítico, buscando-se minimizar o tempo de propagação das atualizações de chaves e políticas. A hipótese de solução envolveu o desenvolvimento de um mecanismo de cache distribuído utilizando Redis para armazenar temporariamente as chaves criptográficas e políticas de acesso. Utilizou-se AWS Step Functions para orquestrar a rotação de chaves e a aplicação de políticas, garantindo que a rotação ocorresse de maneira sequencial e controlada.Implementar um sistema de logging que utilizasse técnicas de NLP para análise comportamental em tempo real envolveu desafios específicos, como o volume de dados, onde se buscou processar grandes volumes de logs rapidamente. O treinamento contínuo foi outro ponto crítico, buscando-se re-treinar modelos de NLP continuamente sem interromper o serviço. A precisão do modelo também foi um desafio, onde se buscou garantir que o modelo de NLP mantivesse alta precisão ao longo do tempo. Para o treinamento contínuo, utilizou-se um sistema de aprendizado incremental que atualizasse o modelo de NLP com novos dados periodicamente, sem a necessidade de re-treinar o modelo inteiro. A complexidade de gerenciar a infraestrutura de Kafka e garantir a escalabilidade e a baixa latência do processamento de logs foi significativa e requereria experimentação contínua.A integração de análises comportamentais e algoritmos de aprendizado de máquina foi explorada como uma tentativa para melhorar a detecção de ameaças. A coleta de dados de acesso e a subsequente análise por modelos de aprendizado de máquina, incluindo redes neurais convolucionais (CNNs) e algoritmos de detecção de anomalias, foram postos como possível soluções, que visavam identificar padrões de comportamento suspeitos. A seleção desses modelos foi baseada na sua capacidade de processar grandes volumes de dados e aprender com eles. No entanto, o sucesso da aplicação desses modelos levantava algumas incertezas, dada a dependência de conjuntos de dados de treinamento extensos e representativos. Além disso, a capacidade dos atacantes de adaptar suas estratégias para contornar os padrões aprendidos pelos modelos introduziu uma variável, questionando a capacidade de resposta e adaptação contínua dos algoritmos em um ambiente de ameaças com mudanças constantes. METODOLOGIA: Para superar os desafios, delineou-se uma metodologia de desenvolvimento focada em atividades experimentais. Essa metodologia envolveu uma série de ações e experimentos específicos para tratar cada barreira identificada, com testes rigorosos para confirmação dos resultados.Para os algoritmos de criptografia customizados, as atividades experimentais incluíram a análise de performance, onde se implementaram protótipos de algoritmos de criptografia AES-GCM com e sem aceleração por hardware (AES-NI). Mediu-se a latência e o throughput em diferentes cenários de carga. Desenvolveu-se um módulo em Rust para operações críticas de criptografia e criaram-se bindings com Python utilizando FFI. Realizaram-se testes de integração para verificar a segurança e desempenho, além de auditorias de segurança para identificar possíveis vazamentos de memória e vulnerabilidades Os experimentos e testes envolveram benchmarking para comparar o desempenho entre implementações com e sem aceleração por hardware, testes de carga para avaliar a estabilidade e eficiência do módulo de criptografia, e testes de integração para monitorar o uso de memória e identificar possíveis vulnerabilidades.Para a rotação automática de chaves e a aplicação de políticas dinâmicas no AWS KMS, implementou-se um cache distribuído utilizando Redis para armazenamento temporário de chaves e políticas. Criaram-se fluxos de trabalho com Step Functions para a rotação de chaves e aplicação de políticas dinâmicas, e configurou-se AWS SNS para notificação em tempo real sobre atualizações de chaves. Os experimentos e testes incluíram a verificação da consistência das chaves e políticas em diferentes instâncias de serviço após a rotação, medição do tempo de propagação das atualizações de chaves e políticas, e avaliação da sincronização de políticas de acesso em tempo real, garantindo que não ocorressem inconsistências.No desenvolvimento de funções Lambda escaláveis, implementou-se um SQS para gerenciar requisições de autenticação, combinada à utilização de base de dados NoSQL para armazenamento de estado entre execuções de funções Lambda, e implementou-se um cache local para armazenar temporariamente as credenciais validadas. Os experimentos e testes incluíram a simulação de cenários com grande volume de requisições para avaliar a capacidade de escalabilidade das funções Lambda, verificação do tempo de execução das funções Lambda em diferentes cargas de trabalho, e avaliação da consistência do estado armazenado e do cache local, garantindo a integridade dos dados.Para o sistema de logging com análise comportamental em tempo real, implementou-se Apache Kafka para ingestão e processamento em tempo real dos logs, utilizando spaCy para análise de linguagem natural com um pipeline de pré-processamento otimizado. Desenvolveu-se um sistema de aprendizado incremental para atualização contínua do modelo de NLP. Os experimentos e testes incluíram o processamento de grandes volumes de logs para avaliar a capacidade de Kafka e do pipeline de NLP, validação da precisão do modelo de NLP com dados reais e atualização incremental, e medição da latência do sistema de logging e análise comportamental em tempo real.Configurou-se API Gateway para rotear requisições de autenticação e implementou-se um sistema de controle de acesso dinâmico utilizando AWS IAM e Lambda. As políticas IAM foram ajustadas em tempo real com base em análises comportamentais e avaliação contínua de risco, utilizando Step Functions para orquestrar mudanças. Utilizou-se AWS CloudWatch para monitoramento contínuo e ajuste dinâmico de políticas. Os experimentos e testes incluíram a verificação da propagação instantânea e segura das políticas de acesso, avaliação da latência e eficácia do sistema de avaliação de risco em tempo real, e auditoria de segurança para garantir que as políticas dinâmicas não introduzissem vulnerabilidades. INFORMAÇÃO COMPLEMENTAR: A adoção de um sistema de login unificado utilizando OpenID Connect na AWS representa um avanço significativo na forma como os usuários finais acessam serviços bancários através de diferentes canais. Ao centralizar o processo de autenticação por meio do Amazon Cognito User Pool, configurado para autenticar contra o diretório LDAP do banco, o projeto não só simplificou o acesso para os usuários, eliminando a necessidade de múltiplas credenciais, como também elevou os padrões de segurança e desempenho em comparação com sistemas de autenticação isolados. A utilização de um Amazon Cognito Custom Authentication Flow e triggers de Lambda para a interação com o diretório LDAP foi fundamental para alcançar esse objetivo, permitindo uma integração segura e eficaz que facilita a autenticação dos usuários e a emissão de tokens JWT.A segurança do sistema foi ainda mais fortalecida pelo uso do AWS KMS para criptografar os tokens gerados, juntamente com o desenvolvimento de funções Lambda específicas para validar as credenciais dos usuários e gerar tokens de acesso. Essas medidas, combinadas com a implementação de um API Gateway para gerenciar as solicitações de autenticação e adicionar controles de acesso com políticas IAM, contribuíram para uma arquitetura robusta, capaz de lidar com picos de acesso sem comprometer a performance. A escalabilidade automática proporcionada pelo AWS Lambda e Amazon Cognito, juntamente com a segurança garantida pela criptografia e pelos testes de penetração bem-sucedidos, demonstram o sucesso do projeto em oferecer uma solução eficiente e segura para o acesso a serviços bancários online. RESULTADO ECONÔMICO: Retenção de clientes, maior nível de segurança em transações financeiras e no acesso a contas PF e PJ. RESULTADO INOVAÇÃO: Novas compreensões quanto ao desenvolvimento e aplicação de mecanismos avançados de autenticação multifatorial, de biometria etc., elevação dos níveis de segurança, unificação de jornadas de login e autenticação. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 15 ID ÚNICO: 109735 NOME: DESENVOLVIMENTO DE NOVOS COMPONENTES DE SEGURANÇA E DETECÇÃO DE FRAUDES PARA SOLUÇÕES DE CARTÕES E ATENDIMENTO DESCRIÇÃO: A presente linha de pesquisa busca elevar a capacidade de processamento de dados em tempo real para treinamento de modelos de detecção de fraude. Buscava-se, com isso, aprimorar os mecanismos de criptografia e a eficiência na gestão de tokens, essenciais para proteger dados sensíveis dos clientes. Foi objeto de estudo também integrar esses avanços em sistemas legados, garantindo uma transição suave e segura entre as tecnologias.Em um cenário anterior, o processo de treinamento de modelos e a integração de sistemas enfrentavam gargalos relacionados à eficiência e segurança. A capacidade de processar grandes volumes de dados em tempo real era limitada pela arquitetura da rede neural e pela implementação do código. Além disso, a integração de modelos de aprendizado de máquina em sistemas legados era dificultada por problemas de serialização e desserialização, especialmente ao lidar com diferentes versões de bibliotecas entre os ambientes de desenvolvimento e produção.Espera-se que as atividades de inovação tecnológica desta linha de pesquisa resultem em uma transformação significativa na capacidade de detecção de fraude e na segurança de dados. A otimização do tamanho do lote e da taxa de aprendizagem, juntamente com a implementação de mecanismos de criptografia ponta a ponta e uma gestão de tokens mais eficaz, devem melhorar a precisão dos modelos e acelerar o processo de treinamento. Além disso, a adaptação de modelos de aprendizado de máquina para integração com sistemas legados, considerando a compatibilidade de bibliotecas, visa minimizar os riscos e as ineficiências operacionais.O marco crítico neste projeto foi a conclusão bem-sucedida da primeira integração funcional de mecanismos de criptografia avançada e gestão de tokens com o processamento de dados em tempo real para treinamento de modelos de Machine Learning (ML) destinados à detecção de fraude. Esse ponto de verificação marcou um avanço significativo na infraestrutura do projeto, evidenciando a superação de desafios técnicos e operacionais anteriores, além de estabelecer uma base sólida para futuras iterações e refinamentos. Mahmoud, A., Salem, A., & Elsamahy, E. (2021)Este marco não é uma entrega por si só, mas um momento crítico que demonstrou a viabilidade de integrar novas soluções de segurança e processamento em um ambiente que anteriormente era limitado por arquiteturas de rede neural ineficientes e desafios na integração com sistemas legados. Ele representou o fim de uma importante fase de desenvolvimento e testes, marcando a transição para a etapa de otimização e ajustes finos. Isso envolveu pontos de decisão cruciais, como a escolha das tecnologias de criptografia e gestão de tokens adequadas, além da definição de estratégias para a integração eficaz dessas soluções com os sistemas de processamento de dados em tempo real. Também incluiu a conclusão das tarefas de adaptação dos modelos de ML para garantir sua compatibilidade e desempenho em um ambiente de sistemas legados, destacando-se como um evento significativo no cronograma do projeto. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: TensorFlow,  PyTorch,  treinamento,  modelos,  detecção,  fraude,  dados,  tempo,  arquitetura,  código,  tamanho,  lote,  taxa,  aprendizagem,  precisão,  integração,  aprendizado,  máquina,  sistemas, serialização,  bibliotecas,  desenvolvimento. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para integrar modelos de machine learning com sistemas legados, foram desenvolvidas e testadas APIs RESTful e middlewares. A comunicação eficiente foi alcançada com latência média de 100 ms por requisição, mantendo a compatibilidade com sistemas existentes. Protocolos de segurança, como TLS 1.3 e autenticação OAuth 2.0, foram implementando visando manter a integridade dos dados durante a transferência e processamento, protegendo informações sensíveis contra acessos não autorizados.Os modelos treinados foram submetidos a testes rigorosos em ambientes de produção real, medindo a precisão e recall na detecção de fraudes. Observou-se uma precisão de 98% e um recall de 95%, resultando em uma significativa redução de falsos positivos e negativos. A criação de pipelines de monitoramento e atualização contínua permitiu a adaptação dos modelos a novos padrões de fraude em tempo real, sem necessidade de interrupções, mantendo uma taxa de atualização de 5 minutos.Assim, foram realizados testes sistemáticos e controlados para identificar os hiperparâmetros ideais. Inicialmente, utilizou-se uma abordagem de busca em grade (grid search) para explorar uma ampla gama de valores possíveis para os principais hiperparâmetros, como tamanho do lote, taxa de aprendizagem, número de camadas ocultas e número de neurônios por camada. Em seguida, uma busca aleatória (random search) foi aplicada para refinar ainda mais os hiperparâmetros dentro dos intervalos mais promissores identificados na busca em grade.Durante esses testes, métricas de desempenho foram monitoradas, incluindo a acurácia, precisão, recall, F1-score e a área sob a curva ROC (AUC-ROC). Além disso, o tempo de treinamento e a utilização de recursos computacionais foram registrados para garantir que o modelo tivesse um bom desempenho, além de ser eficiente.Para cada combinação de hiperparâmetros, o modelo foi avaliado usando validação cruzada k-fold, o que permitiu uma avaliação robusta da performance do modelo em diferentes subconjuntos dos dados. Isso ajudou a identificar possíveis problemas de overfitting ou underfitting. Adicionalmente, técnicas de regularização dropout e L2 regularization foram testadas para melhorar a generalização do modelo.Por meio desses experimentos, constatou-se que um tamanho de lote de 64 e uma taxa de aprendizagem de 0,002 proporcionavam um bom equilíbrio entre a precisão do modelo e o tempo de treinamento. Modelos com mais de três camadas ocultas não mostraram melhorias significativas em termos de métricas de desempenho, indicando que uma arquitetura mais simples era suficiente para a tarefa específica de detecção de fraude. A regularização com uma taxa de dropout de 0,5 foi eficaz em reduzir o overfitting sem comprometer a precisão do modelo.Outro ponto é que a integração de modelos de aprendizado de máquina atualizados dinamicamente em sistemas legados trouxe também desafios na serialização e desserialização de modelos, mais especificamente ao lidar com diferentes versões de bibliotecas entre o ambiente de desenvolvimento e produção.Gestão de Tokens: a codificação de tokenização para proteger dados sensíveis dos clientes trouxe compreensões importantes sobre a gestão de tokens. Armazenar, renovar e invalidar tokens mostrou-se uma tarefa complexa, sendo necessário estruturar um serviço dedicado para gerenciá-los eficientemente. Isso incluiu a necessidade de um mecanismo seguro para mapear tokens a dados sensíveis sem expor esses dados em qualquer ponto do processo.A estruturação de dados com a aplicação de bancos de dados in-memory, levaram a um tratamento eficiente para acesso rápido a dados críticos. No entanto, a volatilidade dos dados armazenados em memória levou ao time a necessidade de estabelecer uma estratégia robusta de persistência e recuperação para garantir que os dados não se perdessem em caso de falhas. DESAFIO TECNOLÓGICO: Um desafio crítico presente foi a dificuldade em ajustar hiperparâmetros de forma eficiente devido à alta dimensionalidade do espaço de busca e ao tempo computacional intensivo necessário para realizar buscas em grade e aleatórias. A interação complexa entre diferentes hiperparâmetros, como o tamanho do lote e a taxa de aprendizagem, frequentemente resultava em comportamentos não lineares e imprevisíveis no desempenho do modelo. Por exemplo, durante os experimentos, foi observado que um pequeno ajuste na taxa de aprendizagem poderia levar a uma convergência muito mais rápida ou, inversamente, a um comportamento oscilatório e instável do modelo. Similarmente, variações no tamanho do lote afetavam não apenas a estabilidade do treinamento, mas também a capacidade do modelo de generalizar para novos dados, com tamanhos de lote menores aumentando a variabilidade das atualizações dos gradientes e tamanhos maiores resultando em convergência mais lenta.Para mitigar esses desafios, uma hipótese de solução poderia envolver o uso de técnicas de otimização de hiperparâmetros mais avançadas, como algoritmos de otimização bayesiana, que podem explorar o espaço de busca de maneira mais eficiente e adaptativa. Nos estudos, observou-se que esses algoritmos utilizam modelos probabilísticos para prever o desempenho do modelo com diferentes combinações de hiperparâmetros e, assim, focam a busca nas regiões mais promissoras do espaço de hiperparâmetros. Além disso, a implementação de pipelines de dados mais robustos e a utilização de técnicas de pré-processamento de dados automatizadas poderiam ajudar a lidar com a variabilidade e a qualidade dos dados. Foram exploradas técnicas de balanceamento de dados, como oversampling e undersampling, e métodos de limpeza de dados, que poderiam ser automatizados para garantir que os dados de entrada estejam em um estado ideal para o treinamento do modelo.No entanto, essas hipóteses trouxeram suas próprias complexidades e incertezas. A configuração precisa dos algoritmos de otimização bayesiana era um desafio em si, pois esses algoritmos também possuem seus próprios hiperparâmetros que precisariam ser ajustados. Além disso, a introdução de novos pontos de falha nos pipelines de dados, como dependências adicionais em ferramentas de pré-processamento, aumentou a complexidade e a fragilidade do sistema.A detecção e prevenção de fraudes, mantendo a conformidade com regulamentações estritas como o PCI DSS (Padrão de Segurança de Dados da Indústria de Cartões de Pagamento). A equipe considerou a possibilidade de utilizar aprendizado de máquina para identificar padrões de fraude, mas a integração dessa tecnologia com sistemas legados e a garantia de que os modelos pudessem ser atualizados em tempo real sem afetar a performance do sistema eram questões sem respostas claras. A segurança dos dados dos clientes era uma preocupação constante, e embora a tokenização e a criptografia ponta a ponta fossem hipóteses viáveis, a execução dessas medidas em um ambiente heterogêneo, multiplataforma trazia pontos de incerteza quanto sua viabilidade, o que demandava mais estudos para sua confirmação de viabilidade.O desafio da detecção de fraudes apresentou-se com a necessidade de o sistema aprender e adaptar-se a novos padrões de fraude em tempo real. A hipótese de desenvolver modelos de machine learning, utilizando TensorFlow ou PyTorch, foi considerada. Esses modelos seriam treinados com grandes volumes de dados transacionais e hospedados em ambientes de computação em nuvem. A ideia era que esses modelos pudessem ser atualizados em tempo real, mas a viabilidade de implementação e a eficácia em um cenário de produção permaneceram como questões abertas. METODOLOGIA: Desafios significativos foram enfrentados ao desenvolver um modelo de detecção de fraude. A metodologia adotada envolveu três etapas principais: preparação dos dados, experimentação com hiperparâmetros e avaliação de desempenho.Preparação dos Dados: Dados transacionais do ambiente bancário foram coletados e limpos, aplicando-se técnicas de normalização e codificação de variáveis categóricas. Utilizou-se o SMOTE para lidar com o desbalanceamento de classes, garantindo que os dados estivessem prontos para o treinamento do modelo.Experimentação com Hiperparâmetros: Foram realizados experimentos utilizando busca em grade (grid search) e busca aleatória (random search) para otimizar hiperparâmetros como tamanho do lote e taxa de aprendizagem. Métricas de desempenho, incluindo acurácia, precisão, recall, F1-score e AUC-ROC, foram monitoradas, utilizando validação cruzada k-fold para avaliar a robustez do modelo.Avaliação de Desempenho e Ajustes: Os resultados indicaram que um tamanho de lote de 64 e uma taxa de aprendizagem de 0,001 proporcionavam um bom equilíbrio entre precisão e tempo de treinamento. No entanto, ajustes na taxa de aprendizagem resultaram em comportamentos oscilatórios, exigindo uma abordagem mais adaptativa.Para melhorar a eficiência da otimização de hiperparâmetros, a otimização bayesiana foi implementada, mostrando uma melhoria significativa na eficiência da busca. Além disso, pipelines de dados foram automatizados para melhorar a qualidade e estabilidade do treinamento do modelo. Embora essas soluções tenham mostrado resultados promissores, sua eficácia completa ainda precisa ser validada em um ambiente de produção.Foram realizados estudos sobre a integração de modelos de machine learning, utilizando TensorFlow e PyTorch, com sistemas legados. Focou-se na atualização em tempo real dos modelos sem afetar a performance do sistema. Avaliou-se o impacto das atualizações em tempo real e desenvolveu-se APIs e middlewares para facilitar a comunicação entre os modelos e os sistemas legados. Protocolos de segurança foram implementados para garantir a integridade dos dados.Testes extensivos em ambientes de produção real mediram a precisão, recall e F1-score dos modelos. Desenvolveram-se pipelines para monitorar a performance dos modelos e realizar atualizações contínuas. Analisou-se a capacidade dos modelos de identificar padrões de fraude proativamente.Os resultados mostraram uma melhoria significativa na detecção de fraudes antes que ocorressem danos, mantendo a performance do sistema. A precisão e recall na detecção de fraudes foram altas, reduzindo falsos positivos e negativos.Adoção de Tokenização e Criptografia Ponta a Ponta: Pesquisas detalhadas avaliaram a viabilidade e a eficácia da tokenização e da criptografia ponta a ponta em ambientes heterogêneos e multiplataforma. Estes estudos ajudaram a equipe a entender como essas tecnologias poderiam ser implementadas de maneira eficaz, mantendo a integridade e a segurança dos dados dos clientes em todas as transações.Desenvolvimento de um Sistema de Mensageria e Bancos de Dados In-memory: Foram realizados estudos sobre o uso de Kafka para gerenciamento de fluxo de mensagens em alta velocidade e a implementação de bancos de dados in-memory para acesso rápido a dados críticos. A pesquisa focou na integração e escalabilidade dessas tecnologias em um ambiente complexo, garantindo que as necessidades de rapidez e eficiência transacional fossem atendidas.Criação de APIs e Adaptadores Customizados para Interoperabilidade: Estudos aprofundados sobre a comunicação com diversas redes de cartões e bancos internacionais informaram o desenvolvimento de APIs e adaptadores customizados. Essas pesquisas abordaram a complexidade de implementação e manutenção dessas interfaces, buscando soluções que promovessem flexibilidade e conformidade sem comprometer a performance do sistema. INFORMAÇÃO COMPLEMENTAR: Para além do exposto, é possível destacar que o desenvolvimento das tecnologias apresentadas visou aprimorar a eficiência e a segurança na detecção de fraudes, um ponto extremamente importante em sistemas financeiros. A capacidade de processar grandes volumes de dados em tempo real, essencial para a detecção efetiva de fraudes, foi significativamente melhorada pela otimização da arquitetura das redes neurais e pela implementação de código mais eficiente.Além disso, a iniciativa de integrar mecanismos de criptografia ponta a ponta e aprimorar a gestão de tokens para proteger dados sensíveis dos clientes destaca iniciativa da empresa em evoluir seus processos e mecanismos para garantia de total segurança em suas plataformasA experimentação e integração de modelos de aprendizado de máquina em sistemas legados, superando desafios de serialização e compatibilidade de bibliotecas, destacam o caráter inovador do projeto. Essa abordagem possibilita evoluções importantes para a empresa, bem como gera novas bases de conhecimento e experiência para o desenvolvimento de novos projetos de inovação no futuro.SUBSIDIOS PARA ATIVIDADES DE P&D:Mahmoud, A., Salem, A., & Elsamahy, E. (2021). Real-time machine learning-basedframework for the analysis of banking financial data. In Lecture Notes in Networks and Systems(Vol. 224, pp. 407–421). Springer. RESULTADO ECONÔMICO: A linha de pesquisa apresentada permitiu processar grandes volumes de dados eficientemente, reduzindo o tempo e os custos associados ao treinamento. Ajustes na arquitetura e na implementação resultaram em operações mais ágeis e econômicas. RESULTADO INOVAÇÃO: A introdução de mecanismos de criptografia ponta a ponta e a gestão avançada de tokens possibilitou aumentar a segurança dos dados sensíveis dos clientes, além de otimizar a performance do sistema, equilibrando segurança e eficiência operacional. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27835;2023;CNPJ: 33885724000119 RAZÃO SOCIAL :BANCO ITAU CONSIGNADO S.A. ATIVIDADE ECONOMICA :Bancos comerciais Cd ATIV.ECONOMICA IBGE :K.64.21-2/00 PORTE Demais ID EMPRESA/ANO :27835;NÚMERO: 16 ID ÚNICO: 109736 NOME: SOLUÇÕES DE AUTOMAÇÃO DE PROCESSOS COMPLEXOS A PARTIR DE ESTRUTURAS LEGADAS E MODERNIZADAS DESCRIÇÃO: Esta linha de pesquisa tem, como objetivo tecnológico, demonstrar a viabilidade técnica de solução de automação de processos complexos sediados em estruturas legadas e em nuvem. Para isso, organizou atividades experimentais buscando proposições a serem validadas a partir de protótipos.Processos de automação referem-se ao uso de ferramentas e scripts para automatizar tarefas complexas ao longo do ciclo de vida do desenvolvimento de software. No contexto aqui estudado, trata-se de construir mecanismos automáticos para coleta, centralização e consolidação de dados de cliente. Na solução atual, a maior parte dos processos era manual,  os dados não eram padronizados (nem na forma de implementação e nem sua taxonomia) impactando no controle de qualidade associado. Não havia processo automático que lidasse com dados em diferentes infraestruturas tecnológicas (mainframe e nuvem). Com isso, a linha de pesquisa se voltou a entender elementos, tecnologias e processos a serem considerados em processos de automação entre diferentes ambientes.A automação de processos, à primeira vista, pode parecer uma solução simples e direta para otimizar as operações de uma empresa,  essa percepção geralmente vem da ideia de que a automação apenas substitui tarefas manuais por softwares seguindo um conjunto predefinido de regras. No entanto, criar e desenvolver fluxos até então descentralizados e em infraestruturas mistas enfrenta complexidade sobre demandar conhecimento profundo de diferentes tecnologias e, de forma criativa, propor combinações para composição de automação. Assim, configurou-se como ponto crítico desta linha de pesquisa diferentes composições levadas a experimento considerando metodologia sistemática, criatividade e um certo risco de insucesso.Vale pontuar, ainda, que a automação de processos em ambientes mainframe e em nuvem é complexa, especialmente em uma estrutura bancária centenária. Mainframes, que são cruciais para operações críticas e transações de alta segurança, utilizam linguagens e protocolos antigos, como COBOL e CICS, que não se integram facilmente com as soluções baseadas em nuvem que utilizam APIs modernas e microserviços, por exemplo. Além disso, a migração e integração de dados entre esses ambientes exigem rigorosos controles de segurança e conformidade regulatória devido à sensibilidade dos dados bancários. A compatibilidade entre diferentes arquiteturas, a necessidade de garantir a continuidade dos serviços e a minimização de riscos tornam a automação um desafio técnico significativo, exigindo soluções personalizadas e uma abordagem meticulosa para evitar interrupções e garantir a integridade dos dados.Ao final, esperava-se a composição de um protótipo de sistema de automação de esteiras de dados a ter sua viabilidade técnica testada em ambiente controlado, compondo uma solução agnóstica de automação independente da maturidade de suas tecnologias. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: automação,  centralização de dados,  padronização de dados,   qualidade de dados,  mainframe,  computação em nuvem,  máquinas de estado. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Esta linha de pesquisa voltou-se a demonstrar a viabilidade técnica de solução de automação de processos baseados em estruturas mainframe e em nuvem. Com isso, permitiu entender e aplicar novas orquestrações de tecnologias que, mesmo que individualmente não sejam novas, apresentam-se como inovadoras ao serem aplicadas a um novo contexto.Os estudos resultaram em um conjunto de aprendizados para o desenvolvimento de processo de automação e que foram considerados em protótipo. Houve compreensão de processo automático de captura de operações que são registrados na tabela história substituindo as informações da partição do RT2 (do mainframe). Essa fonte deve ser utilizada em uma plataforma de análise de dados com ferramentas para preparação, blendagem, análise e compartilhamento de dados. Entendeu-se um mecanismo de leitura de arquivo gerado por analistas subsidiando a criação de scripts para tratar o arquivo e armazená-lo na base histórica. Foi possível entender o tratamento e a inclusão de regras específicas de cada produto sendo o output do processo armazenado no S3 como arquivo JSON.Estudos levaram à proposição de fluxo automático de comunicação com as informações internas realizada por meio de tópicos Kafka para registrar eventos de disponibilização de arquivos. Para monitorá-los, comprovou-se a aplicação de uma função sem servidor acionada por um conector Lambda. Essa função deve se autenticar internamente via token e fazer o download das informações, armazenando-as em um bucket S3.Para otimizar o consumo dos endpoints das APIs envolvidas, foi compreendida a criação de um processo de automação que armazena qualquer informação complementar no bucket S3. As informações transacionais representam a primeira camada de dados (SOR), que são os dados brutos sem transformações. Após análise desses dados, compreendeu-se a necessidade de construir outra camada (SOT) para conter os dados após as transformações necessárias. Após a gravação da camada SOT, houve a compreensão de uma nova etapa de automação com plataforma externa.Entendeu-se a aplicação de Java em ECS na estrutura QuickCloud projetada para implementação, gerenciamento e automação de aplicações em ambientes de nuvem. Para simplificar e padronizar as aplicações backend, a automação foi realizada a partir do acionamento da aplicação Java de uma Step Function através de um EventBridge. Foi necessário entender como configurar uma regra no EventBridge que trate de mudanças de estado específicas e o destino para onde o evento deve ser encaminhado. O objetivo era validar um processo em que, quando um evento correspondente ao padrão definido ocorre, o EventBridge automaticamente dispara a Step Function associada, iniciando seu fluxo de trabalho. Dentro da Step Function, cada etapa é definida para realizar uma tarefa específica, como a execução de uma função Lambda que roda código Java, a manipulação de dados, a tomada de decisões baseada em condições pré-definidas ou a interação com serviços em nuvem. Aprendeu-se que a integração entre EventBridge e Step Functions permite a criação de fluxos de trabalho altamente flexíveis e automatizados, onde a aplicação Java pode ser acionada e manipulada de acordo com a lógica de negócios definida na Step Function, proporcionando uma solução para automação de processos em ambientes cloud.A aplicação do Step Function para orquestrar lambdas de integração permitiu a reconstrução da aplicação legada em mainframe na forma de máquinas de estado. Foi possível entender, a partir da abstração proporcionada por erros e retentivas, a criação de uma lambda genérica para automatizar processos entre APIs e Kafka.Estes aprendizados permitiram construir protótipo buscando validação técnica de orquestrações inéditas de tecnologias em novos contextos resultando em avanços significativos no estado da arte relacionado à automação de processos.Os desenvolvimentos relacionados ao novo fluxo de automação também foram reconhecidos com prêmio Hackathon. DESAFIO TECNOLÓGICO: Para demonstrar a viabilidade técnica, a partir da construção de protótipo, de solução de automação de processos a partir de estruturas legadas mainframe e em nuvem, esta linha de pesquisa lidou com um conjunto de desafios técnicos.Os fluxos de trabalho envolvidos deveriam ser consolidados em uma infraestrutura em nuvem. Com isso, não se tratava de uma mera integração ou adaptação de tecnologias,  havia que se criar estratégia de automação considerando a transformação dos fluxos. Como as tecnologias apresentam configurações distintas, era necessário encontrar formas de compatibilidade – por exemplo, sistemas em mainframe são configurados para dar rollback em caso de erros na aplicação,  já tecnologias em nuvem não têm tratamento para esse tipo de situação. Tal fato coloca dificuldades de alarmes em caso de erros de forma que eles sejam identificados apenas no final do processo. Casos como esse aumentavam a complexidade do projeto.Processos de automação considerando infraestruturas mistas são desafiadores por natureza. Era necessário compreender como construir uma comunicação efetiva entre sistemas realizada por meio de tópico Kakfa combinado com lambda. Embora esta composição seja comumente utilizada em processos de transmissão e processamento de dados em tempo real, estudos mostraram que esse arranjo não aborda completamente a orquestração de sistemas mistos sendo insuficientes tanto frameworks como metodologias específicas para facilitar a integração nestas condições. Estudos de referência indicam que ainda não há uma solução usando Kafka e Lambda de forma a lidar, sem problemas, com questões de padronização e governança de dados em tempo real, o que deveria ser comprovado tecnicamente.Estudos indicaram que a abordagem habitual de microsserviços não atenderia requisitos de processamento em paralelo devido à complexidade de coordenação entre múltiplos serviços distribuídos,  a latência e a sobrecarga de comunicação entre microsserviços poderiam dificultar requisitos de alto desempenho em processamento paralelo. Levou-se a experimento a aplicação de Step Function em conjunto com lambdas responsáveis por papéis específicos dentro do fluxo. A proposição testada era a de configuração do fluxo de automação a partir de estímulo do gateway responsável por receber as requisições de elegibilidade que repassa os dados para um lambda que realiza validações de contrato e resgata as regras de elegibilidade presentes em um banco NoSQL específico para a categoria de produto informada. Ocorre, então, uma transformação de dados para um padrão esperado e o Step Function, com base na regra de elegibilidade consultada, estimularia a execução paralela dos motores. Em seguida, testou-se compor uma tratativa dos diferentes resultados dos motores a fim de serem transmitidos para o lambda responsável pela construção da resposta. A proposição a ser testada era que o lambda processaria os resultados dos motores, persistiria o resultado no banco NoSQL e retornaria o payload de resultado para o gateway estimulado no passo inicial.Considerou-se a hipótese também de incluir no processo um possível tratamento manual até a validação final do processo. Levou-se a experimento a construção de lambda com definições das máquinas de estado utilizando o Step Function colocando o processo manual como ferramenta de apoio para validação. Porém, lidou-se com desafios relacionados a compatibilizar aplicações de Step Function com a construção da lógica para criação de máquina de estado. Apesar de as tarefas no Step Function serem realizadas por um lambda, era incerto ser possível integrar diversas funções lambdas tal como presentes na arquitetura proposta. Experimentos voltaram-se a entender como fazer manuseio das estruturas de dados que transitavam no Step Function. Era incerto ser possível construir ações de processamento paralelo considerando as máquinas de estado de forma a sincronizar resultados. METODOLOGIA: Para superar os desafios encontrados nesta linha de pesquisa foram organizadas as seguintes atividades:1- Estudo sobre governança de dados buscando elementos para proposições relacionadas à construção de camada que lide com dados coletados de diferentes fontesMétricas:- Casos de governança a serem testados- Redução de inconsistência de dados associado a cada caso2- Experimentos controlados buscando entender e avaliar tecnologias relacionadas à automação de processos complexos buscando referências a serem consideradasMétricas:- Número de tecnologias identificadas- Número de PoCs elaboradas e testadas- Percentual de processos automatizados que são concluídos com sucesso sem intervenção manual3- Experimentos considerando diferentes estratégias para otimizar o consumo de APIs. Levou-se a teste diferentes cenários de uso de cache, batching de requisições e técnicas de rate limitingMétricas:- Redução de latência relacionada a APIs- Aumento na taxa de transferência da API4- Execução de PoCs para validar principais soluções em ambiente controladoMétricas:- Número de PoCs executadas- Taxa de sucesso das PoCs em atender os critérios testados5- Experimentações a partir de ferramentas de orquestração de fluxo de trabalho, como Step Functions, buscando entendimentos para delineamento de proposições para suportar processos de desvio e tratamento de errosMétricas:- Número de cenários do fluxo da orquestração- Taxa de sucesso de processos para lidar com erros- Redução da intervenção manual no tratamento de erros6- Estudos buscando referências para desenvolver um processo sistemático para realizar testes de integração e validação manual das aplicações. A partir de delineamentos experimentais associados, foram feitas experimentações com o objetivo de definir um conjunto de critérios e procedimentos para testes manuais considerando a configuração de lambdas e máquinas de estado, Métricas:- Taxa de sucesso a partir dos testes manuais para validação- Redução de problemas de integração pós-validação de orquestração7- Estudos considerando uso de Step Functions e aplicação de Lambda buscando proposições com foco no processamento paralelo e na manipulação de múltiplos caminhos e estados. Experimentos buscando validar propostas da capacidade de sincronização de resultados e a eficiência na gestão de estados complexosMétricas:- Número de cenários de processamento paralelo- Taxa de sucesso de sincronização- Melhoria da eficiência na gestão de estados8- Estudos considerando técnicas avançadas para otimizar a persistência de dados em bancos NoSQL com foco em esquemas de dados que suportem consultas eficientes e escaláveisMétricas:- Número de cenários de persistência de dados testados- Redução no tempo de recuperação de dados9- Desenvolvimento de protótipo para validar as orquestrações de tecnologias em novos contextosMétricas:- Tempo de resposta e taxa de transferência sob condições de carga simulada- Tempo médio de processamento por tarefa/transação- Número de problemas de compatibilidade entre mainframe e nuvem INFORMAÇÃO COMPLEMENTAR: O desenvolvimento desta linha de pesquisa parte da premissa de que, conforme estabelecido pelo Manual de Frascati, a orquestração de tecnologias conhecidas em novos contextos pode ser considerada uma inovação tecnológica. Nela, aplicam-se tecnologias já existentes, como APIs, Kafka, Lambda, Step Functions, entre outras, em um contexto de integração entre ambientes mainframe e nuvem que, por definição, são únicos. A inovação aqui não reside nas tecnologias individualmente, mas na maneira como elas são combinadas e aplicadas para resolver problemas específicos de automação de processos complexos.Como mainframes utilizam linguagens e protocolos antigos, como COBOL e CICS, que não se integram facilmente com soluções modernas baseadas em nuvem que utilizam APIs e microserviços, a compatibilidade entre diferentes arquiteturas, a necessidade de garantir a continuidade dos serviços e a minimização de riscos tornam a automação um desafio técnico significativo. Portanto, é essencial realizar experimentações, a partir de atividades de pesquisa e desenvolvimento, para desenvolver soluções personalizadas que atendam a esses requisitos específicos.Vale pontuar que a impossibilidade de realizar desenvolvimentos diretamente em ambientes produtivos influenciou a forma de organização das atividades experimentais. Qualquer erro ou falha poderia resultar em indisponibilidade de serviços críticos, o que é inaceitável em um contexto bancário. Portanto, a estratégia adotada foi buscar proposições para testar em protótipos, buscando validações técnicas antes de qualquer implementação em ambiente produtivo.Como subsídio para o desenvolvimento desta linha de pesquisa, destacam-se as seguintes referências bibliográficas:- Achanta, M. (2023). Data governance in the age of cloud computing: Strategies and considerations. International Journal of Science and Research (IJSR), 12(11).- Cheng, M., Qu, Y., Jiang, C., & Zhao, C. (2022). Is cloud computing the digital solution to the future of banking? Journal of Financial Stability, 63, 101073.- Kleppmann, M. (2017). Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. O'Reilly Media.- ORGANIZAÇÃO PARA A COOPERAÇÃO E DESENVOLVIMENTO ECNONÔMICO (OCDE). Manual de Frascati: Proposta de práticas exemplares para inquéritos sobre investigação e desenvolvimento experimental. Coimbra: OCDE, 2007.- Rana, M. E., Yik, T. M., & Hameed, V. A. (n.d.). Cloud computing adoption in the banking sector: A comparative analysis of three major CSPs. IEEE. RESULTADO ECONÔMICO: Diminuição de custos associados à redução de redundâncias de processo. RESULTADO INOVAÇÃO: Nova capacidade automática para monitoramento de inclusão e processamento de dados de diversas áreas. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 1 ID ÚNICO: 109737 NOME: EXPERIMENTAÇÃO DE NOVAS TECNOLOGIAS PARA OTIMIZAÇÃO DE PAGAMENTOS E DIGITALIZAÇÃO DE SERVIÇOS BANCÁRIOS DESCRIÇÃO: As atividades de Pesquisa, Desenvolvimento e Inovação (P,D&I) focaram na criação de soluções tecnológicas a serem validadas científica e tecnologicamente. A pesquisa e experimentação foram direcionadas para evoluir as limitações existentes em termos de eficiência, consistência de dados e segurança, experimentando arquiteturas não utilizadas anteriormente. As propostas tecnológicas incluíram a implementação de uma arquitetura de componentes reativos e SPA, desenvolvimento de microserviços escaláveis, ferramentas de autenticação multifator e uma API com conteinerização e orquestração, visando estabelecer novas diretrizes para o desenvolvimento de soluções tecnológicas. O objetivo desta linha de pesquisa foi demonstrar a viabilidade técnica do desenvolvimento e integração de soluções avançadas para otimização de pagamentos e digitalização de serviços bancários.Especificamente, buscou-se melhorar a correção autônoma de repasses incorretos por meio da implementação de uma funcionalidade de autosserviço baseada em um aplicativo de página única (SPA) com arquitetura de componentes reativos. Além disso, foi desenvolvida uma arquitetura de microserviços com backend assíncrono e frontend com renderização no lado do cliente para capturar operações de clientes externos de forma eficiente.Outro foco do projeto foi a digitalização do segmento Private, onde se testou a hipótese de que uma ferramenta com autenticação multifator e governança adequada poderia proporcionar maior autonomia e controle aos clientes no cadastro de pessoas de confiança. Para isso, foi prototipada uma ferramenta com frontend modular e backend distribuído, utilizando comunicação assíncrona entre serviços. Adicionalmente, buscou-se estruturar uma plataforma para oferecer transferências internacionais como serviço, criando uma API específica com técnicas de conteinerização e orquestração de clusters de contêineres, visando garantir escalabilidade e eficiência na troca de mensagens.Os desafios enfrentados incluíram a sincronização de dados em tempo real na arquitetura de microserviços e a manutenção da consistência dos dados no SPA. Problemas de latência e inconsistência de dados também foram observados na integração com APIs de permissionamento. A implementação de autenticação multifator apresentou complexidades na integração segura e eficiente de diferentes formas de autenticação.O marco crítico do projeto surgiu como um ponto de decisão crucial, onde os desafios enfrentados na sincronização de dados em tempo real e na manutenção da consistência dos dados em arquiteturas complexas de microserviços e Single Page Applications (SPA) ameaçavam a viabilidade das soluções tecnológicas propostas. Este momento representou um teste significativo para a equipe de desenvolvimento, pois a eficiência, a consistência de dados e a segurança são fundamentais para a otimização de pagamentos e a digitalização de serviços bancários. A falta de otimização nessas áreas não só comprometeria a funcionalidade e a experiência do usuário final, mas também poderia resultar em falhas de segurança críticas, colocando em risco a integridade dos sistemas bancários digitais.Após os desenvolvimentos, espera-se alcançar interfaces mais responsivas e eficientes, maior segurança e autonomia para os usuários, e APIs que oferecessem menor latência e maior eficiência em operações de câmbio. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Autosserviço,  Componentes reativos,  Gerenciamento de estado,  Microserviços,  Escalonamento horizontal,  Sincronização de dados,  Governança,  Conteinerização,  Orquestração de contêineres,  Protocolo gRPC. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Foi testada a hipótese de que a implementação de uma funcionalidade de autosserviço baseada em um aplicativo de página única (SPA) utilizando uma arquitetura de componentes reativos poderia melhorar a correção autônoma de repasses incorretos por clientes. Para isso, foi prototipado um SPA utilizando uma arquitetura baseada em componentes reativos, implementados com uma linguagem de programação funcional. A reatividade foi avaliada quanto à sua capacidade de permitir atualizações dinâmicas na interface sem a necessidade de recarregamento da página, visando melhorar o tempo de resposta. Técnicas de gerenciamento de estado centralizado foram aplicadas para garantir a consistência dos dados na interface do usuário.Levou-se a experimento a ideia de que uma arquitetura de microserviços com backend assíncrono e frontend com renderização no lado do cliente poderia capturar operações de clientes externos boletadores de forma eficiente, independente da tecnologia utilizada pelo cliente. Para validar essa hipótese, foi criado um componente de software com uma arquitetura de microserviços, onde o backend foi estruturado usando um modelo de servidor assíncrono baseado em eventos, capaz de lidar com um grande número de conexões simultâneas. O frontend utilizou uma abordagem de renderização no lado do cliente com virtual DOM, visando uma interação mais fluida e responsiva. A integração com APIs de permissionamento foi realizada através de um protocolo de comunicação RESTful, consumindo dados de um banco de dados relacional com transações ACID para garantir a consistência dos dados. A computação em nuvem foi empregada para criar microserviços responsáveis pela gestão dos permissionamentos de câmbio, utilizando técnicas de escalonamento horizontal automático para lidar com picos de demanda. Durante os testes de carga, a eficiência e a escalabilidade da arquitetura de microserviços foram confirmadas, validando a hipótese. Contudo, desafios de sincronização de dados em tempo real foram identificados, exigindo a implementação de mecanismos de replicação de dados e otimizações nos protocolos de comunicação.Para a digitalização do segmento Private, foi testada a hipótese de que a implementação de uma ferramenta com autenticação multifator e governança adequada poderia proporcionar maior autonomia e controle aos clientes ao cadastrar pessoas de confiança para movimentar suas contas. Foi então realizada a prototipação de uma ferramenta com frontend desenvolvido utilizando um modelo de componentes modulares com shadow DOM para encapsulamento de estilos e backend estruturado com uma arquitetura de serviços distribuídos utilizando um barramento de mensagens para comunicação assíncrona entre serviços. A autenticação multifator foi implementada utilizando um protocolo de segurança robusto, combinando senha, token e biometria. A autorização foi gerenciada através de políticas de acesso definidas em um servidor de identidade que utiliza tokens JWT (JSON Web Tokens) para garantir a segurança das transações.Para estruturar uma plataforma para oferecer transferências internacionais como serviço, foi testada a hipótese de que a criação de uma API específica, utilizando técnicas de conteinerização e orquestração com clusters de contêineres, poderia garantir escalabilidade e isolamento de processos, proporcionando menor latência e maior eficiência na troca de mensagens. Para testar essa hipótese, foi criada uma API utilizando técnicas de conteinerização e orquestração com clusters de contêineres. A comunicação entre os serviços foi gerenciada por um protocolo de alta performance baseado em gRPC, visando menor latência e maior eficiência na troca de mensagens. A integração com serviços de gerenciamento de eventos, como sistemas de mensageria distribuída baseados em logs, foi fundamental para criar novos recursos para os serviços de negociação de câmbio, possibilitando a detecção e resposta a eventos em tempo real. DESAFIO TECNOLÓGICO: A arquitetura de microserviços apresentou problemas significativos na sincronização de dados em tempo real. A natureza distribuída dos microserviços resultou em inconsistências temporárias de dados, especialmente durante picos de demanda. A latência na propagação de mudanças entre diferentes instâncias de serviços e a dificuldade em garantir a consistência eventual dos dados foram questões críticas. Para mitigar esses problemas, levantou-se a hipótese de implementar um sistema de replicação de dados em tempo real utilizando Apache Kafka para mensageria distribuída. A integração desse sistema com a arquitetura existente e a garantia de baixa latência e alta consistência em um ambiente distribuído permaneceram desafios significativos, cuja eficácia ainda precisava ser validada em cenários de produção.O gerenciamento de estado centralizado no Single Page Application (SPA) enfrentou dificuldades na manutenção da consistência dos dados entre diferentes componentes reativos. A complexidade aumentou com a necessidade de gerenciar atualizações concorrentes e interações complexas entre componentes, resultando em estados inconsistentes e comportamentos inesperados na interface do usuário. A sincronização de estado entre o frontend e o backend também introduziu desafios adicionais, especialmente em condições de rede instáveis. Para resolver esses problemas, foi proposto o uso da biblioteca Redux para gerenciamento de estado, combinada com técnicas de controle de concorrência como optimistic updates e versionamento de estado. No entanto, a implementação dessas técnicas em um sistema altamente interativo e dinâmico poderia introduzir novos overheads de desempenho e complexidades, cuja eficácia ainda precisava ser rigorosamente testada.A integração com APIs de permissionamento, utilizando um protocolo de comunicação RESTful, encontrou problemas de latência e inconsistência de dados devido à complexidade das transações ACID no banco de dados. A latência nas operações de leitura e escrita e a dificuldade em manter a consistência de dados em um ambiente distribuído foram questões críticas. A necessidade de garantir a atomicidade, consistência, isolamento e durabilidade das transações complicou ainda mais a integração. Para melhorar a consistência e a latência das operações, foi sugerida a migração para CockroachDB, um banco de dados distribuído que oferece suporte a transações distribuídas. No entanto, a transição para esse novo sistema de banco de dados era uma tarefa que envolvia riscos significativos de interrupção de serviço e problemas de compatibilidade, além de exigir uma reengenharia substancial dos sistemas existentes.A implementação de autenticação multifator, combinando senha, token e biometria, apresentou desafios na integração dessas diferentes formas de autenticação de maneira segura e eficiente, sem comprometer a experiência do usuário. A coordenação entre diferentes métodos de autenticação e a garantia de que cada método fosse igualmente seguro e resistente a ataques foi uma tarefa complexa. A gestão de tokens e a sincronização de dados biométricos em tempo real também introduziram desafios de segurança e privacidade. Para simplificar a implementação, foi levantada a hipótese de utilizar o framework Auth0, que em tese, ofereceria suporte integrado para múltiplas formas de autenticação e políticas de segurança.A comunicação entre os serviços conteinerizados, gerenciada por gRPC, enfrentou desafios de latência e eficiência, especialmente em ambientes de alta concorrência e complexidade. A sobrecarga de serialização e deserialização de dados, juntamente com a gestão de conexões persistentes e a otimização do tráfego de rede, foram questões críticas. Para reduzir a latência e melhorar a eficiência, foi considerada a hipótese de introduzir técnicas de compressão de dados e otimização de protocolos de comunicação, juntamente com o uso de redes definidas por software (SDN) para gerenciar o tráfego de rede de maneira mais eficiente. METODOLOGIA: 1- Experimentos com Aplicações de Página Única (SPAs) baseadas em uma arquitetura de componentes reativos para a correção autônoma de transferências incorretas:Métricas:Medir o tempo que a interface leva para atualizar após uma interação do usuário, como clicar em um botão ou digitar um texto (responsividade).Avaliar a quantidade de dados transferidos entre o cliente e o servidor durante as atualizações da interface (eficiência).Acompanhar o número de correções autônomas bem-sucedidas feitas pelos clientes usando as SPAs em comparação com o método anterior (taxa de sucesso).2- Experimentos com uma arquitetura de microsserviços com backend assíncrono e renderização no cliente para capturar operações de clientes externos de forma eficiente:Métricas:Medir o tempo que uma solicitação do frontend leva para chegar ao backend e receber uma resposta (latência).Calcular o número de operações de clientes externos que o sistema pode processar em um determinado período (throughput).Monitorar a utilização de recursos do sistema (CPU, memória, rede) à medida que o número de solicitações aumenta (escalabilidade).3- Protótipos de uma ferramenta com autenticação multifator e governança adequada para fornecer maior autonomia e controle aos clientes no cadastro de pessoas de confiança para operar suas contas no Segmento Private:Métricas:Medir o tempo que um usuário leva para se autenticar com sucesso usando o sistema de autenticação multifator (tempo de autenticação).Acompanhar o número de tentativas de autenticação falhas e analisar os motivos do fracasso (segurança).Realizar pesquisas ou entrevistas com os usuários para coletar feedback sobre o nível de controle e autonomia proporcionado pela ferramenta (feedback do usuário).4- Protótipos de uma API para Transferências Internacionais usando técnicas de conteinerização e orquestração de clusters de contêineres:Métricas:Medir o tempo que uma solicitação leva para ser processada e uma resposta ser enviada pela API (latência).Calcular o número de solicitações que a API pode processar por segundo (throughput).Monitorar a utilização de recursos do ambiente conteinerizado (CPU, memória) à medida que o número de solicitações aumenta (escalabilidade). INFORMAÇÃO COMPLEMENTAR: À medida que o setor bancário enfrenta transformações digitais impulsionadas por mudanças nas necessidades dos clientes e a crescente tendência do open banking, as arquiteturas monolíticas tradicionais tornam-se cada vez mais inadequadas. Consequentemente, o setor está em um ponto crucial onde a adoção de microsserviços surge como uma necessidade estratégica para manter a competitividade e a eficiência operacional. No entanto, a transição exige uma abordagem cautelosa e iterativa, marcada por prototipagem e experimentações minuciosas.No mundo acelerado de hoje, agilidade e capacidade de resposta são fundamentais. O cenário do setor bancário está mudando rapidamente, com os clientes esperando experiências digitais perfeitas e novos jogadores fintech desafiando o status quo. Os microsserviços oferecem uma vantagem crucial ao permitir que os bancos desenvolvam e lancem novos recursos com uma velocidade sem precedentes. Ao contrário dos sistemas monolíticos, que muitas vezes paralisam a inovação devido à sua rigidez, os microsserviços fragmentam as aplicações em serviços independentes e gerenciáveis, permitindo ajustes ágeis em resposta às dinâmicas do mercado e às expectativas dos clientes.Além disso, os sistemas legados em muitos bancos evoluíram para se tornarem monstros monolíticos ao longo do tempo, criando uma estrutura labiríntica que é tanto dispendiosa quanto difícil de manter. Os microsserviços aliviam essa complexidade ao quebrar esses sistemas volumosos em unidades menores e compreensíveis. Esta abordagem modular não só melhora a manutenção, mas também facilita atualizações sem comprometer a estabilidade de todo o sistema.A capacidade de escalar serviços de forma independente é outra razão convincente para que os bancos adotem microsserviços. As arquiteturas tradicionais exigem a escalabilidade do sistema inteiro para atender à crescente demanda, resultando em utilização ineficiente de recursos e aumento de custos. Em contraste, os microsserviços permitem a escalabilidade direcionada de componentes específicos. Por exemplo, um banco pode escalar o microsserviço que gerencia transações móveis durante os horários de pico independentemente dos outros serviços, garantindo eficácia em custos e desempenho sustentado sob cargas pesadas.Engajar-se na prototipagem permite que os bancos experimentem diferentes estratégias arquitetônicas em um ambiente controlado antes de se comprometerem com mudanças amplamente disseminadas. Esta abordagem ajuda a identificar possíveis obstáculos, validar premissas e evitar erros custosos. Obter insights iniciais através de protótipos, portanto, reduz os riscos da jornada de migração como um todo.A mudança para uma arquitetura de microsserviços naturalmente introduz complexidades técnicas, como manter a consistência de dados em serviços distribuídos, gerenciar a comunicação entre serviços e orquestrar uma rede distribuída. Experimentando com vários mecanismos de replicação de dados e diferentes protocolos de comunicação entre serviços (como REST ou gRPC), os bancos podem encontrar o equilíbrio ideal entre desempenho e complexidade operacional. Além disso, avaliar diferentes ferramentas de descoberta e orquestração de serviços através da prototipagem permite que os bancos gerenciem efetivamente a natureza distribuída dos microsserviços.Auer, F., Lenarduzzi, V., Felderer, M., & Taibi, D. (2021). From monolithic systems to microservices: An assessment framework. Information and Software Technology, 137, 106600.Ayas, M. H., Leitner, P., & Hebig, R. (2023). An empirical study of the systemic and technical migration towards microservices. Empirical Software Engineering, 28(85).Bhole, K., Nareddy, R., & Laughridge, K. (n.d.). Opening banking through architecture re-engineering: A microservices-based roadmap. Deloitte United States.Wang, Y., Kadiyala, H., & Rubin, J. (2021). Promises and challenges of microservices: an exploratory study. Empirical Software Engineering, 26(4), 63. RESULTADO ECONÔMICO: Redução significativa do tempo de resposta para correções autônomas de transações pelos clientes. Isso melhorou a eficiência operacional, reduzindo custos com suporte e aumentando a satisfação do cliente. RESULTADO INOVAÇÃO: A inovação resultou em uma plataforma tecnológica segura e eficiente, melhorando a agilidade e escalabilidade dos serviços financeiros com autenticação robusta e técnicas de conteinerização, otimizando a gestão de transações. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;"NÚMERO: 2 ID ÚNICO: 109738 NOME: NOVAS TECNOLOGIAS PARA SOLUÇÕES DE CRÉDITO E INVESTIMENTO DESCRIÇÃO: A presente linha de pesquisa objetivou explorar a viabilidade técnica da aplicação de avançados recursos tecnológicos, tais como APIs, funções serverless e inteligência artificial, no contexto de uma plataforma de processamento. A iniciativa se inseriu no âmbito de Pesquisa, Desenvolvimento e Inovação (P,D&I), com o intuito de superar desafios prementes como a lentidão no processamento de requisições, vulnerabilidades de segurança e ineficiências na análise de crédito. No decorrer do projeto, foram conduzidos experimentos e prototipações. Essas atividades experimentais tiveram como objetivo principal investigar, de maneira empírica, a eficácia das tecnologias propostas em endereçar os problemas identificados.Para aprimorar a eficiência do sistema, adotou-se uma abordagem experimental focada na utilização de APIs e funções serverless. Os experimentos visaram quantificar o impacto dessas tecnologias na redução do tempo de resposta às requisições de acesso. Paralelamente, a segurança da plataforma foi objeto de um robusto conjunto de testes, incluindo simulações de ataques DDoS e testes de intrusão, com o propósito de identificar e corrigir vulnerabilidades. A performance da plataforma foi igualmente submetida a uma avaliação experimental, com a otimização de bancos de dados relacionais e a implementação de serviços de busca. Uma central de validações foi criada e testada, com o intuito de verificar sua eficácia em agilizar operações e elevar a satisfação do cliente. No âmbito da análise de crédito, a viabilidade e eficácia de um sistema de inteligência artificial foram testadas, visando avaliar sua precisão nas decisões de crédito e o potencial para redução do risco de inadimplência.Os desafios encontrados durante a fase experimental, tais como o aumento da latência sob carga extrema e a identificação de vulnerabilidades de segurança críticas, forneceram insights valiosos para o aprimoramento contínuo do projeto. A complexidade e latência introduzidas pela centralização das validações e pelos modelos de machine learning foram meticulosamente analisadas, contribuindo para o refinamento das soluções propostas.O marco crítico desta linha de pesquisa manifestou-se ao confrontar a escalabilidade e eficiência das funções serverless sob cenários de alta carga, juntamente com a necessidade de equilibrar a segurança, a integridade e a performance da plataforma. Esse ponto decisivo foi evidenciado durante os experimentos que simularam altas cargas de requisições, revelando um aumento significativo na latência que ultrapassava os limites aceitáveis de desempenho, especialmente acima de 20.000 requisições por minuto. Essa observação apontou para uma limitação crítica na arquitetura puramente serverless, sugerindo a necessidade de uma abordagem híbrida que pudesse manter a eficiência em condições normais de uso, mas também escalasse de maneira eficaz sob demanda extrema.Além disso, apesar de a maioria dos ataques ter sido mitigada com sucesso, a identificação de vulnerabilidades críticas em componentes específicos, como a interface de administração, destacou a necessidade de melhorias substanciais para assegurar a externalização segura da plataforma. A complexidade introduzida pela central de validações, embora efetiva em reduzir atritos e melhorar a experiência do usuário, resultou em um aumento na latência do sistema, especialmente em cenários com múltiplas validações simultâneas, marcando outro ponto crítico que exigia atenção.As expectativas pós-desenvolvimento, fundamentadas nos resultados experimentais, incluíam uma plataforma significativamente mais eficiente, segura e com performance otimizada. A implementação experimental de modelos de machine learning antecipava decisões de crédito mais precisas, com um compromisso com a transparência das decisões automatizadas. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Experimental,  dados,  carga,  processamento,  operações,  desenvolvimento,  hipótese,  otimização. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para validar a hipótese de que a utilização de APIs poderia melhorar a eficiência do sistema, experimentos foram realizados criando funções serverless que processavam requisições de acesso. Utilizou-se uma ferramenta para gerar múltiplas requisições simultâneas, simulando cenários de alta carga. As métricas de desempenho foram coletadas para analisar o tempo de resposta, a latência e a taxa de erro. Observou-se uma redução de 40% no tempo de resposta em comparação com a abordagem tradicional. Observou-se também que, em cenários de carga extrema (acima de 20.000 requisições por minuto), a latência aumentou significativamente, revelando a necessidade de otimizações ou de uma arquitetura híbrida que combine funções serverless com servidores dedicados.A segurança na externalização da plataforma foi outra área de experimentação. Implementações para gerenciar a autenticação de usuários e proteção contra ataques DDoS foram testadas. Testes de intrusão foram realizados para simular ataques cibernéticos e avaliar a robustez das defesas. A plataforma resistiu a maioria dos ataques simulados, mas algumas vulnerabilidades críticas foram identificadas em componentes específicos, como a interface de administração. Isso indicou que melhorias adicionais eram necessárias antes de considerar a externalização segura da plataforma. A integridade dos dados foi mantida, mas o tempo de resposta aumentou durante os ataques, sugerindo a necessidade de mecanismos de mitigação mais eficazes.A performance da plataforma foi testada com a hipótese de que bancos de dados relacionais otimizados e serviços de busca poderiam melhorar significativamente o desempenho. Durante as PoCs, operações de (re)alocação de recursos foram simuladas e painéis de gestão operacional e de transferência de clientes foram gerados. Os resultados mostraram uma melhoria de 50% na velocidade de consulta e uma redução no tempo de processamento de transações. A implementação de índices complexos para melhorar a performance das consultas trouxe em um aumento significativo no tempo de escrita, o que pode impactar negativamente operações em tempo real. A utilização de CPU e memória aumentou, sugerindo a necessidade de balanceamento entre otimização de consultas e eficiência de escrita.A criação de uma central de validações foi experimentada para verificar se poderia reduzir o tempo necessário para executar operações e melhorar a satisfação do cliente. Processos de validação foram orquestrados que verificavam diversas condições antes de permitir a execução de uma operação. Os experimentos mostraram que o sistema sinalizava ao usuário a possibilidade de uma ação apenas quando todos os critérios eram atendidos, reduzindo atritos e o tempo total de execução. Além disso, a complexidade introduzida pela central de validações resultou em um aumento na latência do sistema em cenários com múltiplas validações simultâneas.Outra hipótese testada foi a de que a implementação de um sistema de inteligência artificial para análise de crédito poderia melhorar a precisão das decisões de crédito e reduzir o risco de inadimplência. Modelos de machine learning foram treinados utilizando um conjunto de dados históricos de crédito, e diversas técnicas de aprendizado supervisionado foram aplicadas para identificar padrões e prever a probabilidade de inadimplência.Os modelos foram validados utilizando um conjunto de dados de teste separado, e as métricas de desempenho, como precisão, recall e AUC-ROC, foram avaliadas. Os resultados mostraram uma melhoria na precisão das decisões de crédito em comparação com métodos tradicionais baseados em regras. No entanto, a complexidade dos modelos resultou em tempos de processamento mais longos, o que pode ser um problema em cenários de alto volume de transações. Além disso, a transparência das decisões tomadas pelos modelos de IA foi uma preocupação, sugerindo a necessidade de técnicas de explicabilidade para garantir a confiança dos usuários nas decisões automatizadas. DESAFIO TECNOLÓGICO: Observou-se um aumento na latência ocorreu devido à sobrecarga de inicialização das funções serverless e à limitação na capacidade de escalar instantaneamente para atender a picos de demanda. Além disso, a natureza stateless das funções serverless dificultou o gerenciamento eficiente de sessões de usuário e a manutenção de estado entre requisições consecutivas, contribuindo para a degradação do desempenho. A hipótese de solução foi adotar uma arquitetura híbrida que combinasse funções serverless com servidores dedicados. A ideia era utilizar funções serverless para lidar com cargas de trabalho variáveis e servidores dedicados para garantir a consistência e o desempenho em cenários de alta carga.Testes de intrusão revelaram vulnerabilidades críticas na interface de administração, que poderiam ser exploradas por atacantes para obter acesso não autorizado. Essas vulnerabilidades incluíram falhas na validação de entrada, autenticação inadequada e proteção insuficiente contra ataques de força bruta. A hipótese de solução foi implementar um sistema de defesa em profundidade, utilizando técnicas avançadas de detecção e mitigação de ataques. Isso incluiu a implementação de firewalls de aplicação web (WAF) para proteger contra ataques de injeção e cross-site scripting (XSS), sistemas de prevenção de intrusão (IPS) para detectar e bloquear atividades maliciosas, e autenticação multifator (MFA) para fortalecer a segurança de login. A integração dessas técnicas com a plataforma existente e a garantia de que todas as vulnerabilidades fossem mitigadas foram desafios significativos. A eficácia dessa solução precisou ser rigorosamente testada antes da externalização segura da plataforma.Além disso, o aumento na utilização de CPU e memória indicou que a infraestrutura existente poderia não suportar a carga adicional a longo prazo. O desafio foi encontrar um equilíbrio entre a otimização das consultas e a eficiência das operações de escrita, sem comprometer o desempenho geral do sistema. A hipótese de solução foi implementar técnicas de particionamento de dados (sharding) e replicação para distribuir a carga de leitura e escrita entre diferentes nós do banco de dados. Essa abordagem permitiria que diferentes partes do banco de dados fossem gerenciadas independentemente, melhorando a escalabilidade e o desempenho. A utilização de caches distribuídos, como Redis ou Memcached, para armazenar resultados de consultas frequentes também foi considerada.A centralização das validações significava que todas as requisições precisavam passar por um ponto único de verificação, criando um gargalo. Além disso, a necessidade de realizar múltiplas validações sequenciais aumentou o tempo de processamento total, impactando negativamente a experiência do usuário. A hipótese de solução foi implementar técnicas de paralelização e otimização de processos de validação para reduzir a latência. A utilização de filas de mensagens e processamento assíncrono para gerenciar operações de validação em paralelo foi considerada. Isso permitiria que múltiplas validações fossem processadas simultaneamente, reduzindo o tempo de espera. A eficácia dessa solução dependeria da capacidade de orquestrar eficientemente os processos assíncronos e garantir a integridade dos dados.A transparência das decisões foi uma preocupação, pois modelos de machine learning são frequentemente vistos como ""caixas-pretas"", dificultando a explicação das decisões tomadas aos usuários e reguladores. A hipótese de solução foi implementar técnicas de compressão de modelos e otimização de inferência para reduzir o tempo de processamento. Técnicas como quantização, poda de redes neurais e utilização de hardware especializado poderiam ter melhorado a eficiência dos modelos. Utilizar frameworks de explicabilidade, como LIME ou SHAP, para garantir a transparência das decisões automatizadas também foi considerada. METODOLOGIA: Para lidar com os desafios técnicos encontrados nesta linha de pesquisa, foram organizadas as seguintes atividades:1- Experimentos voltados para avaliar o impacto das APIs e funções serverless na redução dos tempos de resposta a solicitações de acesso e na melhoria da eficiência geral do sistema, simulando cenários de alta carga.Métricas:Tempo de Resposta: Medido em milissegundos, essa métrica avalia o tempo que o sistema leva para responder às solicitações dos usuários.Latência: Mediu o atraso introduzido pelo sistema no processamento de solicitações, indicando sobrecarga de rede e processamento.Taxa de Erro: Acompanhou a porcentagem de solicitações falhadas, destacando possíveis gargalos ou problemas no sistema.2- Experimentos voltados para identificar e abordar vulnerabilidades associadas à externalização da plataforma, com foco em autenticação, proteção contra ataques DDoS e resistência a intrusões. Métricas:Número de Ataques Bem-Sucedidos/Falhados: Mediu a eficácia das implementações de segurança, acompanhando a taxa de sucesso de ataques DDoS e intrusões simulados.Integridade dos Dados: Avaliou se os dados permaneceram consistentes e inalterados durante e após os testes de segurança.Tempo de Resposta Sob Ataque: Monitorou a capacidade de resposta do sistema durante ataques simulados para avaliar o impacto na performance.3- Experimentos exploraram o impacto de bancos de dados relacionais otimizados e serviços de busca na melhoria da performance geral da plataforma, com foco especial na velocidade das consultas e no tempo de processamento das transações.Métricas:Velocidade da Consulta: Medido em milissegundos, essa métrica avaliou o tempo necessário para recuperar dados do banco de dados.Tempo de Processamento de Transações: Avaliou a eficiência na conclusão de operações dentro do banco de dados.Uso de CPU e Memória: Monitorou a utilização de recursos do sistema de banco de dados para identificar possíveis gargalos. Tempo de Escrita: Mediu o tempo necessário para escrever dados no banco de dados, especialmente relevante após a implementação de índices complexos para otimização de consultas.4- Experimentos com o objetivo de determinar se um sistema de validação centralizado poderia reduzir atritos, minimizar erros e melhorar a satisfação do cliente ao agilizar e acelerar as operações dos usuários. Métricas:Tempo de Conclusão da Operação: Mediu o tempo total que os usuários levam para concluir tarefas, avaliando o impacto do processo de validação centralizado.Taxa de Erros dos Usuários: Acompanhou o número de erros cometidos pelos usuários durante as operações, avaliando a eficácia das validações na prevenção de ações incorretas. Latência: Mediu o atraso introduzido pelo processo de validação, especialmente em cenários com múltiplas validações simultâneas.5- Experimentos investigaram a viabilidade e eficácia da implementação de um sistema de IA para análise de crédito, com o objetivo de melhorar a precisão das decisões e reduzir o risco de inadimplência. Métricas:Precisão: Mediu a porcentagem de previsões corretas feitas pelo modelo de IA na avaliação da solvência de crédito.Recall: Avaliou a capacidade do modelo de identificar corretamente os casos positivos, indicando sua eficácia em reconhecer possíveis aprovações de empréstimos.AUC-ROC: Avaliou o desempenho geral do modelo medindo sua capacidade de distinguir entre casos positivos e negativos (solventes vs. não solventes)Tempo de Processamento: Mediu o tempo necessário para o modelo de IA processar solicitações de crédito, crucial para cenários de alto volume. INFORMAÇÃO COMPLEMENTAR: Este projeto, focado na integração de tecnologias avançadas na plataforma de processamento de uma instituição financeira, representa um investimento crucial no crescimento futuro e na competitividade. Ao adotar tecnologias como APIs, funções serverless e inteligência artificial, o banco pode melhorar significativamente sua eficiência operacional, fortalecer as medidas de segurança e tomar decisões de crédito mais informadas. A implementação bem-sucedida dessas tecnologias se traduz em benefícios tangíveis, incluindo tempos mais rápidos de processamento de transações, redução do risco de fraudes e violações de dados, e maior precisão na identificação de clientes com capacidade de crédito, tudo contribuindo para uma experiência aprimorada do cliente, maior lucratividade e uma vantagem competitiva mais forte no cenário financeiro em evolução. Testes rigorosos, análises e refinamentos garantem que o esforço de modernização seja baseado em uma abordagem prática e orientada para resultados, minimizando riscos. Ao enfrentar os desafios identificados por esse processo, o banco demonstra um compromisso tanto com a inovação quanto com a excelência operacional, posicionando-se como líder na adoção e integração de tecnologias de ponta no setor financeiro.Adarbah, H. Y., Al-Badi, A. H., Toosy, S. G., Jajarmi, H., Bocar, A., Ancheta, R., & Ordoubadi, M. S. (2023). Banking on the cloud: Insights into security and smooth operations. Journal of Business, Communication & Technology, 2(2), 1-14.Ghule, S., Chikhale, R., & Parmar, K. (2014). Cloud computing in banking services. International Journal of Scientific and Research Publications, 4(6), 1-8.Mushtaq, M. F., Akram, U., Khan, I., Khan, S. N., Shahzad, A., & Ullah, A. (2017). Cloud computing environment and security challenges: A review. International Journal of Advanced Computer Science and Applications, 8(10).Nguyen, D. S., & Sondano, J. (2023). Resilience and stability in organizations employing cloud computing in the financial services industry. Journal of Computer and Communications, 11(4), 103-148. RESULTADO ECONÔMICO: As otimizações e a precisão aprimorada nas análises de crédito impulsionaram a eficiência operacional e a satisfação dos clientes, contribuindo para um avanço econômico significativo. RESULTADO INOVAÇÃO: Melhoria substancial na eficiência do sistema de processamento de transações, demonstrando o impacto positivo da integração de recursos tecnológicos avançados na otimização de processos operacionais. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 3 ID ÚNICO: 109739 NOME: MODERNIZAÇÃO DE PLATAFORMAS TECNOLÓGICAS A PARTIR DA APLICAÇÃO DE TECNOLOGIAS EM NUVEM DESCRIÇÃO: O objetivo desta linha de pesquisa é demonstrar a viabilidade técnica da modernização de fluxos e processos associados a produtos relacionados às áreas de investimentos. A maior parte dos processos estavam baseados em infraestrutura mainframe ou em estágios iniciais da modernização para a nuvem. Dada a complexidade inerente da convivência de sistemas híbridos, foram organizadas atividades experimentais buscando proposições a serem validadas em protótipos.São projetos constituintes desta linha de pesquisa:- Desenvolvimento de infraestrutura em nuvem para basear a construção de aplicativo, - Desenvolvimento de soluções acopladas para gerir e controlar plataforma voltada a segmentos específicos, - Desenvolvimento de processo de migração, remodelagem e melhoria do processo de abertura dos ETFs para nuvem, - Desenvolvimento de processo de migração de siglas com foco na otimização de desempenho de aplicações, - Desenvolvimento de esteira onshore com aplicação de recursos da nuvem.Como resultado, espera-se validar processo de modernização para nuvem viabilizando o desenvolvimento de novas funcionalidades e a otimização de jornadas. A premissa considerada é que sistemas em nuvem viabilizam condições de disponibilidades e escalabilidades até então não possíveis a partir das restrições da configuração existente. Busca-se, ainda, alcançar maiores níveis de segurança, de excelência operacional, de confiabilidade e de desempenho dos serviços ofertados aos clientes.Vale pontuar que os desenvolvimentos possibilitam o fortalecimento de competências em tecnologias em nuvem com estudos, pesquisas e experimentos voltados à aplicação e solução de problemas complexos.Como marco crítico, considerou-se como ponto de atenção a transição de um processo central e sensível para a infraestrutura em nuvem, em que qualquer falha poderia ter um impacto direto e imediato nos serviços ofertados aos clientes. A complexidade técnica da migração, combinada com a necessidade de assegurar a integridade e o desempenho dos dados durante e após a transição, torna este um momento decisivo. Se comprovada tecnicamente, a criação da infraestrutura em nuvem seria capaz de suportar operações críticas, enquanto falhas podem comprometer a confiança no projeto de modernização como um todo.Por fim, vale pontuar que os desenvolvimentos trouxeram o desafio quanto a gerenciar transições complexas e resolver problemas em tempo real. A habilidade de identificar e mitigar riscos rapidamente durante o processo de migração e construção de infraestrutura modernizada era essencial para garantir a continuidade dos negócios e minimizar interrupções. Porém, havia o risco de não haver validação técnica dos processos ameaçando a modernização no tempo necessário. Dificuldades e falhas neste ponto poderiam levar a atrasos, custos adicionais e uma reavaliação completa da estratégia de migração para a nuvem. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Modernização,  mainframe,  nuvem,  latência,  volumetria,  microsserviços. NATUREZA: Processo ELEMENTO TECNOLÓGICO: O objetivo desta linha de pesquisa é demonstrar a viabilidade técnica da modernização de fluxos e processos associados a produtos relacionados às áreas de gestão financeira e de investimentos, corretora e tesouraria. O desenvolvimento das atividades experimentais estudadas a partir de protótipos possibilitou um conjunto de aprendizados.Foi possível a compreensão de nova arquitetura não mais baseada em estruturas estáticas para processamento configurando-se como elemento novo recursos instanciados com microprocessamentos. Estudos levaram ao entendimento de estratégia de correlação de informações geradas em diferentes camadas, sendo que testes integrados comprovaram tecnicamente as peças componentes desta solução.Experimentações feitas permitiram comprovações para construção de novos mecanismos no processo ETL para aplicação de arquitetura no padrão hexagonal. Também foi possível entender a aplicação de arquitetura de microsserviços, hospedados no PaaS, configurando-se como elemento novo a construção para consumir dados com diferentes origens.Houve comprovação técnica da criação de novos códigos em nuvem de forma a viabilizar melhor a segregação de rede com VPC endpoint. Estudos para aplicação de ferramentas de governança Turbot e Cloudcustodian permitiram compreender armazenamento, reutilização de aplicativos e implantação de arquiteturas sem servidores. Foram feitas experimentações com funções lambdas possibilitando a construção de proposições a serem estudadas.Estudos a partir de protótipos permitiram entender a estruturação dos mecanismos de segurança, de software e de infraestrutura, incluindo nestes pipelines de gates de validação de segurança, risco e vulnerabilidade. Configurou-se como elemento novo o entendimento da possibilidade de acréscimo de camada adicional de segurança, não utilizada até então, baseada em tokens dinâmicos.Foi possível a compreensão de processo para dimensionamento dos fluxos de entradas e saídas de requisições permitindo viabilizar entendimento de comportamento dos clientes e de parceiros. Estudos e experimentos possibilitaram comprovações técnicas para criação de integração configurando-se como elemento novo a arquitetura baseada em eventos e streaming realizados através de tópicos Kafka. Foi possível compreender a abstração do log como fluxo de mensagens resultando em processamento de baixa latência e fácil suporte para diferentes fontes e consumos de dados distribuídos.Por fim, a partir dos protótipos estudados, foi possível entender mecanismo de autenticação e de convivência entre sistemas legados e em nuvem garantindo fluxo e concatenação de dados, bem como o alcance de latências de microssegundos e determinísticas. DESAFIO TECNOLÓGICO: A criação de um processo de integração e migração entre fluxos baseados em mainframe e aqueles em estruturas de nuvem apresenta riscos como incompatibilidade de tecnologias, problemas de comunicação e latência. Devido às especificidades do mainframe e das áreas de negócio envolvidas, não existem soluções prontas para serem replicadas. Um desafio adicional é a impossibilidade de construir e testar a solução em ambiente produtivo, pois isso poderia causar indisponibilidade nos sistemas e afetar o Banco como um todo. Portanto, estudos e experimentos foram organizados para encontrar elementos que componham protótipos a serem validados experimentalmente.Replicou-se, como experimento, condições de crescimento orgânico e aumento de eventos extraordinários para as plataformas. Com isso, foi possível tratar problemas de represamento de dados com erros impedindo transações. Não era conhecida estratégia para tratar tal barreira sendo levadas a experimento diferentes hipóteses. Lidou-se com desafios de lentidão do sistema na implementação do ambiente em nuvem. Buscava-se compreender problemas relacionados à volumetria de dados com a hipótese de que haveria limites máximos. Testes de estresse foram propostos buscando entendimentos para proposição de nova arquitetura para diminuir latência, incluindo tráfego de dados e seus gargalos de tempo da comunicação.Hipóteses foram tratadas buscando composição de solução auto escalável para comprovação técnica de implementação de recursos em microsserviços. Estudos e experimentos permitiram a construção e experimentação de cenários via POCs,  foi considerada a tecnologia Athena procurando tratar as incertezas sobre limitações de escalabilidade e disponibilidade quando o Quicksight importava os dados direto no Bucket S3.Experimentos foram organizados para lidar com incertezas relacionadas a não existir processo de disponibilização de dados para áreas internas com riscos à integração entre dados da plataforma legada e a nova solução. Delineamentos experimentais foram propostos buscando estratégia de integração. Estudos também se voltaram a buscar elementos para composição para lidar com quantidade massiva de dados. As propostas foram consideradas em PoCs buscando validação técnica de fluxo de dados e eficiência de seu processamento.Hipóteses foram delineadas considerando utilização de dois fatores para entendimento de composição para mandates de segurança. Experimentos se voltaram a entender cenários com duas camadas de proteção para a troca de tokens de segurança das requisições. Buscava-se comprovação técnica de hash de sessão aberto em ambiente legado como forma de garantir que a requisição partiu de solicitação de cliente.Havia que se lidar com incertezas sobre o streaming de dados das plataformas existentes. Tratou-se a hipótese de que o uso do Apache Kafka poderia garantir a integração necessária, entendendo-se que se mostrava como alternativa mais viável para garantir que essas integrações fossem assíncronas e em tempo real. Buscava-se comprovação da substituição de integração com processos batch. METODOLOGIA: A fim de avaliar a viabilidade técnica da modernização de fluxos e processos, esta linha de pesquisa foi organizada para buscar entendimentos de novas arquiteturas, abordagens de microsserviços, e mecanismos de integração para serem validados via protótipos. Para lidar com os desafios técnicos encontrados, foram organizadas as seguintes atividades:1- Estudos e testes para desenvolvimento de protótipos de arquitetura de microsserviços em nuvem.Métricas:- Tempo de resposta das transações- Facilidade de integração com sistemas legados- Escalabilidade e elasticidade do sistema- Resiliência e recuperação,  tempo médio de recuperação2- Estudos e experimentos para validar a migração de processos de ETL para arquitetura hexagonal.Métricas:- Redução no tempo de processamento de dados- Eficiência no consumo de recursos- Facilidade de manutenção e modularidade da arquitetura3- Estudos para implementação de soluções de segurança baseadas em tokens dinâmicos e validação automatizada em pipelines de segurança.Métricas:- Taxa de detecção de vulnerabilidades- Tempo de validação4- Experimentos para criação de novos mecanismos de governança e segregação de rede com VPC Endpoint.Métricas:- Redução de falhas de segurança- Tempo de resposta das requisições em ambientes isolados- Facilidade de gestão e controle de acesso5- Testes de estresse e validação da arquitetura de streaming de dados utilizando Apache Kafka.Métricas:- Latência das transações em tempo real- Taxa de sucesso no processamento de eventos distribuídos6- Estudos para desenvolvimento de soluções auto escaláveis em microsserviços, validando o desempenho em cenários de alta demanda.Métricas:- Eficiência do auto escalonamento- Consumo de recursos durante picos de utilização- Tempo de resposta em cenários críticos de carga7- Testes de carga para avaliar o desempenho e latência na comunicação entre sistemas legados e a nuvem.Métricas:- Latência média das transações entre os ambientes- Taxa de sucesso nas integrações- Impacto da comunicação híbrida no desempenho geral do sistema INFORMAÇÃO COMPLEMENTAR: Estruturas centenárias do setor bancário-financeiro ainda enfrentam desafios ao migrar a atual estrutura para bases em nuvem. Entre as principais dificuldades estão questões de segurança e privacidade, já que a migração para a nuvem introduz novas vulnerabilidades e exige medidas robustas para prevenir violações de dados e cumprir com regulamentações. A convivência com sistemas legados configura-se com uma barreira técnica, pois muitos bancos dependem de sistemas antigos que podem não ser compatíveis com ambientes de nuvem modernos,  com isso, há riscos de perda de dados, corrupção e problemas de integração. Como mitigação destes riscos, há que se construir uma governança com foco na manutenção da integridade, da disponibilidade, usabilidade e segurança dos dados durante e após a migração, exigindo estratégias e processos robustos. A gestão de custos é outro desafio, uma vez que a migração inicial pode ser cara e, sem planejamento adequado, os custos contínuos podem aumentar rapidamente. Além disso, a falta de habilidades especializadas entre as equipes em infraestrutura tanto de mainframe quanto de nuvem aumenta a complexidade dos desenvolvimentos.Apesar destes desafios, a migração das estruturas bancárias para a nuvem é inevitável. A nuvem oferece agilidade e escalabilidade, permitindo que os bancos respondam rapidamente às demandas do mercado e às expectativas dos clientes, além de facilitar a implantação de novos produtos e serviços. A adoção de tecnologias avançadas é essencial para evolução de competências como análise de dados e aplicação de novos desenvolvimentos com inteligência artificial. Viabiliza, por isso, insights baseados em dados, experiências personalizadas para os clientes e uma gestão de riscos mais eficiente. A segurança e resiliência aprimoradas também são melhoradas, com ambientes de nuvem facilitando a recuperação de desastres e a continuidade dos negócios. Por fim, a nuvem melhora a experiência do cliente, permitindo interações mais amigáveis e acesso 24/7 aos serviços bancários a partir da digitalização de jornadas.Referências bibliográficasÇ- Megargel, A., Shankararaman, V., & Walker, D. K. (2020). Migrating from monoliths to cloud-based microservices: A banking industry example. Software engineering in the era of cloud computing, 85-108.- Merizzi, N., Dhal, P., Srinivas, V., & Hazuria, S. (2022). Accelerating digital transformation in banking and capital markets with industry clouds. Deloitte Insights.- Singh, M., Tanwar, K. S., & Srivastava, V. M. (2018). Cloud computing adoption challenges in the banking industry. In 2018 international conference on advances in big data, computing and data communication systems (icABCD) (pp. 1-5). IEEE. RESULTADO ECONÔMICO: Redução de custos associados a menor utilização da plataforma legada. Aumento de receita associada à disponibilização de novas funcionalidades dos produtos. RESULTADO INOVAÇÃO: Modernização de plataformas melhorando requisitos de latência, disponibilidade, escalabilidade e segurança. Desenvolvimento de novas funcionalidades ao usuário melhorando a experiência do cliente. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 4 ID ÚNICO: 109740 NOME: NOVAS SOLUÇÕES TECNOLÓGICAS PARA PLATAFORMAS COM DESENVOLVIMENTO DE FUNCIONALIDADES E MELHORIAS PARA O CLIENTE DESCRIÇÃO: O objetivo desta linha de pesquisa é demonstrar a viabilidade técnica da aplicação de novas tecnologias a processos de serviços financeiros de forma a aprimorar o seu desempenho, escalabilidade, capacidade de resposta, e de integrar novas funcionalidades que atendam às necessidades dos clientes e a requisitos regulatórios.A aplicação de novas tecnologias no setor bancário-financeiro é um desafio significativo devido à complexidade associada à variedade na maturidade tecnológica dos sistemas existentes – que frequentemente operam em infraestruturas legadas, como mainframes. A incorporação de tecnologias modernas (como arquiteturas baseadas em microsserviços, event-driven, e soluções envolvendo algoritmos aplicados a novo contexto, entre outros) requer um entendimento profundo das interdependências entre sistemas e processos existentes. Trata-se de situação não trivial considerando uma estrutura bancária-financeira centenária que foi acumulando processos e sistemas em diferentes estágios tecnológicos. O desenvolvimento é particularmente crítico uma vez que a migração e modernização de sistemas não podem comprometer a continuidade dos serviços, a integridade dos dados, ou a experiência do cliente. Por isso, atividades experimentais são essenciais para explorar diferentes composições de soluções tecnológicas, identificar potenciais riscos e limitações, e validar a viabilidade técnica e operacional das soluções através de protótipos.Dentre as áreas impactadas pelas soluções experimentadas nesta linha de pesquisa estão questões de desempenho do processamento de plataformas atendendo clientes,  de consolidação de fluxos de dados descentralizados,  de volumetria associada à integração de fluxos oriundos de diferentes canais lidando com questões de volumetria,  de novos desenvolvimentos de APIs para evoluir serviços e sistemas. Apesar de serem questões comuns a desenvolvimentos tecnológicos de software, as soluções não estão pré-definidas e precisam ser construídas. Os aprendizados decorrentes desta linha de pesquisa poderão ser extrapolados a outros projetos da empresa.Um marco crítico desta linha de pesquisa foi a validação da arquitetura baseada em microsserviços com cache compartilhado e processamento escalável em um ambiente de produção controlado. Era incerto ser possível resolver as limitações técnicas associadas à volumetria de informações e à necessidade de escalabilidade da plataforma. Esta validação envolvia testes rigorosos de desempenho, segurança e conformidade regulatória para garantir que a nova arquitetura suportaria a carga de trabalho esperada sem comprometer a integridade dos dados, a continuidade dos serviços ou a experiência do cliente. Se esta validação não fosse bem-sucedida, a migração e modernização dos sistemas críticos poderiam falhar, colocando em risco todo o projeto.Outro marco crítico envolveu os desenvolvimentos de integração relacionados a dados. Era preciso garantir que os dados fossem precisos, consistentes e disponíveis em tempo real para suportar as novas funcionalidades e capacidades da plataforma. Falhar nesta integração poderia resultar em inconsistências de dados e comprometimento da segurança da informação. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Digitalização,  Modernização,  Tecnologias em nuvem,  Nuvem pública. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Diante de restrições e dificuldades técnicas das soluções existentes, esta linha de pesquisa partiu da premissa de que a modernização de sistemas e processos permitiria superar as limitações e, ao mesmo tempo, melhorar o desempenho estas soluções. Assim, com o objetivo de demonstrar a viabilidade técnica da aplicação de novas tecnologias a processos de serviços financeiros, foram organizados estudos e experimentos que resultaram em um conjunto de aprendizados.Foi possível compreender a composição necessária para criar uma arquitetura com infraestrutura automatizada e gerenciada via código. As experimentações permitiram a análise de aplicações baseadas em microsserviços e containers, utilizando ECS com Fargate. A partir da premissa de que os dados são controlados por um sistema separado e utilizados nos processos de captura, consolidação e gerenciamento, identificou-se a viabilidade de adotar uma solução de cache baseada em Redis, que busca dados uma vez ao dia e os disponibiliza para as aplicações da plataforma. Estudos e experimentos permitiram entender composição para implementação de cluster de cache validada para reduzir o tempo de consulta de dados, permitindo seu reaproveitamento em memória sem a necessidade de integração com um banco de dados físico, e compartilhando esses dados com os demais microsserviços orquestrados. Houve comprovação técnica de que a tecnologia de cache é uma estratégia eficiente para integração e comunicação entre plataformas modernizadas e seus sistemas legados.Houve aprendizados relacionados a processo de migração de dados e componentes entre as diferentes plataformas envolvidas nos sistemas-alvo. Houve comprovação técnica de processo para alcançar uma elevação dos níveis de desempenho de processamento e redução de morosidades e reprocessamento que consumiam recursos da infraestrutura. Os experimentos permitiram comprovação técnica da quebra em lambdas de parte das aplicações que estavam em ECS. Uma vez que as informações para processamento partiam de sistemas legados, foram organizados experimentos para entender como construir um fluxo automatizado que conectasse os sistemas com diferentes maturidades tecnológicas. Foi possível compreender, via estudos e experimentos, a aplicação do serviço storage gateway trazendo os arquivos para um bucket S3 em que o processo de geração de eventos passa a ser iniciado automaticamente através de filas SQS. Aprendizados permitiram entender construções de mensageria Kafka para acessar bases de informação para fins de enriquecimento de informações.Diante de problemas de latência de consultas em banco de dados, aprendizados permitiram construção para a substituição da aplicação do Step Functions e EventBridge por uma aplicação .NET em contêiner EKS. A premissa testada era que seria possível, dessa forma, reorganizar as filas usadas para ordenação das precedências com SQS. Para o desenvolvimento de algoritmos, aprendizados viabilizaram desenvolvimentos em SageMaker. Houve comprovação técnica de composição de algoritmos para volumes de dados extremamente altos em um ambiente distribuído. Para soluções envolvendo dados e analytics, houve comprovação técnica de composições de automações nos processos de disponibilização de dados. Para ambos os casos, algoritmos e automações serão adaptados às diferentes plataformas. DESAFIO TECNOLÓGICO: Os desenvolvimentos desta linha de pesquisa enfrentaram incertezas técnicas,  para superá-las, foram organizadas atividades experimentais, considerando diferentes proposições de solução, para serem avaliadas e validadas em protótipos.Levou-se a experimento a hipótese da aplicação de lambda para executar as funções necessárias para o sistema uma vez que em teoria poderia prover maior facilidade de utilização de um serviço serverless garantindo configuração de escalabilidade e zonas de disponibilidade. Havia riscos dado o entendimento de que lambdas possuem limitações de tempo de processamento e terem o papel de apenas uma única função. Havia que se tratar a hipótese de uma abordagem considerando os domínios com a proposta de entender se uma arquitetura de microsserviços utilizando ECS Fargate poderia ser uma solução com o benefício de uma aplicação serverless mas sem a limitação de tempo de processamento, o que poderia levar a agrupar as diversas funcionalidades de um domínio na mesma aplicação. Além disso, foram investigadas as capacidades de integração do Lambda com outros serviços AWS, como API Gateway para roteamento de requisições, e CloudWatch para monitoramento e logging, visando garantir uma operação eficiente e observável.Questões de volumetria de dados processados levaram a testes considerando o padrão de arquitetura model-view-controller,  também foi considerada para experimentação o desenvolvimento em padrão hexagonal. A arquitetura hexagonal foi considerada e avaliada por sua capacidade de permitir a substituição de componentes externos, como fontes de dados e interfaces de usuário, sem impactar a lógica central da aplicação, o que é essencial para manter a modularidade e facilitar a manutenção em ambientes dinâmicos.Foram propostas pesquisas e experimentações para testar a viabilidade de uso do Kafka com riscos já que algumas informações poderiam ultrapassar limites de tamanho para transporte. Para contornar esse risco, foram exploradas técnicas de compressão de dados e a segmentação de mensagens em partes menores. Foram realizados testes de desempenho para avaliar a latência e o throughput do Kafka, garantindo que ele pudesse suportar a carga de trabalho esperada sem comprometer a integridade e a entrega dos dados.Havia incertezas sobre desenvolvimento de algoritmo associadas à preparação dos dados que é um dos principais desafios na implementação do SageMaker. Era incerto se os dados poderiam ser estruturados para o processamento necessário,  experimentos se voltaram a entender qual algoritmo melhor se adequa ao problema alvo. Para endereçar esta incerteza, foram organizadas experimentações com diferentes algoritmos desenvolvidos. Esses experimentos incluíram o uso de técnicas de pré-processamento de dados, como normalização e feature engineering, para melhorar a qualidade dos dados de entrada. Também foram exploradas diferentes arquiteturas aplicadas a algoritmos para determinar qual abordagem ofereceria os melhores resultados para o problema específico. METODOLOGIA: Esta linha de pesquisa organizou a seguinte metodologia:Experimento 1- Hipótese: Utilizar funções Lambda para operações do sistema- Riscos: Restrições de tempo e limitações associadas às funções Lambda- Mitigação: Investigar as capacidades de integração da Lambda com outros serviços, como o API Gateway, para roteamento de solicitaçõesMétricas:- Medir o tempo de execução gasto pelas funções Lambda- Acompanhar o número de erros durante a execução das funções LambdaExperimento 2- Hipótese: Implementar arquitetura de microserviços usando ECS Fargate- Justificativa: Agrupar funcionalidades de domínio dentro de uma única aplicação pode potencialmente abordar as limitações de tempo de processamento das funções LambdaMétricas:- Latência para cada microserviço responder às solicitações- Acompanhar o tempo necessário para implantar e escalar microserviçosExperimento 3- Desafio: Gerenciar grandes volumes de dados- Hipóteses: Testar e comparar as arquiteturas MVC e Hexagonal- Justificativa: Capacidade de permitir a substituição de componentes externos sem afetar a lógica da aplicaçãoMétricas:- Comparar o tempo necessário para processar um volume específico de dados usando as arquiteturasExperimento 4- Riscos: Limitações potenciais no tamanho dos dados para transporte- Mitigação: Explorar técnicas de compressão de dados e segmentação de mensagens. Realizar testes de desempenho para avaliar a latência e a taxa de transferência do KafkaMétricas:- Taxa de transferência de mensagens processadas pelo Kafka- Latência no envio de mensagens do produtor para o consumidor- Taxa de perda de dados durante a transmissãoExperimento 5- Hipótese: Implementar uma solução de cache baseada em Redis para gerenciamento eficiente de dados- Justificativa: Os dados são controlados por um sistema separado e utilizados nos processos de captura, consolidação e gerenciamento- Validação: Explorar e validar a tecnologia de cache como uma estratégia para integração e comunicação entre plataformas modernizadas e legadasMétricas:- Medir a porcentagem de solicitações de dados atendidas a partir do cache- Tempo de resposta de consulta antes e depois da implementação do cacheExperimento 6- Desafio: Migrar dados e componentes entre plataformas com diferentes maturidades tecnológicas- Hipóteses: Dividir aplicações em ECS em funções Lambda- Justificativa: Explorar e entender a aplicação do serviço Storage Gateway para transferir arquivos para bucket S3. Iniciar automaticamente processo de geração de eventos através de filas SQS. Entender a construção de mensagens Kafka para acessar bases de informações e enriquecer dadosMétricas:- Tempo de migração de dados entre plataformas- Integridade dos dados após a migraçãoExperimento 7- Desafio: Alta latência em consultas ao banco de dados- Hipótese: Substituir Step Functions e EventBridge por uma aplicação .NET em um contêiner EKS para melhorar a latência das consultas- Justificativa: Reorganizar filas usadas para ordenação de precedência com SQSMétricas:- Tempo de resposta da consulta após a implementação da aplicação .NET e reorganização das filas SQS- Monitorar o comprimento das filas SQSExperimento 8- Desafio: Desenvolver algoritmos para volumes altos de dados em um ambiente distribuído, com a preparação dos dados como um desafio na implementação do SageMaker- Hipóteses: Realizar experimentos para determinar o algoritmo mais adequado- Metodologia: Experimentar com diferentes algoritmos de pré-processamento para melhorar a qualidade dos dados de entradaMétricas:- Precisão de diferentes algoritmos usando métricas relevantes (como precisão, recall, F1-score) INFORMAÇÃO COMPLEMENTAR: Os bancos enfrentam desafios significativos ao abordar a modernização de seus sistemas legados. Esses sistemas desatualizados, muitas vezes baseados em arquiteturas inflexíveis de mainframes, têm dificuldades em se adaptar às demandas do cenário bancário digital atual. Eles carecem da agilidade e flexibilidade necessárias para inovações rápidas e enfrentam problemas para integrar-se às soluções modernas baseadas em nuvem, que se tornam cada vez mais essenciais para oferecer serviços bancários competitivos. Além disso, a manutenção desses sistemas envelhecidos está se tornando cada vez mais cara e complexa devido à diminuição do número de profissionais qualificados com a expertise necessária para gerenciá-los.No entanto, a modernização desses sistemas é crucial para o sucesso dos bancos na indústria de serviços financeiros em evolução. Ela permite que os bancos melhorem a eficiência operacional ao agilizar processos, automatizar tarefas e reduzir a dependência de intervenções manuais e tecnologias obsoletas. Mais importante ainda, a modernização abre caminho para que os bancos aumentem sua competitividade digital, oferecendo recursos inovadores, serviços personalizados e experiências de clientes sem fricções que os clientes exigentes de hoje esperam. A falha em modernizar pode levar à diminuição da satisfação dos clientes, perda de participação no mercado para concorrentes mais ágeis e dificuldade em acompanhar mudanças regulatórias e ameaças de segurança em evolução.Nesse contexto, a elaboração e execução de protótipo emerge como um passo crucial no processo de modernização. Ele permite que os bancos testem diferentes abordagens e tecnologias de modernização em um ambiente controlado, reduzindo o risco associado a mudanças em larga escala em sistemas bancários críticos. Através de protótipos, os bancos podem experimentar várias plataformas de nuvem, arquiteturas de microsserviços e estratégias de migração de dados, validando sua viabilidade e avaliando seu impacto em sistemas e processos existentes antes de se comprometer com uma implementação em larga escala. Essa abordagem iterativa e experimental ajuda a identificar problemas potenciais precocemente, facilitando ajustes e refinamentos que minimizam a interrupção das operações bancárias principais. Ao validar suas suposições e otimizar suas estratégias através de protótipo, os bancos podem aumentar significativamente a probabilidade de uma iniciativa de modernização ser bem-sucedida.Referências bibliográficas:- Lekkala, C. (2021). Modernizing Legacy Data Infrastructure for Financial Services. International Journal of Science and Research (IJSR), 10(1).- Megargel, A., Shankararaman, V., & Walker, D. K. (2020). Migrating from monoliths to cloud-based microservices: A banking industry example. Software engineering in the era of cloud computing, 85-108. RESULTADO ECONÔMICO: Espera-se incremento de receita relacionada ao aumento da oferta de serviços,  e incremento da receita devido ao aumento da contratação devido a melhorias relacionadas à jornada do cliente. RESULTADO INOVAÇÃO: Criação de soluções para lidar com altas volumetrias. Aplicação de novas tecnologias que pode se refletir na modernização de diferentes sistemas e processos. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 5 ID ÚNICO: 109741 NOME: DESENVOLVIMENTO DE SOLUÇÕES DE TESTES PARA MONITORAMENTO ATIVO COMO ESTRATÉGIA PARA MINIMIZAR A INDISPONIBILIDADE DE SISTEMAS DESCRIÇÃO: Apesar da evolução tecnológica alcançada por estruturas de software, a indisponibilidade destes sistemas ainda é um problema inclusive em corporações com recursos e especialização técnica diferenciados. Em um ambiente bancário, este problema é crítico uma vez que envolve transações que devem ser processadas em tempo real.Há uma série de fatores que podem resultar nesta indisponibilidade. Sistemas bancários de software integram uma variedade de serviços e funcionalidades configurando estruturas complexas que aumentam os riscos de erros e falhas,  esse potencial é aumentado pelo fato de ainda haver parte dos sistemas em plataforma legada mainframe – que são únicos e rígidos. A necessidade de atualização, de manutenção e de escalabilidade, ainda que inerentes a sistemas de software, aumentam os riscos de indisponibilidade quando considerada a estrutura complexa de grandes organizações.Resolver este problema passa por um conjunto de ações. Esta linha de pesquisa se dedica à concepção e desenvolvimento relacionados a testes de forma a criar processo para detecção e falhas nas jornadas de clientes. Foi possível entender a aplicação de conceitos de monitoração sintética à automação de testes,  além de se configurar como uma abordagem ativa, foi possível alcançar um desenvolvimento compatível com diferentes programas e clientes (a partir de APIs) e integrável a outros testes existentes na corporação. Atividades experimentais também se voltaram a entender como lidar com ambientes em diferentes maturidades tecnológicas e garantir uma automação de testes que permitisse um monitoramento único, sem a necessidade de ser duplicado. Foram organizados estudos que considerassem uma nova orquestração de execuções com entendimentos para construção uma mesma pipeline com retornos apartados entre si permitindo o monitoramento necessário.Os desenvolvimentos dos processos de automação de testes lidaram com a complexidade existente associada a cenários, tecnologias e ambientes distintos. Em um contexto bancário, com milhões de usuários, funcionalidades e transações, havia riscos de não ser possível uma solução que abrangesse a diversidade e criticidade de situações. Ainda que exista no mercado uma gama de ferramentas para testes e monitoração ativa, não havia soluções que apresentassem o desempenho necessário a esse contexto ainda que ajustadas e personalizadas. Entre as tecnologias consideradas, destacam-se Dynatrace, New Relic Synthetics, Pingdom e Catchpoint.Estudos considerando o estado da arte científico indicaram que a aplicação de monitoração sintética em automação de testes para detectar falhas em tempo real é incipiente e não tem sido atualizada. Estudos existentes geralmente se concentram em ambientes de desenvolvimento homogêneos ou modernos, com lacunas considerando soluções que abordem a integração de testes automatizados em infraestruturas mistas (com coexistência entre sistemas legados e novos serviços em nuvem ou arquiteturas distribuídas). Também há poucas referências sobre como criar pipeline que integre diferentes tipos de testes (unitários, integrados, funcionais e não funcionais).Como resultado, a execução desta linha de pesquisa trouxe, como novidade, uma abordagem ativa e integrada de testes para monitoração que minimizaram situações de indisponibilidade.Caso o desenvolvimento não fosse compatível com os outros testes relacionados a desenvolvimentos específicos (vale lembrar que testes unitários, integrados, funcionais e não funcionais são aplicados a cada desenvolvimento feito na empresa), a automação não seria viável. Caso não fosse possível uma solução única para testes em ambientes tecnológicos distintos, havia riscos de aumentar as situações de indisponibilidade. Tais fatos impediriam a conclusão da linha de pesquisa configurando-se, portanto, como marco crítico. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Monitoração sintética,  modularização,  testes automatizados,  automação. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Na solução existente, a execução de testes seguia estratégia de monitoração passiva com realização de testes para monitoramento a partir da experiência real do usuário em cenários autênticos. Considerando sistemas bancários, era possível ter uma análise detalhada de cada transação e sua interação com o cliente. No entanto, isso não se refletia em sistemas mais disponíveis. Como esse tipo de monitoramento depende da análise de eventos que já ocorreram, a detecção de problemas acontece de forma tardia,  além disso, sua cobertura não é abrangente a ponto de considerar a variedade de componentes interconectados dos sistemas bancários. Tais fatores indicaram que a abordagem passiva para testes de monitoramento era insuficiente para evitar a indisponibilidade dos sistemas. Com isso, era necessário entender novas estratégias para automação de testes com estudos voltados a conceitos de monitoração ativa.As principais abordagens desta monitoração – discutidas por Shahin, Babar e Zhu (2017) – forneceram insights em termos de contribuições teóricas, mas careciam de casos e aplicações práticas. Houve comprovação da aplicação de conceitos de monitoração sintética (que monitora a partir de simulações) para a automação de testes almejada. Diferente de outras abordagens, os testes podem ser totalmente automatizados proporcionando uma monitoração contínua do sistema de forma a identificar problemas assim que eles ocorrem,  a monitoração sintética também conseguiu abranger uma ampla gama de funcionalidades e tecnologias que coexistem e precisam ser testados de maneira integrada.Para isso, era necessário entender como emular as interações do usuário e dos fluxos de transação considerando a complexidade das inúmeras variáveis, cenários, produtos e serviços. Levou-se a teste a premissa de que a abordagem da linguagem de programação a ser utilizada na plataforma poderia simplificar o cenário alvo. Foi possível comprovar a aplicação de DSLs (domain-specific language) para desenvolvimento destes cenários. Uma Domain-Specific Language (DSL) é uma linguagem de programação utilizada para um domínio particular de aplicação. Nesse caso, sua aplicação permitiu abstrair a complexidade citada ao mesmo tempo em que permitiu que os especialistas do domínio, que possuem conhecimento detalhado das operações bancárias, definissem cenários de teste sem necessidade de compreender todas as nuances técnicas de cada sistema. Diferente de outras abordagens, como Model-Based Testing (MBT), a DLS permitiu a flexibilidade para permitir a personalização dos testes para cenários específicos sem perder a generalidade necessária para ser aplicada a diferentes sistemas.Com a abordagem de automação de testes definida, estudos foram direcionados para integrar à esteira de desenvolvimento os diversos tipos de testes — unitários, integrados, funcionais e não funcionais — necessários no design, desenvolvimento e implantação dos produtos de software da empresa. A problemática de integrar diversos tipos de testes em uma esteira de desenvolvimento contínuo é amplamente abordada na literatura especializada em engenharia de software e práticas DevOps – Erich, Amrit e Daneva (2017),  Rahman, Helms, Williams, Parnin (2015),  Lwakatare, Kuvaja & Oivo (2016). Ainda assim, há lacunas no estado da arte sobre abordagens e ferramentas que facilitem a integração e execução automatizada desses diferentes tipos de testes em um fluxo contínuo,  consolidá-los em uma esteira maior que ofereça uma visão abrangente e coerente de todas as atividades de teste é dificultada pela falta de frameworks e metodologias que abordem a heterogeneidade dos ambientes de desenvolvimento. Dentre as hipóteses consideradas, houve comprovação de nova composição de pipeline, com abordagem modularizada, de forma a criar processo para o provisionamento de ambientes e testes dinâmicos e apartados. DESAFIO TECNOLÓGICO: Como hipótese de solução, buscou-se entendimentos em monitoração ativa de forma a ser possível testar uma composição para a automação de testes que diminuísse situações de indisponibilidades. Kim, G., Behr, K., Spafford, G. (2013) ofereceram insights sobre a importância de combinar automação de testes com monitoramento ativo como forma de resolver problemas de disponibilidade de sistemas. Adkins, H., Beyer, B., Blankinship, P., Lewandowski, P., Oprea, A., Stubblefield, A. (2020) reforçavam que o monitoramento passivo não seria suficiente para garantir a disponibilidade contínua dos sistemas sugerindo a adoção de práticas de monitoramento ativo e automação.Considerando o monitoramento ativo, entre as principais técnicas estava a análise de logs em tempo real,  porém, havia limites quanto a analisar grandes volumes de dados relacionados à alta demanda por capacidade de processamento e armazenamento. Outra hipótese considerada era a monitoração baseada em transações a partir da criação de scripts automatizados que simulam diferentes transações típicas do usuário ou do sistema. Esses scripts seriam executados periodicamente para verificar se as transações estão sendo processadas conforme o esperado, detectando falhas ou problemas de desempenho. A literatura especializada aponta limitações da abordagem, em especial a dificuldade de cobrir todos os caminhos de uso ou funcionalidades, deixando áreas críticas sem monitoramento efetivo.A criação e desenvolvimento de estrutura de monitoração ativa enfrentou barreiras associadas à insuficiência do conhecimento em fornecer soluções que equilibrem precisão, cobertura e desempenho dos testes, sem sobrecarregar os recursos do sistema – conforme Shahin, Babar e Zhu (2017). Era incerto ser possível superar a contradição técnica de minimizar a disponibilidade do sistema a partir de uma ferramenta que pode reforçar tais falhas.Outra barreira diz respeito à simulação de sistemas complexos e dinâmicos que possa abranger todas as variáveis, cenários, produtos e serviços envolvidos em sistemas bancários. Tal barreira é reforçada por existir uma lacuna no conhecimento técnico-científico sobre como desenvolver e manter testes sintéticos altamente personalizados e flexíveis que possam automaticamente ajustar-se às mudanças nos sistemas que monitoram.Delineamentos experimentais se voltaram a entender a viabilidade da monitoração sintética lidando com as barreiras da emulação do sistema. Como hipótese de solução, testou-se a aplicação de DSLs (Domain-Specific Languages) buscando comprovação de flexibilidade nos testes. A premissa considerada era que criar linguagens específicas de domínio que permitam aos desenvolvedores especificarem comportamentos de testes de forma mais natural e intuitiva poderia simplificar a criação e manutenção de testes sintéticos R. Gupta, S. Kranz, N. Regnat, B. Rumpe and A. Wortmann. (2021) fornecem um mapeamento sistemático do uso de DSLs em testes automatizados de forma a simplificar a definição de testes sintéticos especialmente considerando sistemas complexos.A integração almejada de todas as esteiras de teste em um fluxo contínuo de monitoramento lidou com incertezas sobre como harmonizar ferramentas e processos de teste heterogêneos em uma esteira de desenvolvimento integrada,  a literatura especializada vem abordando a falta de frameworks ou metodologias que possam abordar tal questão.Estudos mostravam a necessidade de mais pesquisas sobre a interoperabilidade entre as diferentes ferramentas de monitoração ativa (Forsgren, N.,  Humble, J.,  Kim, G. (2018)). Experimentos apontavam para dificuldades na configuração e manutenção das integrações entre as ferramentas de monitoração e pipelines de outros testes. Estudos considerando o estado da arte trataram da adaptação de ferramentas de monitoração para diferentes ambientes e necessidades específicas de negócios, mas indicaram a necessidade de suporte melhor para análise preditiva e detecção de anomalias. METODOLOGIA: Para superar os desafios relacionados a novos processos de automação de testes visando soluções para cenários de indisponibilidade, foram organizadas as seguintes atividades:1- Estudos teórico e prático sobre metodologias de monitoração ativa buscando comprovações técnicas para configuração de testes- Percentual de metodologias aplicadas com sucesso em testes práticos.2- Estudos sobre monitoração sintética para entendimento de aplicações buscando referências para composição de solução- Quantidade de aplicações de monitoração sintética analisadas.- Número de referências e casos de uso documentados.3- Delineamentos experimentais para guiar emulação de cenários em ambiente de testes de forma a entender viabilidade da abordagem de monitoração sintética- Percentual de cenários que replicaram com precisão os ambientes de produção.4- Elaboração e execução de PoCs para realização de testes com usuários reais de forma a comparar achados do sistema emulado- Percentual de PoCs que resultaram em insights aplicáveis para a melhoria do sistema.5- Delineamentos experimentais para mapear os sistemas a coexistirem, suas características e principais problemas que possam estar prejudicando a comunicação entre eles e gerando indisponibilidade- Número de sistemas mapeados e documentados.- Número de problemas de comunicação identificados e resolvidos.6- Experimentos considerando modularização levando a teste o provisionamento de ambientes de testes dinâmicos apartados, - Tempo médio de provisionamento dos ambientes de teste.7- Experimentos buscando proposição de execução de testes regressivos de forma apartada de testes integrados, - Percentual de falhas detectadas nos testes regressivos antes da integração.8- Estudos buscando elementos e referências para subsidiar nova proposição de orquestração automática do provisionamento de múltiplos ambientes de teste para múltiplos artefatos associados a uma única pipeline.- Número de elementos e referências identificados.- Número de proposições de orquestração automática validadas. INFORMAÇÃO COMPLEMENTAR: A concepção de pipeline modular foi uma abordagem diferenciada para a execução de testes. Apesar de inicialmente parecer óbvia, ao separar esteiras entre mainframe e nuvem, o envolvimento da execução de testes foi um complicador. Esta abordagem permitiu que diferentes módulos de teste operassem de forma independente, mas coordenada. Era incerto ser possível dividir o processo de teste em segmentos distintos, cada um focado em aspectos específicos da funcionalidade do sistema. Havia que se comprovar se esta maior flexibilidade na gestão dos testes, possibilitando ajustes dinâmicos e a reutilização de módulos em diferentes cenários de teste, seria possível sem a necessidade de reconfiguração completa da pipeline.Este desenvolvimento buscava, na modularização, comprovações da execução de múltiplas cadeias de testes simultaneamente reduzindo o tempo total necessário para testes abrangentes. Além disso, essa abordagem facilitaria a identificação e isolamento de falhas entre ambientes mainframe e em nuvem, uma vez que cada módulo poderia ser avaliado individualmente.Para a implementação de testes integrados automáticos, foram adotadas ferramentas que permitem a simulação de interações complexas entre diferentes sistemas. Utilizou-se uma combinação de frameworks de automação de teste e linguagens de script específicas que, juntas, oferecem a flexibilidade necessária para modelar o comportamento do usuário e as transações entre sistemas heterogêneos. Essa abordagem permite a criação de cenários de teste altamente personalizáveis e adaptáveis capazes de simular uma vasta gama de operações bancárias em diferentes plataformas. A capacidade de automatizar esses testes integrados é fundamental para avaliar a interoperabilidade e o desempenho do sistema como um todo proporcionando insights valiosos sobre a robustez e a resiliência das integrações implementadas.Vale pontuar o papel das PoCs e os experimentos voltados a soluções de automação até então inexistentes. Isso permitiu identificar os principais desafios técnicos especialmente relacionados à integração de sistemas legados com plataformas em nuvem, o que exigiu uma abordagem criativa para superar as barreiras de compatibilidade e comunicação. Como resultado, houve refino das estratégias e soluções viabilizando entendimentos para a automação sem afetar questões de disponibilidade. Como resultado, foi possível estabelecer novos padrões para a realização de testes em sistemas bancários complexos e heterogêneos.Referências bibliográficas:- Adkins, H., Beyer, B., Blankinship, P., Lewandowski, P., Oprea, A., & Stubblefield, A. (2020). Building secure and reliable systems: best practices for designing, implementing, and maintaining systems. O'Reilly Media.- Erich, F. M., Amrit, C., & Daneva, M. (2017). A qualitative study of DevOps usage in practice. Journal of software: Evolution and Process, 29(6), e1885.- Forsgren, N., Humble, J., & Kim, G. (2018). Accelerate: The science of lean software and devops: Building and scaling high performing technology organizations. IT Revolution.- Gupta, R., Kranz, S., Regnat, N., Rumpe, B., & Wortmann, A. (2021). Towards a systematic engineering of industrial domain-specific languages. In 2021 IEEE/ACM 8th International Workshop on Software Engineering Research and Industrial Practice (SER&IP) (pp. 49-56). IEEE.- Kim, G., Behr, K., & Spafford, G. (2013). The phoenix project: A novel about IT. DevOps, and helping your business win, 345.- Lwakatare, L. E., Kuvaja, P., & Oivo, M. (2016). An exploratory study of devops extending the dimensions of devops with practices. Icsea, 104.- Rahman, A. A. U., Helms, E., Williams, L., & Parnin, C. (2015). Synthesizing continuous deployment practices used in software development. In 2015 Agile Conference (pp. 1-10). IEEE.- Shahin, M., Babar, M. A., & Zhu, L. (2017). Continuous integration, delivery and deployment: a systematic review on approaches, tools, challenges and practices. IEEE access, 5, 3909-3943. RESULTADO ECONÔMICO: Possibilidade de monitoração proativa de jornadas de clientes com redução de custos associada a menor indisponibilidade dos serviços. RESULTADO INOVAÇÃO: Processo de monitoração sintética para jornadas e APIs,  Novo processo para testes integrados considerando ambiente em nuvem e mainframe. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;"NÚMERO: 6 ID ÚNICO: 109742 NOME: DESENVOLVIMENTO DE SOLUÇÕES AVANÇADAS PARA DADOS TRANSACIONAIS E DE CLIENTES DESCRIÇÃO: Esta linha de pesquisa está associada a atividades experimentais para a criação e o desenvolvimento de novas soluções relacionadas a dados transacionais e de clientes.Para criar um motor de ingestão de dados, o objetivo tecnológico era entender processo de integração de dados,  ao considerar o reuso de peças de software existentes, implicou em restrições que aumentaram a complexidade do desenvolvimento. A modelagem se refletiu em um pipeline criando roles genéricas – usualmente, o que se tem é um role específico para cada tabela. Ao final, esperava-se ter solução para lidar com dados transacionais e, ao viabilizar a democratização dos dados, poder criar uma inteligência de dados para traçar perfis de clientes usuários de determinados produtos.Atividades também foram organizadas com o objetivo tecnológico de criar processo de consumo de dados de forma transacional – algo inexistente. Havia que se entender como desenvolver APIs transacionais de forma que recebesse requisição e se comunicasse com o data mesh. O processo conhecido fazia migrações de dados para o data mesh (modelo de armazenamento de dados descentralizado) em uma característica warehouse (modelo de armazenamento de dados centralizado). Era necessário evoluir esta solução.Para a criação de modelagem de uso de canal com cliente de forma a consolidar entendimento de suas jornadas, o objetivo tecnológico era desenvolver um sistema capaz de analisar a navegação (de todos os clientes e por longos períodos) para entender, de forma mais acertada, possíveis melhorias da experiência do cliente. Tentativas usuais de solução têm limitações uma vez que são menos analíticas e possuem um caráter mais opinativo. Também se baseiam em estudos a partir de entrevistas com clientes e, por isso, têm um escopo limitado. Ao fazer uso de tecnologia, espera-se cobrir uma gama maior de situações.Como resultado, espera-se viabilizar a criação de portais de análise de dados de forma a fornecer uma abordagem data driven nos diferentes canais em que o cliente atua, com criação de análises de sequências mais prováveis de utilização de serviços. Como diferencial tem-se uma nova forma de tratamento de dados a partir de novos algoritmos.Ao mesmo tempo que considerar o reuso de peças de software existentes diminuiria a complexidade do desenvolvimento, na prática tornou-se um ponto crítico por envolver a necessidade de generalizar pipelines que eram, por definição, específicas. Não compatibilizar peças existentes com as novas poderia inviabilizar o desenvolvimento. Apesar de se tratar de uma empresa que lida, basicamente, com transações, não havia solução relacionada a esse tipo de dados. Assim, configurou-se como marco crítico deste desenvolvimento transformar tais dados em elementos analíticos. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Ingestão de dados,  consumo de dados,  democratização de dados,  ambiente transacional,  cadeia de Markov,  aprendizagem de máquina,  modelos de agrupamento,  ciência de dados,  banco de dados relacional,  banco de dados não-relacional. NATUREZA: Processo ELEMENTO TECNOLÓGICO: A partir de estudos sobre a solução atual, com entendimento dos fundamentos e das redundâncias desnecessárias, foi possível propor estratégia para processo de ingestão de dados genéricos, via pipeline. Houve entendimentos para saber quais recursos de software criar e quais manter, uma vez que se buscava o reuso e não novas implantações. Houve compreensão de novo uso para o step function, cuja aplicação nesta situação, não era óbvia.Como elemento novo, houve entendimento para criar forma para consumo de dados, a transacional, em oposição à usual abordagem data warehousing. Houve comprovação técnica da aplicação do Athena para que as consultas não concorressem – não havia sido criado, ainda, uma forma de comunicação considerando esta tecnologia. Partiu-se da premissa que o Athena realiza consultas analíticas que não são concorrentes e não estão em um formato cliente servidor. Isso viabilizou, como elemento novo, utilizá-la para realizar consultas transacionais – isto é, aquelas que buscam por um único grupo de informação específica e de forma ágil. Não existia outra solução de consumo de dados considerando o ambiente transacional.Houve aprendizados que viabilizaram nova modelagem de análise da jornada do cliente a partir da sua navegação on-line. Houve entendimento de novo processo de identificação automática das atividades dos clientes considerando navegação internética. As soluções existentes funcionam em um contexto em que o componente principal de uma página é um produto. Aqui, tratava-se de uma situação em que os websites hospedam serviços e eles podem ser utilizados em conjunto fazendo com que não seja óbvio o mecanismo de análise de páginas da internet. O diferencial foi o desenvolvimento de novo algoritmo capaz de identificar as atividades realizadas pelo cliente dentro do canal.Aprendizados também viabilizaram a adaptação da estrutura de dados e criação de um algoritmo capaz de realizar o treinamento e o deploy de modelos baseados em cadeia de Markov de múltiplas ordens e diretamente em um banco de dados. As soluções existentes para treinamento de modelos geralmente são feitas carregando os dados em memória para então realizar o treinamento. No modelo aqui almejado, a análise deve ser feita simultaneamente considerando todos os dados de todos os clientes de um determinado segmento e por um longo período. Assim, há quantidade de dados analisados que não conseguem ser armazenados na memória o que limita o treinamento com ferramentas de mercado. Como elemento novo, houve aprendizados para criação de uma biblioteca para treinamento e implantação dos modelos baseados em cadeia de Markov diretamente no banco de dados.Cadeias de Markov são modelos estatísticos que utilizam probabilidade condicional para determinar a chance de transição de um estado para outro. Por essas características, construiu-se a hipótese de que eles conseguiriam modelar a navegação do cliente sendo capazes de predizer o comportamento dos usuários daquele canal. Porém, normalmente este modelo fica restrito a predizer qual a próxima página ou serviço que o cliente irá acessar. Houve aprendizados para utilizar a estrutura interna do modelo de comportamento de navegação para criar uma visualização em forma de grafo capaz de mostrar como os clientes acessam os serviços. Treinamentos associados à cadeia de Markov utilizam sequências de eventos que, ao final, são armazenadas na estrutura de dados interna do modelo em forma de probabilidades condicionais. Portanto, quando o evento observado na etapa de treinamento é informado, o modelo é capaz de predizer qual ou quais serão os próximos eventos. Como elemento novo, foram criadas operações de filtragem de público diretamente na estrutura de dados interna do modelo. A novidade é porque usualmente esse tipo de modelo a filtragem implica na seleção de público ou tipo de evento feito nos dados antes da etapa de treinamento por causa de uma limitação do próprio algoritmo do modelo. DESAFIO TECNOLÓGICO: Lidou-se com desafio associado a processos de integração visando criar um motor de ingestão de dados. Não era conhecido como criar processo para ingerir tabelas e criar roles de forma a se obter um recurso para todas as tabelas e consolidar uma solução genérica. Também não se sabia como encapsular a ingestão de dados, considerando outros sistemas de origem, sem replicá-los nestas respectivas origens. Buscava-se comprovação de um motor que facilitasse o acesso, a análise e o compartilhamento desses dados. Era incerto qual arquitetura poderia basear o desenvolvimento,  houve entendimento que o processo usual não resolveria buscando-se entender como fazer a solução com grafo de integração.Era incerto se o Athena seria capaz de unir diversas bases de dados, com permissionamentos e localizações diferentes, em um único JOIN. Havia que experimentar se ele possibilitaria o retorno da resposta em tempo viável para operações transacionais como seria em uma consulta analítica. Não havia muitas referências e casos de uso para serem utilizados como inspiração.Lidou-se com desafio sobre a forma de organização dos dados pois não se sabia como transformar a navegação dos clientes em atividades. Havia a barreira quanto à análise das interações dos clientes através de sequências de páginas carregadas e eventos observados. Esta forma de organização dos dados dificultou a identificação de padrões de navegação e colocou riscos à criação de processos de modelagem. O processo de análise dos resultados mostrou-se confuso, pois com a disponibilização de dados tal como dada era difícil identificar os serviços utilizados pelos clientes e correlacioná-los. Houve a compreensão de que a análise do comportamento dos clientes pode ser prejudicada pelo uso de atalhos criados e pela interrupção no uso dos serviços, dificultando a modelagem. Como hipótese, foi considerado que as interações dos clientes com o canal deveriam ser consideradas uma sequência de atividades.O desconhecimento de padrões de navegação, e até o desconhecimento da existência deles, constituiu uma barreira nas fases iniciais de estudo. Buscou-se em modelos baseados em aprendizado de máquina possíveis soluções. Havia dificuldades relacionadas a não se conseguir dados representativos em termos estatísticos de forma a obter a capacidade preditiva necessária. Para aplicação desta tecnologia havia que identificar padrões nas navegações dos clientes e agrupar os serviços disponibilizados de acordo com o seu uso. Assim, havia que correlacionar a navegação com o trabalho em que o cliente está, e depois agrupar serviços de acordo com o trabalho a ser executado.A partir da solução identificada, lidou-se com o desafio com a inferência na etapa em que são analisados novos dados. Experimentou-se executar tanto o treinamento quanto a implantação do modelo diretamente nas estruturas de dados. Foi criada uma biblioteca capaz de treinar um modelo analisando diretamente as bases de dados de navegação e armazenar sua estrutura interna. Houve comprovação de que, com essa biblioteca, a quantidade de dados analisados durante o treinamento de inferência fica limitada apenas pela tecnologia do banco de dados utilizada.Lidou-se com desafios quanto a cadeias de Markov, pois não há soluções disponíveis no mercado capazes de lidar com cadeias de ordem superior e grandes volumes de dados. Como hipótese, foi testado a possibilidade de o armazenamento das estruturas de dados internas de o modelo resolver os problemas de desempenho associados a grandes volumes.Durante o desenvolvimento da solução, havia a necessidade de fazer algumas análises que guiam os processos e elas deveriam ser feitas por público específico. As soluções disponíveis no mercado focam no caráter preditivo do modelo que possui etapas definidas de treinamento e inferência. Considerando o algoritmo original, toda filtragem de dados deve ser feita antes do treinamento, o que representava uma barreira ao seu uso. METODOLOGIA: De forma a lidar com os desafios encontrados nos desenvolvimentos desta linha de pesquisa, foram organizadas as seguintes atividades:1- Estudos para proposição de processo de integração de dados para dados transacionais- Tempo médio de integração de dados- Percentual de integrações bem-sucedidas)- Redução de redundâncias e inconsistências nos dados após a integração2- Experimentos buscando possíveis processos de criação de roles genéricas- Tempo médio para a criação de cada role- Redução no tempo de configuração de novos usuários3- Estudos para proposições de pipelines para criação de processo de consumo de dados transacionais com experimentos considerando Step Function- Latência média dos pipelines (a partir do tempo de processamento)- Taxa de sucesso na execução das Step Functions- Escalabilidade dos pipelines (considerando a capacidade de lidar com volumes crescentes de dados)4- Estudos buscando elementos para definição de arquitetura buscando soluções de grafos de integração- Tempo médio de consulta e integração de dados utilizando a arquitetura de grafos- Redução de redundâncias e inconsistências nos dados5- Experimentos considerando o Athena buscando proposições que levassem à comprovação técnica de união de bases de dados e retorno em tempo viável- Tempo médio de resposta das consultas no Athena- Percentual de consultas bem-sucedidas)6- Estudos para subsidiar experimentações com modelos avançados como cadeias de Markov, árvore de predição compacta, redes neurais profundas buscando mapear complexidades e padrões nos dados que não seriam perceptíveis em métodos tradicionais- Comparação de desempenho dos respectivos modelos com métodos tradicionais- Redução de erros de previsão7- Estudos sobre arquiteturas buscando referências de componentes e soluções com maior aderência e integração com o canal- Grau de aderência e integração dos componentes com o canal (medido por testes de integração)8- Estudos e experimentos buscando elementos para proposição de desenvolvimento de biblioteca- Taxa de adoção da biblioteca pelos desenvolvedores9- Experimentos a partir de ferramentas de Big Data, Apache Spark e Glue buscando estruturação de dados que facilitasse criação de análises visuais- Tempo médio de processamento e estruturação dos dados- Taxa de sucesso na criação de análises visuais10- Mapeamento dos fluxos de canais e tecnologias envolvidos no desenvolvimento buscando proposições de padronização- Grau de padronização alcançado (medido por conformidade com as diretrizes de padronização)- Redução de variabilidade nos processos de desenvolvimento11- Testes funcionais e não funcionais das soluções propostas- Percentual de testes bem-sucedidos- Identificação e resolução de bugs e problemas de desempenho12- Testes unitários e integrados buscando validação de desempenho das soluções propostas- Percentual de testes bem-sucedidos INFORMAÇÃO COMPLEMENTAR: Apesar de uma instituição financeira lidar com uma vasta quantidade de dados transacionais diariamente, não havia processo eficiente para tratar esses dados para fins analíticos. Tradicionalmente, os dados transacionais são utilizados para o processamento de transações do dia a dia, como pagamentos ou transferências,  sem tratamento, não havia competência de extrair insights valiosos que poderiam ser utilizados na tomada de decisões. A necessidade de transformar dados transacionais brutos em informações analíticas úteis justifica a organização de atividades experimentais que poderiam resultar em inovação tecnológica. O objetivo era superar limitações dos sistemas tradicionais que se concentram primariamente na função transacional sem aproveitar o potencial analítico dos dados.Os padrões de navegação dos clientes, quando adequadamente analisados, podem oferecer insights profundos sobre o comportamento do usuário, preferências e necessidades não atendidas. No entanto, a falta de ferramentas adequadas para processar e analisar esses dados de navegação limita a capacidade de entender e melhorar a experiência do cliente. É preciso encontrar uma forma de aprimorar a jornada on-line do cliente. O entendimento das restrições das abordagens tradicionais traz a necessidade de estudos para desenvolvimento de novos algoritmos e modelos de dados, como cadeias de Markov, para capturar e interpretar a complexidade da navegação do cliente.Por fim, os desenvolvimentos alvo desta linha de pesquisa permitem que a instituição saia de uma situação da existência de dados que não se comunicam entre si para uma organização que permita que se eles reflitam em uma maior capacidade analítica.Esta linha de pesquisa abordou diferentes formas e abordagens de armazenamento e tratamento de dados utilizando trabalhos como:- Winter, R., & Hackl, T. (2023, February). Data mesh at scale: Exploration of currentpractices in large organizations. University of St.Gallen, Institute of InformationManagement.Para subsidiar as ações relacionadas a Cadeias de Markov, foram utilizados os seguintes trabalhos:- M. Kaur, ""Why Stochastic Models That Are So Famous, Become Infamous In Reliability Engineering,"" 2020 Annual Reliability and Maintainability Symposium (RAMS), Palm Springs, CA, USA, 2020, pp. 1-8.- Schweitzer, P. J. (2021). A survey of aggregation-disaggregation in large Markov chains. Numerical solution of Markov chains, 63-88.- Vermeer, S., & Trilling, D. (2020). Toward a Better Understanding of News User Journeys: A Markov Chain Approach. Journalism Studies, 21(7), 879–894. RESULTADO ECONÔMICO: Redução de tempo de análise de dados por produto. Entendimento melhor do cliente a partir da identificação de que serviços são mais importante e como o cliente os utiliza. RESULTADO INOVAÇÃO: Novas estratégias para produtos mais específicos a cada cliente. Novo modelo capaz de simplificar a experiência de navegação. Contribuição à cultura data driven e analítica da empresa. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;"NÚMERO: 7 ID ÚNICO: 109743 NOME: SOLUÇÃO INTEGRADA PARA INGESTÃO, ANÁLISE, CLUSTERIZAÇÃO, MODELAGEM E DEMOCRATIZAÇÃO DE DADOS DESCRIÇÃO: Esta linha de pesquisa envolve processos para ingestão, análise, clusterização, modelagem e democratização de dados.Apesar de serem dimensões usuais, as soluções existentes não suportavam a quantidade e a variedade de dados gerados, seu crescimento exponencial, e a flexibilidade para integração de novos tipos de dados. Sistemas anteriores dependiam de processos de análise mais tradicionais e fragmentados,  tais soluções geralmente envolviam sistemas de banco de dados relacionais para armazenar e gerenciar dados com limitações quanto à escalabilidade e ao processamento de grandes volumes de dados,  muitas vezes exigiam processos manuais para a integração de dados de diferentes fontes, e processos manuais de ingestão e limpeza de dados que dependiam de scripts personalizados e rotinas de ETL com manutenção e ajustes contínuos. Ferramentas e plataformas operavam em silos, dificultando a visão unificada dos dados e a sua análise integrada.Com o objetivo tecnológico de entender como acelerar processo de ingestão de dados que alimentem motores de processamento, buscava-se comprovação de solução genérica para processos de integração de dados em qualquer linha de negócio. Para superar restrições relacionadas ao processamento de altas volumetrias de dados, o objetivo tecnológico era entender processo que permita a comparação de uma mesma base de dado em datas ou até mesmo versões diferentes de uma mesma base de forma a consolidar volumes. Havia que se tratar a questão de que, na solução nativa, os elevados volumes resultavam em reprocessamento de dados pois não se identificava o último registro e ele não era salvo,  isso resultava em perdas de dados no caso de alguma interrupção. Assim, o objetivo tecnológico foi entender como controlar o último registro executado superando desafios de heterogeneidade de dados, de grandes volumes, também buscando criar um sistema de recuperação automática após interrupções.Para desenvolver clusterizações de clientes de forma a guiar conversas e ofertas de produtos mais contextualizadas, o objetivo tecnológico foi criar processo de captura e centralização de dados. Entre os desafios destacam-se o desconhecimento de como absorver diferentes regras de negócio, de unificar as marcações de modo a manter a explicabilidade da regra, principalmente considerando origens de dados baseadas em diferentes infraestruturas.A ideia de desenvolver uma plataforma gamificada que dê transparência ao cliente sobre sua situação em determinados produtos levou ao objetivo tecnológico de lidar com dados de diferentes fontes, tratá-los e democratizar esses dados para diferentes áreas. O objetivo tecnológico era entender como construir solução para disponibilizar consumo de grande volume de dados que são gerados e armazenados na estrutura da plataforma de modelagem. Plataformas de gamification vêm ganhando espaço como ferramenta de engajamento,  porém, como é um desenvolvimento relativamente novo e carece de boas práticas a serem seguidas – o que se reflete em vários desafios tecnológicos a serem superados via atividades experimentais Changiz Hosseini et. Al. (2022).Ao final, almejava-se a criação de processos automatizados de explicitar e tratar os dados do cliente sem que o engenheiro de dados precisasse desenvolver novas rotinas e códigos complexos. Com isso, será possível construir plataformas de ingestão e integração de dados diminuindo o tempo de disponibilização das informações para a tomada de decisão.Aqui, um ponto crítico foi lidar com a volumetria e a flexibilidade associadas a dados. As tecnologias envolvidas não atendiam, tal como estavam, ao escopo do desenvolvimento. Apesar de causar estranheza, a princípio, uma vez que tais dimensões são inerentes ao desenvolvimento de software, houve a compreensão da necessidade de extrapolar conhecimentos existentes buscando avançar nos respectivos estados da arte. Se não fossem superadas tais barreiras, seria difícil viabilizar os desenvolvimentos almejados. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Gestão de dados,  integração de dados,  base de dados,  clusterização,  democratização de dados,  advanced analytics,  mineração de dados,  gamificação. NATUREZA: Processo ELEMENTO TECNOLÓGICO: O desenvolvimento das atividades desta linha de pesquisa resultou em um conjunto de aprendizados.A solução conhecida para processo de ingestão de dados envolvia o banco de dados Cassandra (sistema de gerenciamento distribuído projetado para lidar com grandes volumes de dados) em NoSQL (sistema que suporta grande variedade de modelos de dados), mas ela não atendia às funcionalidades necessárias. Porém, a partir desta base foi possível viabilizar processo de integração de dados entre camadas de diferentes origens e destinos, resultando em novo uso para as tecnologias citadas. Aprendizados levaram à proposição de monitoramento de notificação de alterações nos dados de origem: a notificação acontece no CDP (plataforma de dados do cliente) de forma que, ao ocorrer modificações estruturais em tabelas neste ambiente – como criação, remoção e atualização de tabelas – uma notificação é disparada através de um tópico Kafka.Além disso, experimentos consideraram o Step Function e sua combinação com Lambda de forma a viabilizar fluxos da aplicação, algo que ainda não havia sido utilizado no contexto de integração de dados. Com isso, houve comprovação de processo de orquestração dos serviços de integração para que as pipelines de ingestão possam ser executadas de forma assíncrona.Aprendizados para desenvolver processo para não duplicação dos dados levou à criação de ferramenta genérica que faz as comparações entre as versões das bases – em um Bucket S3 no formato *parquet para orquestrar um conjunto de Glue Jobs utilizando Spark. Apesar das tecnologias não serem inovadoras por si só, trata-se de novidade ao aplicá-la para este novo contexto refletindo em avanços no respectivo estado da arte.Para superar o desafio de tempo de ingresso dos dados à plataforma única – na solução existente era cerca de 400 horas – estudos permitiram viabilizar construção de peça que eliminava a necessidade de diversos microsserviços,  a solução usual seria usar a compartimentação de responsabilidade dos microsserviços para agilizar tal ingresso, algo que não se comprovou tecnicamente. Para construir as clusterizações, estudos se voltaram a técnicas de árvore de decisão, DBScan, KMeans e KNN, com entendimento do algoritmo baseado em árvore de decisão.Para a plataforma de gamificação, não era conhecido processo para obtenção de dados oriundos de diversas fontes devido às características específicas de interatividade em tempo real e personalização dinâmica das experiências dos usuários. O principal desafio era garantir a integridade dos dados durante a interação do usuário com o jogo. Estudos permitiram comprovar a utilização de dados democratizados no mesh através de consultas via glue e obtendo os dados fechados em D-1 de forma que não sofressem alterações enquanto o cliente estivesse no game. Assim, foi desenvolvido processo para obtenção de dados de diversas fontes sem demandar controles adicionais da parte consumidora.Diante dos desafios quanto à fragmentação das informações, houve entendimentos de lógica para lidar com a grande quantidade de arquivos pequenos sendo consolidado em módulo automático de mapeamento e concatenação de small files. Tais tipos de arquivos podem representar um gargalo significativo ao desempenho de sistemas distribuídos. Aprendizados levaram à criação de 2 lambdas, Mapper e Concat, utilizando python, para a geração de arquivos parquets aptos para consumo. A solução usual seria usar o glue,  porém, experimentos permitiram entender que a lógica de funcionamento criada a partir de lambdas teriam melhor tempo de processamento.Em suma, os aprendizados possíveis a partir de tecnologias existentes aplicadas a novo contexto resultaram em respectivos avanços no estado da arte das áreas de processos para ingestão, análise, clusterização, modelagem e democratização de dados. DESAFIO TECNOLÓGICO: A criação de processo de ingestão de dados lidou com o desafio de equilibrar o tempo de processamento com a volumetria apresentada – era necessário diminuir o tempo de ingestão utilizando a mesma volumetria da solução anterior. Havia que se superar restrições quanto à criação de scripts HQL, ShellScripts, entre outros processos convencionais, que resultavam em cerca de 400 horas para ingestão do banco de dados. Lidou-se com desafio de considerar grande quantidade de tabelas e em estruturas diferentes – sistemas legados e em nuvem. Não era factível criar um modelo de processamento para cada uma delas, sendo necessário organizar experimentos para compatibilizar as diferenças. Como hipótese, levou-se a teste a estratégia de convivência com a disponibilização da base produtiva de dados liberando o histórico para estudos a partir da conta consumer e seguindo a arquitetura data mesh.Apesar de problemas de volumetria serem dimensões comuns associadas a dados, havia que se lidar com problemas de interrupção que levavam ao reprocessamento dos dados. Algumas hipóteses foram consideradas – como uma biblioteca python desenvolvida por cientistas de dados com a premissa de disponibilizar bases através de processos que não ocupam memória. Como o mesmo usuário pode se encaixar em agrupamentos diferentes, havia riscos quanto à unificação da marcação do público. Como hipótese, experimentou-se a criação de um critério de priorização para eleger a categoria predominante desse usuário de modo e não comprometer a distribuição da população no estudo.Para a plataforma de gamificação, não era conhecido processo para obtenção de dados de diversas fontes. Algumas hipóteses de solução foram testadas. A primeira tentativa foi utilizar a resposta on-line de APIs síncronas e houve comprovação de retorno exato da foto do cliente naquele momento,  no entanto, ele não atendia ao outro requisito necessário de ter as informações consolidadas em tempo real não sendo factível esperar até o fim do dia. A segunda hipótese considerada foi utilizar a resposta assíncrona por eventos do tópico Kafka que trouxe o retorno sobre a movimentação da conta do cliente,  não houve comprovação a partir do entendimento da necessidade de um controle duplicado desses dados. Percebeu-se que se configurava um fluxo longo colocando complexidade devido à quantidade de clientes. Houve comprovação da terceira hipótese: utilizar dados democratizados no data mesh através de consultas via glue e obtendo os dados fechados em D-1 de forma que não sofresse alterações enquanto o cliente estivesse no game. Com isso, foi possível criar processo inédito para obtenção de dados de diversas fontes sem demandar controles adicionais da parte consumidora.Lidou-se com restrições associadas à abordagem de fragmentação dos dados. O fato de serem arquivos muito pequenos levava a um desempenho não otimizados nas consultas. Isso principalmente devido ao Athena precisar listar todos os locais de partição, processo que demanda tempo. Como hipótese, partiu-se da premissa que a manipulação de arquivos de maior tamanho, e hospedados em S3, poderiam resultar em operação mais ágil em comparação com os mesmos registros distribuídos em múltiplos arquivos menores. A hipótese de concatenar os arquivos menores contou com desafios de lidar com tabelas de partições de grande volume. Não era factível utilizar os recursos AWS para isso pois eles não atendidam à necessidade de uma instância com elevada capacidade de memória para efetuar as concatenações de maneira eficiente e no menor tempo possível. Experimentos considerando o Glue indicaram que ele executa essas operações de forma sequencial, em uma única instância, impondo limitações ao processo de otimização do armazenamento e acesso aos dados,  por isso, havia que se testar outras hipóteses. METODOLOGIA: Para superar desafios técnicos, foram organizadas as seguintes atividades:1- Experimentos para validar as proposições de peças da plataforma considerando tecnologias como Cassandra, Aurora, lambdas, step function, entre outrosMétricas:- Tempo médio de resposta das operações realizadas- Percentual de operações bem-sucedidas em relação ao total de operações- Capacidade do sistema de lidar com aumentos na carga de trabalho sem degradação de desempenho2- Experimentos para normalização e compatibilização de forma a consolidar ferramenta genéricaMétricas:- Percentual de dados que foram normalizados corretamente- Percentual de fontes de dados diferentes que foram integradas com sucesso- Tempo médio necessário para normalizar um conjunto de dados- Avaliação da precisão e integridade dos dados após normalização3- Estudos considerando a orquestração entre lambda e step function com experimentos e testes buscando comprovação técnica de processo orientado a eventosMétricas:- Avaliação de latência a partir do tempo médio de execução das funções Lambda orquestradas por Step Functions- Capacidade do sistema de escalar automaticamente com aumento de eventos4- Estudos sobre ferramentas Glue com experimentos para ferramenta genérica para comparação entre bases de dados, Métricas:- Tempo médio necessário para completar processos de ETL- Percentual de comparações corretas entre diferentes bases de dados5- Execução e avaliação de PoCs considerando cenários de: grande quantidade de tabelas, tipos de estruturas de tabelas diferentes, tipos variados de tecnologias de banco de dadosMétricas:- Percentual de tecnologias de banco de dados diferentes que foram integradas com sucesso- Avaliação de desempenho do sistema em cenários com grande quantidade de tabelas e estruturas variadas6- Experimentos considerando proposições com Cassandra embarcado para entendimento de refinos necessários na infraestrutura buscando comprovação técnica de processo sem lentidão ou perda de dadosMétricas:- Latência das operações a partir do tempo médio de execução das operações no Cassandra embarcado- Capacidade do Cassandra embarcado de lidar com aumentos na carga de trabalho7- Experimentos buscando estabelecer priorização a partir de processo de unificação da marcação do público com testes para validação de volumetria dos clusters, coerência com a realidade e possíveis impactos para o negócio em termos de comunicação, crédito, engajamento e experiência do clienteMétricas:- Avaliação qualitativa e quantitativa dos impactos nas áreas de comunicação, crédito, engajamento e experiência do cliente- Avaliação da volumetria dos clusters a partir do tamanho médio dos clusters gerados e sua coerência com a realidade do negócio8- Estudos e experimentos de hipóteses para obtenção de dados de outras fontes. Experimentos a partir da resposta on-line de APIs síncronas e de respostas assíncronas por eventos do tópico Kafka, de uso da estrutura data mesh através de consultas via glueMétricas:- Tempo médio de resposta das APIs síncronas- Latência dos eventos Kafka a partir do tempo médio de processamento- Taxa de sucesso das consultas glue- Avaliação da precisão dos dados obtidos de diferentes fontes9- Experimentos considerando lambdas buscando comprovação técnica de abordagem para lidar com grande quantidade de arquivos pequenosMétricas:- Tempo médio necessário para processar os arquivos pequenos- Percentual de operações bem-sucedidas em relação ao total de operações- Capacidade do sistema de escalar automaticamente com aumento na quantidade de arquivos10- Estudos e experimentos buscando elementos para composição de estratégia de mitigação para concatenar arquivos menores e, ao mesmo tempo, com tabelas de partições de grande volumeMétricas:- Avaliação do desempenho das tabelas de partições após a concatenação- Percentual de operações de concatenação bem-sucedidas- Percentual de utilização de CPU, memória e I/O durante o processo de concatenação. INFORMAÇÃO COMPLEMENTAR: Questões de volumetria e escalabilidade são inerentes ao desenvolvimento de sistemas de software projetados para manipular grandes conjuntos de dados e atender a um número crescente de usuários ou transações. A princípio, essas questões podem ser percebidas como desafios convencionais de engenharia em que a aplicação de conhecimentos e práticas estabelecidas poderia levar à identificação e implementação de soluções eficazes. Isso acontece principalmente porque esses problemas geralmente envolvem a otimização do uso de recursos computacionais, como memória e poder de processamento, e o gerenciamento eficiente de grandes volumes de dados.No entanto, a distinção entre um problema convencional de engenharia ou de um desafio técnico que exige desenvolvimentos experimentais é mais complexo. Deve ser feita considerando se as variáveis envolvidas são conhecidas e se há restrições de conhecimento para resolver o desafio. Ou seja, a distinção é definida pela não solução, e não apenas pelo problema, das questões envolvidas. Isso porque, em muitos desses casos, as soluções existentes e as práticas de engenharia de software tradicionais não são suficientes para resolver problemas de volumetria e escalabilidade de maneira eficaz. É possível que o estado da arte das tecnologias envolvidas não ofereça as capacidades necessárias para lidar com a escala e a complexidade dos dados ou do sistema em questão. Essa possível lacuna entre as necessidades do projeto e as soluções disponíveis destaca a natureza única desses desafios. É o caso encontrado por esta linha de pesquisa.Em contextos de desenvolvimento de software de alta complexidade, novos tipos de dados, padrões de acesso e requisitos de desempenho podem surgir estabelecendo condições diferentes de aplicação. Isso pode resultar em situações em que soluções usuais não atendam, ainda que o problema seja considerado comum. Essa situação pode se configurar uma incerteza ao desenvolvimento ao estabelecer lacunas de conhecimento técnico-científico.Portanto, enquanto questões de volumetria e escalabilidade podem se assemelhar a problemas de engenharia tradicionais à primeira vista, sua complexidade, a falta de soluções óbvias e a natureza dinâmica do ambiente de desenvolvimento as distinguem como desafios técnicos que podem, potencialmente, levar à inovação tecnológica, a partir de atividades experimentais e abordagens criativas.Citam-se exemplos de trabalhos acadêmicos que abordam questões de volumetria que não têm solução óbvia:- Grolinger, K., Higashino, W. A., Tiwari, A., & Capretz, M. A. M. (2013). ""Data Management in Cloud Environments: NoSQL and NewSQL Data Stores"". Journal of Cloud Computing: Advances, Systems and Applications, 2(1), 22.- Chen, L., et al. (2013). ""Big Data Challenge: A Data Management Perspective"". Frontiers of Information Technology & Electronic Engineering, 19(12), 1586-1597.Como base para entendimentos sobre plataforma gamificada, tem-se a seguinte referência:- Changiz Hosseini, Oda Humlung, Asle Fagerstrøm, Moutaz Haddara (2022), An experimental study on the effects of gamification on task performance, Procedia Computer Science, Volume 196, Pages 999-1006, ISSN 1877-0509. RESULTADO ECONÔMICO: Aumento da velocidade da disponibilização de dados, redução do tempo de processamento de dados a partir de automatização da comparação entre bases, redução de tempo de ingresso de jornadas na plataforma única de 400 horas para 4 horas. RESULTADO INOVAÇÃO: Novo processo genérico de ingestão de dados, novo processo de clusterização com uso intensivo de dados, novo processo gamificado a partir de dados de fontes diferentes DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 8 ID ÚNICO: 109744 NOME: SOLUÇÕES DE AUTOMAÇÃO DE PROCESSOS COMPLEXOS A PARTIR DE ESTRUTURAS LEGADAS E MODERNIZADAS DESCRIÇÃO: Esta linha de pesquisa tem, como objetivo tecnológico, demonstrar a viabilidade técnica de solução de automação de processos complexos sediados em estruturas legadas e em nuvem. Para isso, organizou atividades experimentais buscando proposições a serem validadas a partir de protótipos.Processos de automação referem-se ao uso de ferramentas e scripts para automatizar tarefas complexas ao longo do ciclo de vida do desenvolvimento de software. No contexto aqui estudado, trata-se de construir mecanismos automáticos para coleta, centralização e consolidação de dados de cliente. Na solução atual, a maior parte dos processos era manual,  os dados não eram padronizados (nem na forma de implementação e nem sua taxonomia) impactando no controle de qualidade associado. Não havia processo automático que lidasse com dados em diferentes infraestruturas tecnológicas (mainframe e nuvem). Com isso, a linha de pesquisa se voltou a entender elementos, tecnologias e processos a serem considerados em processos de automação entre diferentes ambientes.A automação de processos, à primeira vista, pode parecer uma solução simples e direta para otimizar as operações de uma empresa,  essa percepção geralmente vem da ideia de que a automação apenas substitui tarefas manuais por softwares seguindo um conjunto predefinido de regras. No entanto, criar e desenvolver fluxos até então descentralizados e em infraestruturas mistas enfrenta complexidade sobre demandar conhecimento profundo de diferentes tecnologias e, de forma criativa, propor combinações para composição de automação. Assim, configurou-se como ponto crítico desta linha de pesquisa diferentes composições levadas a experimento considerando metodologia sistemática, criatividade e um certo risco de insucesso.Vale pontuar, ainda, que a automação de processos em ambientes mainframe e em nuvem é complexa, especialmente em uma estrutura bancária centenária. Mainframes, que são cruciais para operações críticas e transações de alta segurança, utilizam linguagens e protocolos antigos, como COBOL e CICS, que não se integram facilmente com as soluções baseadas em nuvem que utilizam APIs modernas e microserviços, por exemplo. Além disso, a migração e integração de dados entre esses ambientes exigem rigorosos controles de segurança e conformidade regulatória devido à sensibilidade dos dados bancários. A compatibilidade entre diferentes arquiteturas, a necessidade de garantir a continuidade dos serviços e a minimização de riscos tornam a automação um desafio técnico significativo, exigindo soluções personalizadas e uma abordagem meticulosa para evitar interrupções e garantir a integridade dos dados.Ao final, esperava-se a composição de um protótipo de sistema de automação de esteiras de dados a ter sua viabilidade técnica testada em ambiente controlado, compondo uma solução agnóstica de automação independente da maturidade de suas tecnologias. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: automação,  centralização de dados,  padronização de dados,   qualidade de dados,  mainframe,  computação em nuvem,  máquinas de estado. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Esta linha de pesquisa voltou-se a demonstrar a viabilidade técnica de solução de automação de processos baseados em estruturas mainframe e em nuvem. Com isso, permitiu entender e aplicar novas orquestrações de tecnologias que, mesmo que individualmente não sejam novas, apresentam-se como inovadoras ao serem aplicadas a um novo contexto.Os estudos resultaram em um conjunto de aprendizados para o desenvolvimento de processo de automação e que foram considerados em protótipo. Houve compreensão de processo automático de captura de operações que são registrados na tabela história substituindo as informações da partição do RT2 (do mainframe). Essa fonte deve ser utilizada em uma plataforma de análise de dados com ferramentas para preparação, blendagem, análise e compartilhamento de dados. Entendeu-se um mecanismo de leitura de arquivo gerado por analistas subsidiando a criação de scripts para tratar o arquivo e armazená-lo na base histórica. Foi possível entender o tratamento e a inclusão de regras específicas de cada produto sendo o output do processo armazenado no S3 como arquivo JSON.Estudos levaram à proposição de fluxo automático de comunicação com as informações internas realizada por meio de tópicos Kafka para registrar eventos de disponibilização de arquivos. Para monitorá-los, comprovou-se a aplicação de uma função sem servidor acionada por um conector Lambda. Essa função deve se autenticar internamente via token e fazer o download das informações, armazenando-as em um bucket S3.Para otimizar o consumo dos endpoints das APIs envolvidas, foi compreendida a criação de um processo de automação que armazena qualquer informação complementar no bucket S3. As informações transacionais representam a primeira camada de dados (SOR), que são os dados brutos sem transformações. Após análise desses dados, compreendeu-se a necessidade de construir outra camada (SOT) para conter os dados após as transformações necessárias. Após a gravação da camada SOT, houve a compreensão de uma nova etapa de automação com plataforma externa.Entendeu-se a aplicação de Java em ECS na estrutura QuickCloud projetada para implementação, gerenciamento e automação de aplicações em ambientes de nuvem. Para simplificar e padronizar as aplicações backend, a automação foi realizada a partir do acionamento da aplicação Java de uma Step Function através de um EventBridge. Foi necessário entender como configurar uma regra no EventBridge que trate de mudanças de estado específicas e o destino para onde o evento deve ser encaminhado. O objetivo era validar um processo em que, quando um evento correspondente ao padrão definido ocorre, o EventBridge automaticamente dispara a Step Function associada, iniciando seu fluxo de trabalho. Dentro da Step Function, cada etapa é definida para realizar uma tarefa específica, como a execução de uma função Lambda que roda código Java, a manipulação de dados, a tomada de decisões baseada em condições pré-definidas ou a interação com serviços em nuvem. Aprendeu-se que a integração entre EventBridge e Step Functions permite a criação de fluxos de trabalho altamente flexíveis e automatizados, onde a aplicação Java pode ser acionada e manipulada de acordo com a lógica de negócios definida na Step Function, proporcionando uma solução para automação de processos em ambientes cloud.A aplicação do Step Function para orquestrar lambdas de integração permitiu a reconstrução da aplicação legada em mainframe na forma de máquinas de estado. Foi possível entender, a partir da abstração proporcionada por erros e retentivas, a criação de uma lambda genérica para automatizar processos entre APIs e Kafka.Estes aprendizados permitiram construir protótipo buscando validação técnica de orquestrações inéditas de tecnologias em novos contextos resultando em avanços significativos no estado da arte relacionado à automação de processos.Os desenvolvimentos relacionados ao novo fluxo de automação também foram reconhecidos com prêmio Hackathon. DESAFIO TECNOLÓGICO: Para demonstrar a viabilidade técnica, a partir da construção de protótipo, de solução de automação de processos a partir de estruturas legadas mainframe e em nuvem, esta linha de pesquisa lidou com um conjunto de desafios técnicos.Os fluxos de trabalho envolvidos deveriam ser consolidados em uma infraestrutura em nuvem. Com isso, não se tratava de uma mera integração ou adaptação de tecnologias,  havia que se criar estratégia de automação considerando a transformação dos fluxos. Como as tecnologias apresentam configurações distintas, era necessário encontrar formas de compatibilidade – por exemplo, sistemas em mainframe são configurados para dar rollback em caso de erros na aplicação,  já tecnologias em nuvem não têm tratamento para esse tipo de situação. Tal fato coloca dificuldades de alarmes em caso de erros de forma que eles sejam identificados apenas no final do processo. Casos como esse aumentavam a complexidade do projeto.Processos de automação considerando infraestruturas mistas são desafiadores por natureza. Era necessário compreender como construir uma comunicação efetiva entre sistemas realizada por meio de tópico Kakfa combinado com lambda. Embora esta composição seja comumente utilizada em processos de transmissão e processamento de dados em tempo real, estudos mostraram que esse arranjo não aborda completamente a orquestração de sistemas mistos sendo insuficientes tanto frameworks como metodologias específicas para facilitar a integração nestas condições. Estudos de referência indicam que ainda não há uma solução usando Kafka e Lambda de forma a lidar, sem problemas, com questões de padronização e governança de dados em tempo real, o que deveria ser comprovado tecnicamente.Estudos indicaram que a abordagem habitual de microsserviços não atenderia requisitos de processamento em paralelo devido à complexidade de coordenação entre múltiplos serviços distribuídos,  a latência e a sobrecarga de comunicação entre microsserviços poderiam dificultar requisitos de alto desempenho em processamento paralelo. Levou-se a experimento a aplicação de Step Function em conjunto com lambdas responsáveis por papéis específicos dentro do fluxo. A proposição testada era a de configuração do fluxo de automação a partir de estímulo do gateway responsável por receber as requisições de elegibilidade que repassa os dados para um lambda que realiza validações de contrato e resgata as regras de elegibilidade presentes em um banco NoSQL específico para a categoria de produto informada. Ocorre, então, uma transformação de dados para um padrão esperado e o Step Function, com base na regra de elegibilidade consultada, estimularia a execução paralela dos motores. Em seguida, testou-se compor uma tratativa dos diferentes resultados dos motores a fim de serem transmitidos para o lambda responsável pela construção da resposta. A proposição a ser testada era que o lambda processaria os resultados dos motores, persistiria o resultado no banco NoSQL e retornaria o payload de resultado para o gateway estimulado no passo inicial.Considerou-se a hipótese também de incluir no processo um possível tratamento manual até a validação final do processo. Levou-se a experimento a construção de lambda com definições das máquinas de estado utilizando o Step Function colocando o processo manual como ferramenta de apoio para validação. Porém, lidou-se com desafios relacionados a compatibilizar aplicações de Step Function com a construção da lógica para criação de máquina de estado. Apesar de as tarefas no Step Function serem realizadas por um lambda, era incerto ser possível integrar diversas funções lambdas tal como presentes na arquitetura proposta. Experimentos voltaram-se a entender como fazer manuseio das estruturas de dados que transitavam no Step Function. Era incerto ser possível construir ações de processamento paralelo considerando as máquinas de estado de forma a sincronizar resultados. METODOLOGIA: Para superar os desafios encontrados nesta linha de pesquisa foram organizadas as seguintes atividades:1- Estudo sobre governança de dados buscando elementos para proposições relacionadas à construção de camada que lide com dados coletados de diferentes fontesMétricas:- Casos de governança a serem testados- Redução de inconsistência de dados associado a cada caso2- Experimentos controlados buscando entender e avaliar tecnologias relacionadas à automação de processos complexos buscando referências a serem consideradasMétricas:- Número de tecnologias identificadas- Número de PoCs elaboradas e testadas- Percentual de processos automatizados que são concluídos com sucesso sem intervenção manual3- Experimentos considerando diferentes estratégias para otimizar o consumo de APIs. Levou-se a teste diferentes cenários de uso de cache, batching de requisições e técnicas de rate limitingMétricas:- Redução de latência relacionada a APIs- Aumento na taxa de transferência da API4- Execução de PoCs para validar principais soluções em ambiente controladoMétricas:- Número de PoCs executadas- Taxa de sucesso das PoCs em atender os critérios testados5- Experimentações a partir de ferramentas de orquestração de fluxo de trabalho, como Step Functions, buscando entendimentos para delineamento de proposições para suportar processos de desvio e tratamento de errosMétricas:- Número de cenários do fluxo da orquestração- Taxa de sucesso de processos para lidar com erros- Redução da intervenção manual no tratamento de erros6- Estudos buscando referências para desenvolver um processo sistemático para realizar testes de integração e validação manual das aplicações. A partir de delineamentos experimentais associados, foram feitas experimentações com o objetivo de definir um conjunto de critérios e procedimentos para testes manuais considerando a configuração de lambdas e máquinas de estado, Métricas:- Taxa de sucesso a partir dos testes manuais para validação- Redução de problemas de integração pós-validação de orquestração7- Estudos considerando uso de Step Functions e aplicação de Lambda buscando proposições com foco no processamento paralelo e na manipulação de múltiplos caminhos e estados. Experimentos buscando validar propostas da capacidade de sincronização de resultados e a eficiência na gestão de estados complexosMétricas:- Número de cenários de processamento paralelo- Taxa de sucesso de sincronização- Melhoria da eficiência na gestão de estados8- Estudos considerando técnicas avançadas para otimizar a persistência de dados em bancos NoSQL com foco em esquemas de dados que suportem consultas eficientes e escaláveisMétricas:- Número de cenários de persistência de dados testados- Redução no tempo de recuperação de dados9- Desenvolvimento de protótipo para validar as orquestrações de tecnologias em novos contextosMétricas:- Tempo de resposta e taxa de transferência sob condições de carga simulada- Tempo médio de processamento por tarefa/transação- Número de problemas de compatibilidade entre mainframe e nuvem INFORMAÇÃO COMPLEMENTAR: O desenvolvimento desta linha de pesquisa parte da premissa de que, conforme estabelecido pelo Manual de Frascati, a orquestração de tecnologias conhecidas em novos contextos pode ser considerada uma inovação tecnológica. Nela, aplicam-se tecnologias já existentes, como APIs, Kafka, Lambda, Step Functions, entre outras, em um contexto de integração entre ambientes mainframe e nuvem que, por definição, são únicos. A inovação aqui não reside nas tecnologias individualmente, mas na maneira como elas são combinadas e aplicadas para resolver problemas específicos de automação de processos complexos.Como mainframes utilizam linguagens e protocolos antigos, como COBOL e CICS, que não se integram facilmente com soluções modernas baseadas em nuvem que utilizam APIs e microserviços, a compatibilidade entre diferentes arquiteturas, a necessidade de garantir a continuidade dos serviços e a minimização de riscos tornam a automação um desafio técnico significativo. Portanto, é essencial realizar experimentações, a partir de atividades de pesquisa e desenvolvimento, para desenvolver soluções personalizadas que atendam a esses requisitos específicos.Vale pontuar que a impossibilidade de realizar desenvolvimentos diretamente em ambientes produtivos influenciou a forma de organização das atividades experimentais. Qualquer erro ou falha poderia resultar em indisponibilidade de serviços críticos, o que é inaceitável em um contexto bancário. Portanto, a estratégia adotada foi buscar proposições para testar em protótipos, buscando validações técnicas antes de qualquer implementação em ambiente produtivo.Como subsídio para o desenvolvimento desta linha de pesquisa, destacam-se as seguintes referências bibliográficas:- Achanta, M. (2023). Data governance in the age of cloud computing: Strategies and considerations. International Journal of Science and Research (IJSR), 12(11).- Cheng, M., Qu, Y., Jiang, C., & Zhao, C. (2022). Is cloud computing the digital solution to the future of banking? Journal of Financial Stability, 63, 101073.- Kleppmann, M. (2017). Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems. O'Reilly Media.- ORGANIZAÇÃO PARA A COOPERAÇÃO E DESENVOLVIMENTO ECNONÔMICO (OCDE). Manual de Frascati: Proposta de práticas exemplares para inquéritos sobre investigação e desenvolvimento experimental. Coimbra: OCDE, 2007.- Rana, M. E., Yik, T. M., & Hameed, V. A. (n.d.). Cloud computing adoption in the banking sector: A comparative analysis of three major CSPs. IEEE. RESULTADO ECONÔMICO: Diminuição de custos associados à redução de redundâncias de processo. RESULTADO INOVAÇÃO: Nova capacidade automática para monitoramento de inclusão e processamento de dados de diversas áreas. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 9 ID ÚNICO: 109745 NOME: PROCESSAMENTO DE DADOS PARA GESTÃO DE AUTENTICAÇÕES E CONSENTIMENTOS MULTIBANCÁRIOS DESCRIÇÃO: Esta linha de pesquisa tem como objetivo obter novas compreensões a fim de aperfeiçoar a eficiência e segurança nos processos de gestão de autenticações e consentimentos dentro de um ecossistema de compartilhamento de dados bancários entre várias instituições financeiras. Para alcançar tal propósito, propõe-se a adoção de uma arquitetura serverless, com experimentações de funções Lambda, visando assegurar uma maior escalabilidade do sistema e a redução significativa dos custos operacionais. Foi proposta a experimentação de uma arquitetura serveless que o sistema se ajuste automaticamente às demandas variáveis, sem a necessidade de gerenciamento direto de servidoresAlém disso, foram realizados experimentos com bases de dados não relacionais, com o intuito de melhorar o armazenamento e a recuperação de informações complexas, visando não apenas a alta performance, mas também a agilidade do sistema em responder às solicitações. Essa abordagem é complementada pelo desenvolvimento de mecanismos voltados à segurança dos dados, incluindo o gerenciamento de concorrência e a aplicação de técnicas avançadas de controle de versão e transações, fundamentais para assegurar a integridade e a confiabilidade dos dados manipulados. A serialização eficiente de dados, por meio do uso de bibliotecas especializadas que reduzem o tamanho dos dados transmitidos, visa otimizar ainda mais a performance do sistema, garantindo uma transferência de dados rápida e segura entre as diferentes entidades.Espera-se elevar o nível de eficiência e segurança na gestão de autenticações e consentimentos nos ambientes de compartilhamento de dados. O desenvolvimento da arquitetura serverless e das bases de dados não relacionais deve proporcionar um sistema mais agilidade e escalabilidade, além de prover recursos mais robustos em termos de segurança da informação.Vale destacar ainda um ponto crítico no desenvolvimento do projeto, sendo o desenvolvimento e a operacionalização corretos de uma arquitetura serverless robusta, combinada com o uso de bases de dados não relacionais, para aprimorar a eficiência e segurança nos processos de gestão de autenticações e consentimentos dentro do ecossistema de compartilhamento de dados bancários. Esse avanço foi fundamental, pois demonstrou a viabilidade de o sistema projetado para ser altamente escalável, que se ajusta automaticamente às demandas variáveis de tráfego, sem a necessidade de gerenciamento direto de servidores, atingindo assim uma redução significativa dos custos operacionais.A integração bem-sucedida de funções Lambda e a adoção de bases de dados não relacionais não foram pontos triviais, tendo a necessidade de evoluir substancialmente o armazenamento e a recuperação de informações complexas, proporcionando alta performance e agilidade no sistema para responder às solicitações de maneira eficaz. A complementação dessa abordagem com o desenvolvimento de mecanismos avançados de segurança de dados, incluindo gestão de concorrência, controle de versão sofisticado e técnicas de transação, foi crítica para assegurar a integridade e a confiabilidade dos dados manipulados em um ambiente compartilhado. Além disso, a serialização eficiente de dados, por meio do uso de bibliotecas especializadas que minimizam o tamanho dos dados transmitidos, otimizou ainda mais a performance do sistema, garantindo uma transferência de dados rápida e segura entre a empresa e demais instituições financeiras. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Relacional,  dados,  IAM,  serviços de autenticação,  consentimento,  modelo,  serialização,  descentralizada,  pesquisa. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Buscou-se a comprovação científica de que um sistema de funções distribuídas poderia ser utilizado para criar um sistema de processamento de eventos em tempo real, altamente distribuído e resiliente, utilizando arquiteturas de microsserviços com comunicação entre regiões geográficas. Tal hipótese foi levada a experimentações. Configurou-se um sistema distribuído de funções em múltiplas regiões, com replicação de eventos de autenticação e consentimento entre regiões. Utilizou-se um sistema de orquestração de workflows complexos entre essas funções, com failover automático em caso de falha de uma região. O sistema obteve tempos de failover inferiores a 1 segundo e latência média de 250 ms por função.Buscou-se comprovar a viabilidade da aplicação de técnicas avançadas de caching e comunicação, como cache distribuído e comunicação assíncrona, para melhorar a performance do sistema de autenticação e consentimento. Assim, implementou-se um cache distribuído e configurou-se comunicação assíncrona entre funções e o sistema de armazenamento de dados. Foram realizados testes de carga com 5000 conexões simultâneas e mediu-se a latência de acesso a dados em cache. O cache distribuído reduziu a latência de acesso aos dados em até 86%, com um tempo médio de 3 ms para dados em cache. A comunicação assíncrona demonstrou alta eficiência, com latências médias de 15 ms por mensagem.Foi testada ainda a ideia de que o uso de pooling de conexões com protocolos avançados poderia melhorar a performance e a eficiência de comunicação no sistema distribuído. Assim, criou-se um pooling de conexões utilizando um protocolo avançado com multiplexação de streams, suportando até 10.000 conexões simultâneas. Foram executados testes de carga para medir a latência e a eficiência de comunicação. O pooling de conexões resultou em uma melhora de performance de até 64%, com redução do tempo de estabelecimento de novas conexões para 6 ms e latência média de comunicação de 18 ms.A consistência eventual e transações foram exploradas. A hipótese de que a consistência eventual poderia ser combinada com técnicas de aprendizado de máquina para prever e otimizar operações de leitura em um sistema distribuído foi testada por meio da criação de um modelo de consistência eventual para operações de leitura, combinado com um algoritmo de aprendizado de máquina para prever padrões de acesso e otimizar a distribuição de dados. Realizaram-se testes com 200.000 transações simultâneas, medindo a latência e a precisão das previsões. A combinação de consistência eventual e aprendizado de máquina resultou em uma melhoria de performance de 46%, com latências médias de 16 ms e precisão de previsão de 94%.A exploração de blockchain para gestão descentralizada de identidade e acesso também foi realizada. A hipótese era que blockchain poderia ser utilizado para criar um sistema de gestão de identidade e acesso com contratos inteligentes para automação de autenticações e consentimentos. Implementaram-se contratos inteligentes em plataformas de blockchain para automatizar processos de autenticação e consentimento. Realizaram-se testes de carga com 10000 transações simultâneas, medindo a latência e a eficiência dos contratos inteligentes. Os contratos inteligentes demonstraram alta eficiência, com latências médias de 6 segundos em uma plataforma e 0.9 segundos em outra, validando a viabilidade de automação de processos de autenticação e consentimento. Além disso, a hipótese era que blockchains privadas com mecanismos de consenso avançados poderiam fornecer maior controle sobre a performance e a segurança das transações foi testada com a implementação de uma blockchain privada com mecanismos de consenso avançados, realizando testes de carga com 2000 transações simultâneas e medindo a latência e a resiliência do sistema. Os mecanismos de consenso avançados demonstraram alta resiliência e baixa latência, com tempos médios de 1 segundo por transação. DESAFIO TECNOLÓGICO: Durante a execução dos experimentos para sincronização de estado em tempo real, a equipe observou que, sob condições de alta variabilidade de latência, que podia oscilar entre 50 ms e 1200 ms, o algoritmo de consenso modificado falhava em manter a consistência de estado em até 30% das transações sob teste. A hipótese de solução envolveu a introdução de um sistema de buffer adaptativo que armazenaria temporariamente as transações durante picos de alta latência, com o algoritmo ajustando dinamicamente o tamanho do buffer com base na latência média observada nos últimos minutos. No entanto, a implementação desse sistema introduziu complexidades adicionais, como a necessidade de um mecanismo eficiente de resolução de conflitos para transações que eram armazenadas no buffer por períodos prolongados.Para o sistema de failover automático, experimentos iniciais mostraram que o tempo de detecção e resposta a falhas era inaceitavelmente alto, com média de 500ms, muito acima do objetivo de 100ms. A equipe propôs a utilização de uma rede de sensores de falha distribuídos, que poderia aumentar a precisão da detecção de falhas através de um modelo de votação ponderada. Cada sensor avaliaria a saúde do sistema com base em um conjunto de métricas pré-definidas, e um algoritmo de agregação ponderada consolidaria essas avaliações para determinar a necessidade de failover. A complexidade da calibração dos pesos atribuídos a cada sensor, no entanto, revelou-se um desafio significativo, exigindo múltiplas iterações de ajuste fino para alcançar a sensibilidade desejada.Na otimização de caching, a volatilidade das cargas de trabalho resultou em taxas de acerto de cache abaixo de 60%, significativamente menor do que a meta de 90%. A equipe testou um modelo de aprendizado de máquina que utilizava redes neurais recorrentes (RNN) para prever a demanda baseando-se em padrões de acesso históricos. Embora os modelos iniciais tenham mostrado promessa, ajustando-se a padrões de acesso com uma precisão de até 78%, a latência introduzida pelo tempo de inferência do modelo, que poderia chegar a 200 ms, comprometeu a eficácia da solução em um ambiente de produção.O desafio do pooling de conexões sob cargas extremas revelou que o novo protocolo de comunicação experimental atingia sua capacidade máxima a aproximadamente 10.000 conexões simultâneas, abaixo do objetivo de 50.000. A equipe propôs a implementação de uma camada de abstração que permitiria a multiplexação de conexões virtuais sobre um número limitado de conexões físicas, utilizando um esquema de priorização baseado na criticidade da tarefa. No entanto, a complexidade de implementar um sistema de priorização eficaz sem introduzir latência adicional foi um desafio notável.Para a previsão de padrões de acesso utilizando aprendizado de máquina, a adaptação rápida dos modelos a mudanças quase em tempo real em padrões de acesso provou ser problemática. Os modelos demoravam cerca de 5 minutos para se adaptar a novos padrões, um atraso considerável em um ambiente onde os padrões de acesso podem mudar em questão de segundos. A equipe explorou a utilização de modelos de aprendizado de máquina mais leves e técnicas de treinamento incremental, que permitiriam aos modelos ajustarem-se mais rapidamente. Ainda assim, encontrar o equilíbrio certo entre velocidade de adaptação e precisão do modelo permaneceu um desafio.Além disso, a segurança dos dados em um ambiente distribuído apresentou desafios significativos, especialmente na implementação de criptografia de ponta a ponta e gerenciamento de chaves em escala. A equipe propôs uma solução de gerenciamento de chaves descentralizado, que utilizava blockchain para manter um registro imutável e seguro das chaves de criptografia. Contudo, a latência adicional introduzida pela verificação de chaves via blockchain, especialmente em regiões com alta latência de rede, exigiram soluções criativas, como o caching local seguro de chaves frequentemente acessadas. METODOLOGIA: Para superar os desafios técnicos identificados, adotou-se uma metodologia de desenvolvimento experimental centrada na iteratividade, validação contínua e ajuste fino das soluções propostas. Inicialmente, realizou-se uma análise detalhada das condições de rede e padrões de uso para identificar as principais variáveis que afetam a sincronização de estado e a latência de rede.Avaliou-se também o desempenho atual do sistema de failover e caching sob diferentes cargas de trabalho para estabelecer uma linha de base, juntamente com uma análise dos protocolos de comunicação e mecanismos de segurança para identificar gargalos e vulnerabilidades.Com base nessa análise preliminar, formularam-se hipóteses para a melhoria da sincronização de estado, incluindo a implementação de um sistema de buffer adaptativo e um algoritmo de consenso modificado. Desenvolveram-se modelos de aprendizado de máquina para a previsão de demanda de caching e adaptação rápida de modelos a mudanças de padrões de acesso, além de propor um novo protocolo de comunicação experimental e um sistema descentralizado de gerenciamento de chaves.A fase seguinte envolveu a implementação de protótipos para cada uma das soluções propostas, utilizando dados simulados em ambientes controlados para avaliar a viabilidade técnica e identificar possíveis ajustes. Executaram-se testes de carga e stress para avaliar a robustez dos sistemas de failover e pooling de conexões sob condições extremas. Estes experimentos em ambiente controlado permitiram a realização de testes das soluções de sincronização de estado e caching com cargas de trabalho variáveis, ajustando os modelos de aprendizado de máquina com base nos resultados dos testes para refinar a capacidade de previsão e adaptação dos modelos.A fase de validação e ajuste fino consistiu na validação das soluções em um ambiente de produção limitado, monitorando o desempenho e coletando feedback para ajustes finos. O ajuste fino dos algoritmos e sistemas com base nos dados coletados focou na otimização de performance e segurança. Por fim, a avaliação compreensiva dos resultados obtidos em comparação com os objetivos definidos, utilizando métricas de desempenho específicas para cada desafio, permitiu uma iterativa refinamento das soluções e abordagens conforme necessário.Especificamente, para cada desafio, delinearam-se experimentos específicos, acompanhados de testes para confirmação das hipóteses. Para a sincronização de estado, testes de latência variável com simulação de diferentes condições de rede foram utilizados para validar a eficácia do buffer adaptativo e do algoritmo de consenso.No sistema de failover, simularam-se falhas em componentes críticos do sistema sob diferentes condições para avaliar a precisão e tempo de resposta do sistema de monitoramento baseado em quóruns dinâmicos. A otimização de caching foi avaliada através de testes de carga para verificar a eficácia do modelo de aprendizado de máquina na previsão de demanda e ajuste das políticas de caching.Para o pooling de conexões e protocolo de comunicação, testes de estresse avaliaram a capacidade máxima de conexões simultâneas e a eficiência do protocolo de comunicação sob cargas extremas. Na segurança de dados, a robustez do sistema de gerenciamento de chaves descentralizado e a eficácia da criptografia de ponta a ponta foram testadas através da simulação de ataques. INFORMAÇÃO COMPLEMENTAR: Os desenvolvimentos realizados demonstraram uma melhoria significativa na eficiência operacional e na gestão de recursos, graças à implementação de funções Lambda para o processamento em tempo real de eventos de autenticação e consentimento. A utilização de um sistema de banco de dados não relacional, em conjunto com estratégias de serialização e deserialização de dados, reduziu drasticamente o overhead de comunicação, resultando em uma diminuição considerável da latência de acesso aos dados. Além disso, a adoção de técnicas de caching e o pooling de conexões contribuíram para uma melhora de até 50% na performance em cenários de alta demanda, destacando a eficácia das soluções implementadas para otimizar a escalabilidade e a eficiência operacional.Além disso, a exploração de uma solução de Gestão de Identidade e Acesso (IAM) descentralizada, baseada em tecnologia blockchain e redes de confiança, apresentou resultados promissores em termos de segurança e performance. Apesar dos desafios associados à latência e aos custos das transações em blockchains públicas, a combinação de blockchains privadas com mecanismos de consenso eficientes mostrou-se uma abordagem viável. Além disso, o desenvolvimento de protocolos de criptografia de ponta a ponta e técnicas de anonimização garantiu a proteção das informações compartilhadas na rede, cumprindo com as regulamentações de privacidade de dados para a gestão de consentimentos em tempo real. RESULTADO ECONÔMICO: A integração de funções Lambda e bancos de dados não relacionais reduziu o overhead de comunicação e melhorou a eficiência operacional, diminuindo custos operacionais em cenários de alta demanda. RESULTADO INOVAÇÃO: A inovação trouxe entendimentos valiosos de como desenvolver um sistema IAM descentralizado que une blockchain e redes de confiança, elevando a segurança e eficiência na gestão de consentimentos. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;"NÚMERO: 10 ID ÚNICO: 109746 NOME: SOLUÇÕES AVANÇADAS DE INTELIGENCIA ARTIFICIAL PARA SISTEMAS BANCÁRIOS DESCRIÇÃO: Esta linha de pesquisa trata de evoluções relacionadas a técnicas de inteligência artificial de forma a viabilizar a análise de dados bancários e novos sistemas avançados de autenticação de usuários.Visando construir uma solução para analisar, processar e interpretar grandes volumes de dados bancários, foram organizadas atividades com objetivo tecnológico de construir um modelo de Machine Learning que lidasse com análises preditivas e comportamentais extraídas de dados históricos e transacionais. Tal modelo deveria absorver demandas de segurança e privacidade dos dados,  foram organizadas atividades com objetivo tecnológico de desenvolver práticas avançadas de criptografia, incluindo criptografia de ponta a ponta e técnicas de anonimização de dados, assegurando a conformidade com regulamentações globais de proteção de dados.Com o advento da tecnologia, novas formas de aplicar golpes, processos/transações fraudulentas surgiram e suas ocorrências cresceram consideravelmente. Aza, I., Munir, K., & Almutairi, M. (2022). Dado o problema, a presente linha de pesquisa busca abordar também a experimentação de tecnologias baseadas em IA Generativa para a ideação e desenvolvimento de soluções que diminuam as ocorrências por fraudes em transações/processos bancários, sendo então realizados estudos e a fim de obter novos conhecimentos capazes de viabilizar a construção dessas soluções. Vorobyev, I., & Krivitskaya, A. (2022).O desafio de processar eficientemente grandes conjuntos de dados em tempo real para a detecção de fraudes era elevado, dada a arquitetura subotimizada das redes neurais e por implementações de código que não maximizavam a performance. A integração de modelos de aprendizado de máquina em sistemas legados acabava por gerar problemas como serialização e desserialização decorrentes de incompatibilidades entre as versões das bibliotecas utilizadas nos ambientes de desenvolvimento e produção. Essas dificuldades não apenas retardavam o processo de treinamento e implementação dos modelos, mas também colocavam em risco a integridade e a segurança dos dados dos clientes.Para o desenvolvimento de um sistema avançado de autenticação e identificação de usuários, o objetivo tecnológico era entender algoritmos de reconhecimento facial. Este sistema foi pensado para aprimorar a segurança das transações e acessos aos serviços bancários. As atividades experimentais se voltaram a entender técnicas de aprendizado profundo buscando comprovações de sistema capaz de analisar características faciais únicas de cada usuário com alta precisão, mesmo em condições variáveis de iluminação e ângulos de face.Espera-se que os desenvolvimentos presentes nesta linha de pesquisa resultem em um aumento considerável da eficácia da detecção de fraudes e na proteção de dados. A otimização da arquitetura da rede neural, juntamente com a implementação eficiente do código, visa obter uma capacidade aprimorada de processamento de dados em tempo real, resultando em modelos mais precisos e rápidos.Ademais, um marco crítico no desenvolvimento deste projeto foi a superação dos desafios associados à integração efetiva de tecnologias de inteligência artificial, especificamente um modelo de Machine Learning avançado, dentro do ecossistema de sistemas bancários legados. A capacidade de desenvolver e validar um modelo de aprendizado de máquina capaz de realizar análises preditivas e comportamentais, processando volumes massivos de dados bancários em tempo real para a detecção de fraudes, constituiu um avanço significativo.Além disso, o desenvolvimento bem sucedido e a validação de práticas inovadoras de criptografia e anonimização de dados, adaptadas para trabalhar em conjunto com o modelo de IA, marcaram outro ponto de virada. Este avanço foi crítico ao demonstrar que é possível fortalecer a segurança dos dados sensíveis dos clientes e, ao mesmo tempo, utilizar a inteligência artificial para melhorar a autenticação de usuários e a detecção de fraudes. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Inteligência artificial,  biometria facial,  IA generativa,  ciência de dados,  machine learning,  visão computacional. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Os desenvolvimentos desta linha de pesquisa viabilizaram um conjunto de aprendizados.Foram organizados estudos buscando entendimentos para viabilizar construção de modelos de machine learning (ML) e ciência de dados para processar e analisar informações de clientes advindas de diversas fontes de dados. O intuito era de gerar insights personalizados e recomendações de produtos e serviços. Como elemento novo, entendeu-se a integração a partir do AWS Glue para orquestração dos dados.Foi possível comprovar que a implementação de modelos de ML avançados, combinados com técnicas de ciência de dados (multi-estrutural) e o poder de processamento do AWS Glue, poderia permitir uma análise detalhada e em tempo real das informações dos clientes, resultando em recomendações personalizadas, análises de perfil comportamental, definição de métricas e cálculos para definição de estratégias.Atividades experimentais permitiram entender a codificação de algoritmos de clustering, como K-means, para segmentar os clientes com base em comportamentos e preferências,  criação de modelos de regressão e árvores de decisão para prever comportamentos futuros dos clientes, como a probabilidade de churn ou a propensão para adquirir novos produtos,  desenvolvimento de mecanismos de recomendação utilizando técnicas de filtragem colaborativa e aprendizado profundo para sugerir produtos e serviços adaptados às necessidades individuais dos clientes.Com isso, houve um conjunto de constatações. Os modelos de regressão logística e árvores de decisão foram treinados para prever comportamentos futuros dos clientes, como a probabilidade de churn. O modelo de árvore de decisão alcançou uma precisão de 85% e uma pontuação F1 de 0.82, enquanto o modelo de regressão logística apresentou uma precisão de 80% e uma pontuação F1 de 0.78.Foram organizados experimentos visando comprovação de viabilidade dos recursos de IA para biometria facial. Houve a compreensão de mecanismos de autenticação biométrica multifatorial dinâmica que integrasse reconhecimento facial utilizando Redes Neurais Convolucionais (CNNs), juntamente com outros indicadores biométricos e comportamentais. Os entendimentos possibilitaram a construção de uma solução altamente adaptativa para diversos canais do banco, sendo agnóstica a todo o ecossistema da empresa.Estudos levaram ao entendimento de que a combinação de múltiplas formas de autenticação biométrica e comportamental, ajustadas dinamicamente de acordo com o contexto da transação e o perfil de risco do usuário, poderia oferecer uma segurança significativamente melhorada para transações bancárias, reduzindo a incidência de fraudes e melhorando a experiência do usuário. Para isso, foram organizados estudos que permitiram aprendizados para desenvolvimento de algoritmos para normalizar as entradas biométricas de diferentes canais. Para o treinamento de modelos, houve a compreensão da aplicação de um conjunto de dados diversificado para treinar modelos de CNN para reconhecimento facial, juntamente com modelos para outras modalidades biométricas. Além disso, como elemento novo, foi criado um sistema de pontuação de risco para avaliar o contexto da transação e ajustar dinamicamente os requisitos de autenticação.Foi possível constatar que a precisão combinada das modalidades biométricas supera as soluções de fator único ou duplo, atualmente em uso. Além disso, foi possível observar nos experimentos que a capacidade do sistema de ajustar os requisitos de autenticação com base no risco demonstra uma melhoria na segurança sem comprometer a conveniência.Experimentos levaram ao entendimento de como redistribuir valores de intensidade dos pixels de forma que o histograma da imagem resultante seja uniformemente distribuído, e como simular a incidência de luz sobre o rosto usando modelos físicos. Isso envolveu a aplicação de técnicas de renderização que ajustam artificialmente a iluminação da imagem, tentando replicar as condições de iluminação ideais. DESAFIO TECNOLÓGICO: Os principais desafios enfrentados pelos times de pesquisadores foram:Integrar dados heterogêneos, que variam em formato (estruturados, semi-estruturados, não estruturados) e fonte (transações on-line, interações de call center, logs de dispositivos móveis), em um formato unificado para análise. Foi considerada a hipótese de desenvolver uma integração de dados heterogêneos através de um middleware de integração e estruturando um gerenciador de APIs. Essa hipótese trazia o fato de que estruturar APIs sem configurações de segurança robustas, como encriptação de dados em trânsito e em repouso, autenticação forte e autorização com base em roles, poderia expor dados sensíveis a ataques de interceptação, Man-in-the-Middle (MitM) ou brechas de segurança em APIs públicas. Era preciso mensurar os riscos e definir todos os pontos de incerteza para que se chegasse em uma solução viável.Superar ataques de apresentação avançados (spoofing) e deepfakes: Os deepfakes utilizam redes neurais generativas adversariais (GANs) para criar imagens de faces extremamente realistas que poderiam enganar sistemas de biometria facial. Além disso, técnicas de spoofing como máscaras 3D, fotos de alta resolução e vídeos são usadas para tentar burlar os sistemas de segurança.Foi levantada a hipótese de abordar a detecção de ""vivacidade"" (liveness detection) que analisa micro-movimentos faciais e padrões de piscar, impossíveis de replicar em fotos ou vídeos. Isso poderia ser realizado através de algoritmos que capturariam e analisariam sequências de imagens em tempo real, com o intuito de identificar sutis movimentos naturais da face e da respiração. Foi pensada a hipótese de aplicar redes neurais convolucionais (CNNs) especialmente treinadas para distinguir entre faces reais e imagens sintéticas geradas por GANs. Essa hipótese envolveria o treinamento dessas redes com vastos conjuntos de dados compostos por imagens reais e deepfakes para aprimorar sua capacidade de detecção.Desafio de desenvolver um sistema de identificação capaz de analisar e processar imagens em baixa quantidade de luz. O time pensou na hipótese de codificar técnicas como a normalização de histograma, que poderia ajustar o contraste da imagem, e a transformação de iluminação baseada em modelos físicos que simulariam a incidência de luz sobre o rosto. Essas técnicas, em tese, poderiam ajudar a garantir que o sistema de reconhecimento facial possa operar eficientemente sob diferentes condições de iluminação.Levantou-se também a hipótese de levantar redes neurais siamesas que são treinadas não apenas para reconhecer as características faciais, mas também para entender e adaptar-se às variações na aparência de um mesmo indivíduo. Koshy, R., & Mahmood, A. (2020).Modelagem preditiva com dados esparsos. O desafio aqui era melhorar a precisão dos modelos de ML em face de dados esparsos, especialmente em segmentos de clientes com poucas interações ou transações. Uma possível solução seria explorar técnicas de aprendizado profundo, como redes neurais autoencoders, para identificar padrões latentes em dados esparsos. A combinação dessas técnicas com métodos de imputação sofisticados poderia melhorar a densidade dos dados. METODOLOGIA: Para superar as barreiras, foram organizadas as seguintes atividades experimentais:Estudo sobre Middleware de Integração e Gerenciamento de APIs: Uma investigação profunda foi realizada para desenvolver um middleware de integração capaz de unificar dados de formatos e fontes variadas. Além disso, foi realizado um estudo sobre a estruturação de um gerenciador de APIs que enfatizasse configurações de segurança robustas. Essas pesquisas abordaram riscos potenciais, como ataques de interceptação e brechas de segurança, propondo soluções para mitigá-los.Pesquisa em Detecção de ""Vivacidade"": Foi conduzida uma análise sobre o uso de algoritmos para capturar e analisar sequências de imagens, visando identificar micro-movimentos faciais e padrões de piscar. Este estudo explorou a viabilidade de implementar tais técnicas para fortalecer a segurança contra spoofing e deepfakes.Estudo sobre Redes Neurais Convolucionais (CNNs): Uma pesquisa extensiva foi realizada sobre o treinamento de CNNs com vastos conjuntos de dados, incluindo imagens reais e deepfakes, para aprimorar a capacidade de distinguir entre faces autênticas e sintéticas.Pesquisa em Técnicas de Ajuste de Contraste e Modelos de Iluminação: Estudos foram realizados sobre a aplicação de normalização de histograma e transformação de iluminação para melhorar a qualidade de imagens capturadas em condições de baixa luz, visando a eficácia do reconhecimento facial.Estudo sobre Redes Neurais Siamesas: Uma pesquisa abordou o desenvolvimento de redes neurais siamesas que reconhecem as características faciais e adaptam-se às variações na aparência, mesmo sob diferentes condições de iluminação.Pesquisa em Aprendizado Profundo e Técnicas de Imputação: Um estudo foi dedicado à exploração de redes neurais autoencoders e métodos sofisticados de imputação para identificar padrões latentes em dados esparsos. Essa pesquisa visou melhorar a densidade dos dados e, consequentemente, a precisão dos modelos de machine learning. INFORMAÇÃO COMPLEMENTAR: O desenvolvimento de mecanismos da biometria facial envolveu a coleta e processamento de grandes conjuntos de dados de imagens faciais, respeitando estritamente as regulamentações de privacidade e proteção de dados. O sistema foi treinado para distinguir efetivamente entre imagens autênticas e tentativas de spoofing, utilizando técnicas de detecção de vivacidade para prevenir acessos não autorizados através de fotos, vídeos ou máscaras.A infraestrutura de processamento de dados foi desenhada para suportar o processamento em tempo real, utilizando tecnologias como Amazon Kinesis e Apache Kafka, permitindo que os modelos de Machine Learning operassem com latência mínima. Isso possibilitou a entrega de recomendações e análises personalizadas quase instantaneamente, melhorando a experiência do usuário ao interagir com os serviços bancários. Além disso, a solução foi projetada para ser altamente escalável, utilizando orquestração de contêineres com Kubernetes e AWS ECS, facilitando a gestão de microserviços e a implementação de atualizações e melhorias contínuas.Para validar a eficácia e a robustez dos modelos de machine learning e ciência de dados desenvolvidos, foram realizadas extensas etapas de testes e prototipagem, envolvendo diversas equipes da empresa. Inicialmente, equipes de ciência de dados e engenharia de software colaboraram para criar protótipos dos algoritmos de clustering, regressão e árvores de decisão. Esses protótipos foram submetidos a rigorosos testes de desempenho e precisão, onde se constatou, por exemplo, que o modelo de árvore de decisão alcançou uma precisão de 85% e uma pontuação F1 de 0.82.Em seguida, equipes trabalharam em conjunto para testar e validar serviços de orquestração e processamento dos dados, garantindo que a integração dos diversos fluxos fosse eficiente e segura. Paralelamente, a equipe de segurança da informação conduziu experimentos para testar a viabilidade dos mecanismos de autenticação biométrica multifatorial, utilizando Redes Neurais Convolucionais (CNNs) e outros indicadores biométricos e comportamentais. Esses testes envolveram a participação de usuários internos que simularam diversas condições de uso, permitindo ajustar e refinar os algoritmos de reconhecimento facial e outras modalidades biométricas. A colaboração entre essas equipes foi crucial para identificar e resolver desafios técnicos, garantindo que os modelos fossem não apenas precisos, mas também seguros e escaláveis.REFERÊNCIAS BIBLIOGRÁFICAS:Vorobyev, I., & Krivitskaya, A. (2022). Reducing false positives in bank anti-fraud systems basedon rule induction in distributed tree-based models. Computers & Security, 120, 102786.Koshy, R., & Mahmood, A. (2020). Enhanced deep learning architectures for face liveness detection forstatic and video sequences. Entropy, 22(10), 1186.Aza, I., Munir, K., & Almutairi, M. (2022). Deepfake: A threat to cybersecurity and community safety.Applied Sciences, 12(19), 9820. RESULTADO ECONÔMICO: As atividades de inovação apresentadas resultaram na otimização dos processos de detecção de fraude diminuiu o tempo de análise em aproximadamente 40%, aumentando a eficiência e reduzindo a necessidade de recursos humanos e técnicos. RESULTADO INOVAÇÃO: Obteve-se avanços notáveis na segurança de dados, com a integração de criptografia ponta a ponta e gestão de tokens, melhora na proteção contra fraudes e evolução no processamento de dados sensíveis. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 11 ID ÚNICO: 109747 NOME: NOVAS TECNOLOGIAS PARA ESTRUTURAÇÃO DE COMPONENTES DE ATENDIMENTO E SEGURANÇA DOS DADOS DE CLIENTES DESCRIÇÃO: A presente linha de pesquisa busca a obtenção de novos conhecimentos quanto à segurança de dados e a melhoria da experiência de atendimento ao cliente para a empresa, focando no desenvolvimento de tecnologias de ponta como sistemas de autenticação multifatorial (MFA) e criptografia de ponta a ponta (E2EE). Além disso, visa integrar chatbots alimentados por inteligência artificial (IA) em ambiente cloud, com o objetivo de tornar a interação com o cliente mais eficiente e personalizada. Essas tecnologias prometem elevar a proteção contra ameaças digitais e aprimorar a qualidade do serviço ao cliente, atendendo às suas necessidades de maneira mais intuitiva e precisa.Para alcançar os objetivos propostos, o projeto delineia uma série de atividades experimentais, incluindo o desenvolvimento e teste de protótipos de MFA, a implementação de algoritmos de criptografia E2EE, e a criação e o treinamento de modelos de IA para chatbots, utilizando técnicas de processamento de linguagem natural (PLN). Essas atividades são essenciais para validar as soluções propostas, assegurando sua eficácia e segurança em proteger dados sensíveis e melhorar a interação com o usuário. Através desses esforços, o projeto busca gerar conhecimento avançado sobre a aplicação dessas tecnologias no contexto bancário, enfatizando a importância da inovação contínua para enfrentar desafios de segurança e atendimento ao cliente. Moepi, G. L., & Mathonsi, T. E. (2021, December)O resultado esperado deste projeto é a formulação de respostas precisas e personalizadas às consultas dos usuários, melhorando significativamente a proteção de dados sensíveis e a experiência do cliente na empresa. Espera-se que as inovações implementadas, particularmente os avanços em MFA, criptografia E2EE e a integração de chatbots de IA, configurem um ambiente bancário mais seguro contra violações de dados, e proporcionem um serviço de atendimento ao cliente mais intuitivo e eficiente.Um ponto crítico neste projeto de inovação tecnológica foi a implementação bem-sucedida e a integração operacional de um sistema de autenticação multifatorial (MFA) robusto, combinado com a criptografia de ponta a ponta (E2EE), em ambiente de produção da empresa. Esse marco foi crítico, pois demonstrou a viabilidade de elevar significativamente a segurança dos dados dos clientes, ao mesmo tempo em que se mantém uma experiência de usuário fluida e sem interrupções. A superação dos desafios técnicos e operacionais para harmonizar essas tecnologias de segurança com os sistemas legados do banco, sem comprometer a usabilidade ou a performance, representou um avanço notável.Além disso, a criação, o treinamento e a integração bem-sucedidos de chatbots alimentados por inteligência artificial (IA) em ambiente cloud, utilizando técnicas avançadas de processamento de linguagem natural (PLN), constituiu outro marco crítico deste projeto. Este avanço foi crítico ao transformar a interação do banco com seus clientes, tornando-a mais eficiente, personalizada e intuitiva. A capacidade dos chatbots de IA de entender e responder às consultas dos usuários em tempo real, com respostas precisas e personalizadas, enquanto garantem a proteção de dados sensíveis, possibilitou um avanço significativa na experiência de atendimento ao cliente. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Autenticação,  integração,  experimentos,  chaves,  dados,  chatbots,  multifatorial,  infraestrutura,  resiliência. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: São elementos de novidade nesta linha de pesquisa: A compreensão de que era possível integrar chamadas de API para serviços de verificação de identidade, utilizando SDKs de autenticação biométrica para dispositivos móveis por meio da construção de um módulo de Autenticação Multifatorial (MFA). Isso foi possível por meio da execução de experimentos, que até constataram uma ligeira latência adicional no processo de autenticação devido às múltiplas etapas de verificação, mas sem comprometer significativamente a experiência do usuário.Foram simuladas 10.000 tentativas de login simultâneas para medir latência e desempenho, resultando em tempos médios de resposta de 100ms para verificação de credenciais, 150ms para impressão digital e 220ms para reconhecimento facial, com uma taxa de sucesso de 98% dentro de 1 segundo, constatando que o sistema suportava alta carga com eficiência. Foram comparadas ainda 20.000 imagens faciais e 20.000 impressões digitais para avaliar precisão e confiabilidade, obtendo-se taxas de falsos positivos de 0.5% para reconhecimento facial e 0.3% para impressão digital, e taxas de falsos negativos de 1% e 0.7%, respectivamente, demonstrando alta precisão e confiabilidade. Testes de penetração e simulações de falhas demonstraram que o sistema bloqueou tentativas excessivas de login após 5 tentativas falhas, nenhuma tentativa de spoofing foi bem-sucedida, e o sistema manteve funcionalidade reduzida durante falhas de componentes, graças à arquitetura distribuída e redundância, evidenciando robustez e resiliência.Comprovação de viabilidade do desenvolvimento de E2EE nas comunicações entre o cliente e os servidores do banco usando protocolos TLS (Transport Layer Security) para a camada de transporte e algoritmos AES (Advanced Encryption Standard) com chaves de 256 bits para criptografar mensagens no lado do cliente antes de serem enviadas. Isso possibilita o desenvolvimento de mecanismos de troca de chaves segura, empregando o protocolo Diffie-Hellman para a geração de chaves compartilhadas sem a necessidade de um canal seguro prévio.A execução de experimentos com o EventBridge para orquestrar eventos entre serviços na cloud trouxe o entendimento de que é possível estruturar um modelo de comunicação assíncrona e acoplamento frouxo entre os componentes do sistema de atendimento, por meio da configuração de regras de eventos que disparam funções Lambda. Essas por sua vez, executam lógicas de negócios específicas, como a iniciação de sessões de chatbot.Para melhorar a precisão na identificação de intenções dos usuários por Chatbots de IA, realizou-se um experimento utilizando tecnologias avançadas de aprendizado profundo e Processamento de Linguagem Natural (NLP). Inicialmente, coletou-se um vasto conjunto de dados de interações reais com usuários, abrangendo diversas expressões linguísticas e contextos culturais. Após a coleta, os dados foram pré-processados para remover ruídos e padronizar os formatos de texto.Modelos de transformadores BERT (Bidirectional Encoder Representations from Transformers) e GPT (Generative Pre-trained Transformer), foram treinados devido à sua capacidade de compreender contextos bidirecionais e gerar texto coerente. Adicionalmente, Redes Neurais Recorrentes (RNNs) com LSTMs (Long Short-Term Memory) foram aplicadas para analisar sequências de texto e prever intenções baseadas em contextos anteriores.Foram desenvolvidas técnicas de análise de sentimento utilizando ferramentas como VADER (Valence Aware Dictionary and sEntiment Reasoner) para entender as emoções subjacentes às mensagens dos usuários. Algoritmos de clusterização foram utilizados para agrupar mensagens semelhantes, facilitando a identificação de padrões de intenção. Os resultados preliminares mostraram uma melhoria significativa na precisão da identificação de intenções, com BERT e GPT demonstrando excelente desempenho na compreensão de contextos complexos e geração de respostas relevantes. DESAFIO TECNOLÓGICO: Durante o desenvolvimento dos experimentos com o MFA, os pesquisadores enfrentaram dois grandes desafios. O primeiro foi a latência na comunicação entre os componentes distribuídos do sistema, especialmente durante a verificação de dados biométricos em tempo real. A hipótese era que a configuração de uma rede de baixa latência e o uso de técnicas de otimização de rede, como caching distribuído, poderiam reduzir significativamente os atrasos. No entanto, a complexidade de criar e manter essas otimizações em um ambiente de nuvem pública representava um risco considerável, pois exigiria um monitoramento contínuo e ajustes dinâmicos.O segundo desafio foi a precisão do reconhecimento facial em condições de baixa iluminação e variações faciais, como uso de acessórios ou mudanças na expressão. A hipótese para resolver esse problema envolvia o treinamento de modelos de reconhecimento facial mais robustos utilizando redes neurais convolucionais (CNNs) e um conjunto de dados diversificado. No entanto, essa abordagem exigia um nível avançado de engenharia de dados, incluindo a coleta e rotulação de um grande volume de dados variáveis, além de ajustes finos nos hiperparâmetros dos modelos. A dificuldade técnica em garantir a qualidade e a diversidade dos dados de treinamento, bem como a necessidade de ajustar continuamente os modelos para manter a precisão, representava um risco significativo para a viabilidade técnica da solução.Configurou-se um grande desafio gerenciar chaves criptográficas de forma segura, especialmente no ambiente distribuído. A troca de chaves segura sem comprometer a facilidade de uso para o usuário final ou a segurança dos dados também era um ponto de incerteza, dado a complexidade técnica.Para esse problema, o time pensou na hipótese de adotar um sistema de gerenciamento de chaves (KMS) que suportasse rotação automática de chaves e armazenamento seguro. Foi pensada também a alternativa de desenvolvimento do protocolo Diffie-Hellman para a troca de chaves, sendo necessário explorar mais esse ponto para ter certeza, dado que reside uma complexidade em manter a infraestrutura segura contra ataques de intermediários.A ambiguidade linguística, com palavras e frases possuindo múltiplos significados contextuais, dificultou a precisão dos modelos de NLP, especialmente em diálogos curtos. As nuances culturais, incluindo expressões idiomáticas e gírias, variaram amplamente, complicando a interpretação correta pelos modelos de IA. Nah, F. F.-H., Zheng, R., Cai, J., Siau, K., & Chen, L. (2023).Além disso, as limitações computacionais representaram um desafio significativo, dado que o treinamento de modelos avançados de NLP é intensivo em recursos e requer hardware especializado.Uma possível hipótese de solução poderia envolver a integração de técnicas de aprendizado por reforço, ajustando os modelos com base no feedback dos usuários em tempo real. Outra abordagem seria a utilização de modelos híbridos que combinam transformadores com técnicas baseadas em regras culturais e linguísticas específicas. No entanto, essas soluções apresentam incertezas em termos de viabilidade computacional e precisão, exigindo rigorosos testes e investimentos significativos.Manter a arquitetura baseada em eventos escalável e resiliente diante de picos de demanda inesperados representou um desafio técnico significativo. A complexidade aumenta com a integração de múltiplos serviços AWS e a necessidade de garantir a entrega de eventos em tempo real sem perda de dados.Dado o problema, foi pensada a hipótese de desenvolver serviços na cloud como o SQS para filas de mensagens e o a AWS SNS para notificações, que poderia facilitar o gerenciamento de picos de carga. A criação de padrões de design como circuit breaker poderia ajudar a aumentar a resiliência. Ainda assim, a configuração ideal que garanta escalabilidade e resiliência sob todas as condições possíveis demandava contínua experimentação para mais comprovações. METODOLOGIA: APIs internas foram expostas e integradas com funções serverless para permitir comunicação bidirecional entre os diferentes componentes do sistema. Desenvolveu-se um pipeline de captura de imagens faciais utilizando câmeras de alta resolução, e os sensores de impressão digital foram integrados com estações de trabalho para capturar dados biométricos.Tanto as imagens faciais quanto os dados de impressões digitais foram enviados para funções serverless que processaram e compararam esses dados com os armazenados no banco de dados. O fluxo de autenticação multifatorial foi gerenciado por serviços de identidade na nuvem, com verificação de credenciais, impressão digital e reconhecimento facial em tempo real.Estudo e experimentação com algoritmos de cache seguros para tokens de autenticação: este trabalho envolveu o desenvolvimento de algoritmos de cache que garantem a segurança dos tokens de autenticação. Os experimentos realizados incluíram testes de penetração para avaliar a resistência desses algoritmos contra ataques, bem como análises de performance para verificar a eficácia do cache em acelerar o processo de autenticação sem comprometer a segurança.Para abordar a complexidade na compreensão de intenções pelos Chatbots de IA, dados coletados foram submetidos a um rigoroso processo de pré-processamento para remover ruídos, como erros tipográficos e informações irrelevantes, e para padronizar os formatos de texto, garantindo consistência. Em seguida, treinou-se os modelos utilizando transformadores, como BERT e GPT, devido à sua capacidade de captar nuances contextuais e semânticas da linguagem. Adicionalmente, aplicaram-se Redes Neurais Recorrentes (RNNs) com LSTMs para analisar sequências de texto e prever intenções baseadas em contextos anteriores.Implementaram-se técnicas de análise de sentimento para entender as emoções subjacentes às mensagens dos usuários e algoritmos de clusterização para agrupar mensagens semelhantes. Para avaliar a precisão dos modelos, utilizaram-se técnicas de validação cruzada, ajustando-se os hiperparâmetros conforme necessário. Um sistema de feedback contínuo foi implementado, onde as respostas dos chatbots foram monitoradas e avaliadas pelos usuários, permitindo ajustes e refinamentos constantes nos modelos. Periodicamente, os modelos foram atualizados com novos dados coletados e feedback dos usuários para garantir que o chatbot permanecesse relevante e eficaz.Foram realizados também estudos de caso práticos, incluindo a implementação de circuit breakers e bulkheads em sistemas de teste, para avaliar o impacto desses padrões na resiliência e escalabilidade de sistemas distribuídos sob alta demanda.Estudos para otimização de algoritmos de reconhecimento biométrico, que teve por objetivo reduzir a latência induzida por sistemas de autenticação multifator (MFA) com foco em métodos biométricos.Isso envolveu a criação e teste de novos algoritmos de reconhecimento biométrico focados em eficiência e velocidade,  comparação dos tempos de resposta dos novos algoritmos com os antigos em diversos dispositivos e condições de uso,  testes de cache para tokens de autenticação, avaliando o impacto na velocidade e segurança, com o intuito de obter uma redução significativa no tempo de processamento da autenticação biométrica, mantendo ou aumentando os níveis de segurança.Estudos e experimentações do protocolo Diffie-Hellman Modificado. Isso envolveu a exploração da viabilidade de um protocolo Diffie-Hellman modificado para a troca segura de chaves em um ambiente distribuído.Inicialmente foi realizada a modificação do protocolo Diffie-Hellman para aumentar a segurança contra ataques de intermediários,  depois a execução de simulações de ataque para testar a robustez do protocolo modificado,  em seguida foi feita a aplicação do protocolo em um ambiente de teste para avaliar a usabilidade e segurança, com o intuito de melhorar a eficiência e resistência a ataques de intermediários. INFORMAÇÃO COMPLEMENTAR: Com base nos estudos e experimentos realizados pelos pesquisadores envolvidos, evidenciou-se que a integração das tecnologias mencionadas, juntamente com a implementação de estratégias de autenticação avançadas como Autenticação Multifatorial (MFA) e autenticação de ponta a ponta, oferece à empresa uma infraestrutura robusta e segura, capaz de lidar com as exigências operacionais e de segurança do ramo bancário.Observa-se que a experimentação do API Gateway como ponto de entrada para as solicitações dos chatbots de IA permite um gerenciamento eficaz e seguro dessas interações, garantindo que apenas solicitações autenticadas sejam processadas, enquanto o AWS Lambda, operando em uma arquitetura sem servidor, assegura uma resposta dinâmica às variações de demanda sem a necessidade de intervenção manual para escalonamento de recursos. Esta abordagem não apenas maximiza a eficiência operacional, mas também minimiza os custos relacionados à manutenção de infraestrutura física.Além disso, a adoção de MFA e autenticação de ponta a ponta introduz camadas adicionais de proteção, essenciais para a salvaguarda de transações e dados sensíveis. A MFA adiciona uma barreira significativa contra acessos não autorizados, exigindo a verificação de identidade por meio de múltiplos fatores independentes, enquanto a autenticação de ponta a ponta garante que todas as comunicações entre o cliente e o sistema sejam criptografadas, protegendo-as contra interceptações maliciosas.O desenvolvimento do CloudWatch para o monitoramento contínuo do desempenho dos chatbots proporciona uma capacidade de observação e ajuste em tempo real, facilitando a detecção e correção de problemas operacionais de forma ágil, o que é crucial para a manutenção da estabilidade e confiabilidade do sistema em um ambiente tão crítico quanto o bancário.REFERÊNCIAS BIBLIOGRÁFICAS:Nah, F. F.-H., Zheng, R., Cai, J., Siau, K., & Chen, L. (2023). Generative AI and ChatGPT:Applications, challenges, and AI-human collaboration. Journal of Organizational Computingand Electronic Commerce, 33(4), 277-304.Moepi, G. L., & Mathonsi, T. E. (2021, December). Multi-Factor Authentication Method for Online Banking Services in South Africa. In 2021 International Conference on Electrical, Computer and Energy Technologies (ICECET) (pp. 1-5). IEEE. RESULTADO ECONÔMICO: Maiores níveis de satisfação do cliente, alinhado ao aprimoramento do atendimento, o que se converte em retenção de clientes e captação de mais oportunidades e core para o banco. RESULTADO INOVAÇÃO: Obtenção de novos conhecimentos a respeito de tecnologias como ML e PLN,  evolução de soluções de atendimento com obtenção de novos aprendizados em segurança da informação e dados sensíveis de clientes. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;"NÚMERO: 12 ID ÚNICO: 109748 NOME: SOLUÇÕES PARA PREVENÇÃO A FRAUDE E SISTEMAS DE AUTENTICAÇÃO PARA PLATAFORMA BANCÁRIA AGNÓSTICA AO BANCO DESCRIÇÃO: O projeto teve como objetivo principal avaliar a eficácia de um sistema de autenticação multifatorial (MFA) avançado, integrando tecnologias de reconhecimento biométrico, OTP via SMS, tokens de hardware e algoritmos de inteligência artificial (IA) para a detecção de comportamentos fraudulentos, visando aumentar a segurança nas transações e operações de login em plataformas bancárias. Antes da implementação deste sistema, as plataformas dependiam de métodos de autenticação menos sofisticados, o que deixava margem para uma maior ocorrência de fraudes e acessos não autorizados. Com a integração do MFA e algoritmos de IA, esperava-se uma redução significativa nessas incidências, melhorando assim a segurança e a confiança nas operações bancárias online. Sinigaglia, F., Carbone, R., Costa, G., & Zannone, N. (2020).Durante o desenvolvimento do projeto, foram realizadas análises comparativas entre usuários de um sistema de autenticação de fator único e aqueles utilizando o sistema MFA proposto. Os resultados obtidos mostraram uma diminuição considerável nas tentativas bem-sucedidas de fraude e nos acessos não autorizados no grupo que utilizou o MFA, confirmando a eficácia da abordagem multifatorial. Ajustes foram feitos nos algoritmos de IA para otimizar a detecção de fraudes e minimizar falsos positivos, melhorando a experiência do usuário sem comprometer a segurança. Além disso, foram realizadas adaptações nos métodos de autenticação e na interface de usuário, visando um melhor equilíbrio entre segurança e usabilidade.Após os desenvolvimentos e ajustes, o projeto demonstrou a viabilidade e os benefícios de integrar um sistema de autenticação multifatorial avançado em plataformas bancárias. A implementação de tecnologias biométricas, juntamente com a aplicação de algoritmos de IA para análise comportamental, provou ser uma estratégia eficaz na prevenção de fraudes e acessos não autorizados.A demonstração bem-sucedida da eficácia do sistema de autenticação multifatorial (MFA) avançado, que integra tecnologias de reconhecimento biométrico, OTP (One-Time Password) via SMS, tokens de hardware e algoritmos de inteligência artificial para a detecção de comportamentos fraudulentos em plataformas bancárias pode ser considerado um ponto chave nas atividades de desenvolvimento deste projeto. Esse marco foi alcançado após a conclusão de um estudo comparativo abrangente, que revelou uma diminuição significativa nas incidências de fraudes e acessos não autorizados entre os usuários que adotaram o sistema MFA em comparação com aqueles que dependiam de métodos de autenticação menos sofisticados.A realização de ajustes finos nos algoritmos de IA para aprimorar a detecção de atividades fraudulentas e a redução de falsos positivos, juntamente com as adaptações na interface de usuário para facilitar a usabilidade, representou um avanço significativo. Esse resultado comprovou a viabilidade do desenvolvimento do sistema MFA proposto sobre métodos de autenticação, além de fortalecer a segurança e conveniência para as operações bancárias online. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Autenticação,  MFA (Autenticação Multifatorial),  IA,  fraude,  segurança,  algoritmos,  reconhecimento biométrico,  OTP via SMS,  tokens de hardware. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: São elementos tecnologicamente inovadores deste projeto:Experimentos foram conduzidos com o objetivo de obter insumos para a criação de uma solução de middleware eficiente, que fosse capaz de garantir a segurança cibernética. Um dos experimentos envolveu a simulação de ataques de repetição e man-in-the-middle contra diversas configurações de autenticação. Observou-se que a vulnerabilidade diminuía significativamente ao utilizar-se uma combinação inédita de TLS (Transport Layer Security) com autenticação baseada em tokens de hardware. Esta combinação possibilitou a obtenção de novos conhecimentos: observou-se uma redução substancial nas vulnerabilidades a ataques de repetição e man-in-the-middle (MitM). No caso dos ataques de repetição, os tokens de hardware, que incorporam elementos temporais ou de uso único, impediram eficazmente a reutilização de mensagens interceptadas. Em relação aos ataques MitM, a integração de TLS com tokens de hardware tornou extremamente difícil a inserção de atacantes na comunicação, garantindo a detecção imediata de qualquer tentativa de interceptação ou alteração.Além disso, esses conhecimentos possibilitaram o desenvolvimento de um protocolo de comunicação personalizado. Este protocolo integra criptografia de ponta a ponta, assegurando que os dados permaneçam criptografados durante todo o trajeto, do emissor ao receptor. Também implementa um sistema de autenticação baseado em desafio-resposta, onde um desafio é enviado e deve ser corretamente respondido, aumentando a segurança em relação ao uso de senhas estáticas.Além disso, no desenvolvimento do sistema de pontuação de risco dinâmico, empregou-se uma abordagem customizada na análise de dados para extrair padrões associados a transações fraudulentas. Utilizando uma combinação avançada de técnicas de aprendizado de máquina, como redes neurais convolucionais (CNNs) e máquinas de vetores de suporte (SVMs), analisaram-se milhões de transações. Um dos avanços significativos foi a compreensão de como realizar a aplicação de Análise de Componentes Principais (PCA) para a redução da dimensionalidade dos dados. Esse entendimento permitiu descobrir características latentes fortemente indicativas de fraude. Essa descoberta foi fundamental para o desenvolvimento de um modelo híbrido, que combinou CNNs para detecção de padrões complexos e SVMs para classificação baseada nas características latentes, permitindo a identificação e a prevenção proativa de fraudes.Ao aplicar técnicas de explicabilidade em modelos de ML complexos, foram realizados experimentos para avaliar a eficácia de diversos métodos na identificação de características importantes. Um desses experimentos envolveu o emprego de SHAP (SHapley Additive exPlanations) para decompor as previsões de uma rede neural profunda treinada para detectar fraudes, comparando os resultados com aqueles obtidos por meio do uso de LIME (Local Interpretable Model-agnostic Explanations). Observou-se que SHAP fornecia uma visão mais coerente e abrangente das contribuições das características, facilitando o ajuste preciso do modelo para reduzir a taxa de falsos positivos sem comprometer a eficácia na detecção de fraudes. Baesens, B., Höppner, S., & Verdonck, T. (2021).Em relação às abordagens híbridas de modelagem, foram realizados experimentos comparando o desempenho de modelos complexos, como redes neurais profundas, com modelos mais simples, como árvores de decisão, em conjuntos de dados de transações bancárias. Um dos experimentos mais significativos envolveu a implementação de um sistema de votação ensemble, onde as previsões de múltiplos modelos simples eram combinadas para gerar uma previsão final. Esse sistema foi comparado com o desempenho de um único modelo complexo, revelando que, embora o modelo complexo apresentasse uma precisão ligeiramente superior, o sistema de votação ensemble oferecia melhor interpretabilidade, facilidade de manutenção e maior resiliência a mudanças nos padrões de dados. DESAFIO TECNOLÓGICO: Um dos principais obstáculos da linha desta linha de pesquisa foi a integração harmoniosa de tecnologias de reconhecimento biométrico, OTP via SMS e tokens de hardware em um ambiente bancário agnóstico. Esse desafio exigiu uma complexa coordenação de sistemas e protocolos de segurança para assegurar a interoperabilidade e eficácia na autenticação. Além disso, a calibração dos algoritmos de IA, a fim de identificar comportamentos fraudulentos com precisão, sem gerar um número excessivo de falsos positivos, representou um desafio técnico significativo, afetando diretamente a experiência do usuário e a eficiência operacional do sistema.Levantou-se a hipótese de adotar frameworks de desenvolvimento e APIs de segurança padronizadas que facilitassem a integração de diferentes tecnologias de autenticação. No entanto, a eficácia dessa abordagem permaneceu incerta, dada a diversidade de dispositivos e sistemas operacionais utilizados pelos clientes. Paralelamente, a codificação de técnicas de aprendizado de máquina mais sofisticadas e a alimentação constante dos algoritmos de IA com novos dados de transações poderiam, em teoria, melhorar a detecção de fraudes, minimizando os falsos positivos.Um dos obstáculos mais notáveis foi encontrar o equilíbrio certo entre segurança aprimorada e usabilidade. Embora o sistema MFA tenha demonstrado eficácia na redução de acessos não autorizados e tentativas de fraude, a complexidade adicional na experiência de autenticação gerou feedback negativo dos usuários, indicando uma barreira potencial à adoção plena do sistema. Além disso, a implementação eficiente de múltiplas formas de verificação sem causar atrasos significativos no acesso dos usuários legítimos apresentou-se como um desafio, dada a necessidade de processar e validar cada forma de autenticação de maneira rápida e segura.Para a autenticação multifatorial, foi considerada a possibilidade de integrar frameworks e serviços especializados em segurança e autenticação, visando simplificar a implementação de tecnologias biométricas ao mesmo tempo em que se mantinha a conformidade com regulamentações de privacidade. A ideia de utilizar Inteligência Artificial e Machine Learning, por meio do SageMaker, para análise comportamental em tempo real e identificação de padrões de uso suspeitos foi vista como uma potencial adição de segurança.A experimentação de modelos de ML complexos e ""caixas-pretas"", como redes neurais profundas, pode oferecer um alto desempenho na detecção de fraudes, mas muitas vezes à custa da interpretabilidade. Isso representa um desafio significativo quando é necessário explicar a base de uma determinada decisão a clientes ou a reguladores, especialmente em casos de disputa. Além disso, a conformidade com regulamentações que exigem transparência nas decisões automatizadas, impõe a necessidade de modelos que possam fornecer insights claros sobre seus processos de tomada de decisão. Baesens, B., Höppner, S., & Verdonck, T. (2021).Para abordar essa complexidade, a equipe ponderou a adoção de técnicas de interpretabilidade e explicabilidade de modelos, como LIME (Local Interpretable Model-agnostic Explanations) e SHAP (SHapley Additive exPlanations). Essas técnicas visam decompor as previsões de modelos complexos em contribuições compreensíveis de cada característica, proporcionando insights sobre como diferentes atributos influenciam as decisões do modelo.Além disso, foi considerada a possibilidade de implementar abordagens híbridas, como a combinação de CNN + árvore de decisão, cruzando modelos de alta precisão, mas de difícil interpretação, com modelos mais simples e explicáveis, utilizando, por exemplo, árvores de decisão ou modelos lineares para as camadas de decisão finais. Contudo, observou-se que a integração dessas técnicas e abordagens poderia apresentar desafios próprios, incluindo a complexidade adicional no treinamento e na manutenção dos modelos. METODOLOGIA: Pesquisa sobre Frameworks de Desenvolvimento e APIs de Segurança: Foram estudados e avaliados frameworks e APIs que facilitaram a integração de diversas tecnologias de autenticação, como reconhecimento biométrico, OTP via SMS e tokens de hardware, em um ambiente bancário.Experimentação com Técnicas de Aprendizado de Máquina Avançadas: Técnicas de machine learning sofisticadas foram implementadas e testadas, alimentando constantemente os algoritmos de IA com novos dados de transações para aprimorar a detecção de fraudes e minimizar falsos positivos.Avaliação de Usabilidade versus Segurança: Estudos de usuário foram realizados para encontrar o equilíbrio certo entre segurança aprimorada e usabilidade, considerando a complexidade na experiência de autenticação e o feedback dos usuários.Desenvolvimento de Métodos de Verificação Rápida e Segura: Foram pesquisados e testados métodos para processar e validar múltiplas formas de autenticação de maneira rápida e segura, sem causar atrasos significativos no acesso dos usuários legítimos.A integração de tecnologias biométricas, mantendo a conformidade com as regulamentações de privacidade, foi explorada por meio de frameworks e serviços especializados em segurança e autenticação.Uso de Inteligência Artificial para Análise Comportamental: O uso de IA e machine learning, por meio de ferramentas como SageMaker, para análise comportamental em tempo real e identificação de padrões de uso suspeitos foi considerado como uma adição de segurança.Exploração de Técnicas de Interpretabilidade e Explicabilidade de Modelos: Técnicas como LIME e SHAP foram estudadas e aplicadas para tornar as decisões de modelos de ML complexos mais interpretáveis e explicáveis, proporcionando insights sobre como diferentes atributos influenciam as decisões do modelo.Implementação de Abordagens Híbridas para Modelagem: A combinação de modelos de alta precisão, mas de difícil interpretação, com modelos mais simples e explicáveis, como árvores de decisão ou modelos lineares, para as camadas de decisão finais foi testada.Avaliação da Complexidade no Treinamento e Manutenção dos Modelos: Os desafios relacionados à complexidade adicional no treinamento e na manutenção dos modelos decorrentes da integração de técnicas de interpretabilidade e explicabilidade foram analisados.Estudo sobre a Aceitação de Explicações por Reguladores e Usuários Finais: Foi investigado como as explicações geradas pelos modelos foram recebidas por reguladores e usuários finais, considerando as incertezas quanto à aceitação dessas explicações. INFORMAÇÃO COMPLEMENTAR: O desenvolvimento de um sistema de autenticação multifatorial avançado, aliado à inteligência artificial, trouxe benefícios tangíveis não apenas para a segurança das operações em plataformas bancárias, mas também para a experiência geral do usuário. A utilização de reconhecimento biométrico, OTP via SMS e tokens de hardware, combinada com a capacidade analítica dos algoritmos de IA, resultou em um processo de autenticação mais robusto e personalizado. Isso não só dificultou ações fraudulentas, mas também aprimorou a capacidade dos sistemas em oferecer uma experiência de uso fluida e segura, ajustando-se automaticamente às necessidades e comportamentos dos usuários. Tal desenvolvimento tecnológico representa um avanço significativo na forma como as instituições financeiras abordam a segurança e a usabilidade, promovendo um ambiente online mais seguro e confiável.Além disso, a otimização dos processos de autenticação e a eficiência na detecção de tentativas de fraude têm um impacto positivo direto na operacionalidade das instituições financeiras. A redução de custos associados a fraudes e a melhoria na eficiência operacional são claros indicadores do valor agregado por essas tecnologias. Adicionalmente, a adaptação e integração de serviços de cloud computing, como AWS Cognito e Lambda, na gestão de identidades e autenticações, e o uso de SageMaker para o desenvolvimento de modelos de machine learning, ressaltam a importância da flexibilidade e da escalabilidade dos sistemas de segurança.Além disso, nos experimentos realizados para aprimorar a segurança das transações através de autenticação multifatorial, foi explorada a eficácia da biometria comportamental combinada com a autenticação de dois fatores (2FA). Este estudo envolveu a análise de padrões comportamentais, que são únicos para cada indivíduo. A integração desses dados comportamentais com métodos tradicionais de 2FA, como SMS ou tokens de autenticação de aplicativos, resultou em um sistema de autenticação robusto que diminuiu ainda mais a possibilidade de acesso não autorizado.É importante salientar também que diversas equipes conduziram experimentos e testagens para validar a eficácia e robustez dos sistemas propostos. A equipe de segurança cibernética simulou ataques de repetição e man-in-the-middle, descobrindo que a combinação de TLS com autenticação baseada em tokens de hardware reduzia vulnerabilidades em 70%, resultando em um protocolo de comunicação personalizado com criptografia de ponta a ponta e autenticação baseada em desafio-resposta, que melhorou o tempo de resposta a ameaças em 40%.A equipe de ciência de dados desenvolveu um sistema de pontuação de risco dinâmico utilizando redes neurais convolucionais (CNNs) e máquinas de vetores de suporte (SVMs). A aplicação de PCA reduziu a dimensionalidade dos dados em 50%, aumentando a precisão da detecção de fraudes em 15%, com uma taxa de detecção de 92% e redução de falsos positivos para 5%. Técnicas de explicabilidade como SHAP e LIME foram empregadas para ajustar modelos de ML complexos, com SHAP reduzindo a taxa de falsos positivos em 10% sem comprometer a eficácia.A equipe de desenvolvimento comparou sistemas de votação ensemble com modelos complexos, constatando que o ensemble oferecia uma precisão de 88% e maior resiliência de manutenção em comparação com a precisão de 90% do modelo complexo. Adicionalmente, a equipe de autenticação explorou a biometria comportamental combinada com autenticação de dois fatores (2FA), resultando em um sistema que reduziu a possibilidade de acesso não autorizado em 25%.REFERÊNCIAS BIBLIOGRÁGICAS:Sinigaglia, F., Carbone, R., Costa, G., & Zannone, N. (2020). A survey on multi-factorauthentication for online banking in the wild. Computers & Security, 95, 101745.https://doi.org/10.1016/j.cose.2020.101745Baesens, B., Höppner, S., & Verdonck, T. (2021). Data engineering for fraud detection. DecisionSupport Systems, 142, 113492. https://doi.org/10.1016/j.dss.2021.113492 RESULTADO ECONÔMICO: A redução significativa de fraudes e acessos não autorizados diminuiu prejuízos financeiros, elevando a confiança dos usuários e potencializando a economia operacional. RESULTADO INOVAÇÃO: A integração eficaz de autenticação multifatorial e IA aprimorou a detecção de comportamentos fraudulentos, elevando padrões de segurança e usabilidade das soluções. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;"NÚMERO: 13 ID ÚNICO: 109749 NOME: EXPERIMENTAÇÃO DE JORNADA AGNÓSTICA PARA UNIFICAÇÃO DE SERVIÇOS BANCÁRIOS EM UMA PLATAFORMA ÚNICA DESCRIÇÃO: Menos de um terço dos maiores bancos do mundo estão investindo em ecossistemas financeiros digitais de uma “maneira significativa”. Uma pesquisa diz que cerca de 25% dessas organizações estão investindo em testes piloto de sistemas bancários centrais. (MIT Technology Review Insights, & Boston Consulting Group, (2023).Diante disso, a presente linha de pesquisa propõe o desenvolvimento de mecanismos que viabilizem a modernização de serviços estruturais do banco, com ênfase na integração eficaz de sistemas mainframe antigos com as tecnologias de cloud computing modernas para evolução na entrega de serviços ao cliente. Especificamente, procurou-se desenvolver um robusto mecanismo de serialização e deserialização para facilitar a comunicação entre os sistemas legados e as plataformas modernas, otimizar o armazenamento e a performance na cloud, reforçar a segurança dos dados e gerenciar eficientemente as transações e falhas entre microserviços.Foi trabalhada uma abordagem experimental para testar a viabilidade técnica de alguns componentes a serem aplicados futuramente na infraestrutura da empresa. Foram desenvolvidos e testados protótipos de algoritmos de serialização e deserialização personalizados para converter dados de EBCDIC para UTF-8, utilizando técnicas de mapeamento de caracteres e normalização de dados. Estruturas aninhadas em JSON/XML foram prototipadas para representar campos compostos e registros hierárquicos dos mainframes utilizando esquemas JSON Schema e XML Schema para validação. Esses protótipos foram validados através de testes com conjuntos de dados representativos para verificar a integridade e precisão dos dados convertidos.Explorou-se o uso de Elastic Block Store (EBS) para otimizar o armazenamento na cloud e a performance, criando protótipos que foram submetidos a testes de carga e desempenho. Além disso, tecnologias de emulação de terminais foram experimentadas para garantir a compatibilidade com aplicações COBOL, avaliando a eficácia dessas soluções em ambientes controlados.A segurança foi significativamente reforçada através de experimentações com AWS IAM e AWS KMS para o gerenciamento de identidades, acessos e chaves de criptografia. Foram realizados testes de segurança utilizando ferramentas como OpenVAS e Nessus para identificar possíveis vetores de ataque. A viabilidade da atualização para os protocolos TLS 1.2 e 1.3 foi testada através de protótipos que passaram por rigorosos testes de desempenho e segurança com as ferramentas como JMeter, Gatling e SSL Labs, garantindo que a configuração mitigava vulnerabilidades sem introduzir novos vetores de ataque.Para gerenciar transações e falhas entre microserviços, foi desenvolvido um protótipo de sistema de ""sagas"". Este protótipo foi testado em condições normais e de falha utilizando ferramentas de simulação como Chaos Monkey, assegurando que as sagas mantinham a integridade dos dados mesmo em operações complexas e distribuídas. A coordenação entre microserviços foi experimentada com orquestradores de sagas como Netflix Conductor e Axon Framework.O marco crítico deste projeto foi a necessidade de desenvolver e testar protótipos de mecanismos de serialização e deserialização, junto com a introdução de APIs agnósticas para a integração entre sistemas mainframe legados e tecnologias de cloud computing. Esses desenvolvimentos experimentais possibilitaram uma comunicação eficaz entre as diferentes arquiteturas tecnológicas, superando os desafios de incompatibilidade e limitações de agilidade impostas pelos sistemas antigos. Além disso, a adoção de Elastic Block Store (EBS) melhorou o armazenamento e a performance na cloud.Espera-se que o banco tenha uma infraestrutura de TI mais ágil, segura e capaz de responder rapidamente às demandas do mercado e às expectativas dos clientes. A integração eficiente dos sistemas legados com as novas tecnologias deve permitir a introdução de serviços digitais inovadores com uma experiência de usuário aprimorada. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: API,  agnóstica,  arquitetura,  acesso,  Restful,  integração,  distribuídas,  dados,  criptografia. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para superar o desafio de integrar sistemas que utilizavam chamadas de procedimento remoto (RPC) e formatos de dados proprietários, foi necessário realizar uma série de experimentações técnicas focadas nos mecanismos de serialização/deserialização de dados e nos protocolos de autenticação segura. Durante os testes, foi implementado um mecanismo robusto de serialização/deserialização que converteu os tipos de dados proprietários dos mainframes em formatos JSON ou XML, utilizados pelas APIs RESTful. Os experimentos demonstraram que a conversão de dados poderia ser realizada com uma taxa de sucesso de 98%, garantindo a integridade e a precisão dos dados.No re-hosting, a compreensão sobre a arquitetura de armazenamento da cloud e as capacidades de I/O levou à experimentação de soluções de armazenamento de alto desempenho. Configurou-se o ambiente para maximizar a taxa de transferência e a IOPS (Input/Output Operations Per Second). Os testes de desempenho indicaram que a configuração otimizada resultou em uma melhoria de 40% na taxa de transferência e um aumento de 35% na IOPS comparado com as soluções de armazenamento tradicionais. Exploraram-se tecnologias de emulação de terminais avançadas, integrando soluções que ofereciam latência reduzida e maior compatibilidade com as aplicações COBOL existentes. Os resultados mostraram que a latência foi reduzida em 25%, garantindo uma experiência de usuário final ininterrupta e eficiente.A segurança foi reforçada através do desenvolvimento de um sistema de gerenciamento de identidade e acesso, juntamente com uma solução personalizada para gestão de chaves de criptografia. Estudos realizados apontaram que a ativação do TLS não era suficiente. Foi necessário entender as versões do protocolo TLS, as cifras suportadas e a importância da ordem de preferência dessas cifras. Constatou-se que versões anteriores do TLS (1.0 e 1.1) eram vulneráveis a ataques, como o POODLE, e que cifras mais robustas, como ECDHE (Elliptic Curve Diffie-Hellman Ephemeral) para o intercâmbio de chaves, ofereciam mais segurança e melhor desempenho. Com essas compreensões, configurou-se o TLS 1.2 (e posteriormente o 1.3) como padrão para todas as comunicações entre microserviços, especificando uma lista de cifras que excluía algoritmos conhecidos por suas vulnerabilidades. Os testes de segurança indicaram que a implementação do TLS 1.2 e 1.3 reduziu a vulnerabilidade a ataques em 95%.Para cada operação de negócio que exigia várias transações entre microserviços, delineou-se uma série de passos (sagas), identificando claramente as operações de compensação para cada passo em caso de falhas. Isso exigiu uma análise profunda dos fluxos de negócios e uma modelagem cuidadosa para garantir que todas as situações de falha fossem adequadamente tratadas, mantendo a integridade dos dados. Os experimentos de simulação de falhas indicaram que a implementação de sagas reduziu a perda de dados em 98% e melhorou a resiliência do sistema em 45%.Compreendeu-se que a criptografia AES-256, por si só, não garantia segurança completa sem uma gestão de chaves eficaz. Desenvolveu-se um serviço personalizado para gerenciamento de chaves que oferecesse não apenas armazenamento seguro, mas também políticas de rotação de chaves e controles de acesso granulares para as chaves de criptografia. Além de adotar o AES-256 para criptografia de dados em repouso e em trânsito, configurou-se a rotação automática de chaves, garantindo que as chaves antigas fossem substituídas regularmente sem interromper os serviços. Isso foi complementado por políticas de acesso restritas às chaves, onde apenas serviços e indivíduos autorizados poderiam requisitar o uso de chaves para criptografia e descriptografia. Os testes de segurança e eficiência mostraram que a rotação automática de chaves reduziu o risco de comprometimento das chaves em 90% e assegurou a continuidade dos serviços sem interrupções. DESAFIO TECNOLÓGICO: Configurou-se um grande desafio prover a integração de sistemas com formatos de dados proprietários dos mainframes. A principal complexidade técnica estava na incompatibilidade de tipos de dados. Mainframes frequentemente utilizavam formatos binários compactos, como EBCDIC, enquanto JSON e XML utilizam UTF-8. Além disso, os dados dos mainframes incluíam campos compostos e registros hierárquicos, sem correspondentes diretos em JSON ou XML, exigindo estruturas aninhadas e algoritmos de serialização/deserialização personalizados. A preservação da integridade dos dados era crucial, pois qualquer erro poderia comprometer a precisão dos dados financeiros e de transações críticas. Propôs-se um mecanismo robusto de serialização/deserialização, incluindo mapeamentos personalizados, validações rigorosas e técnicas de otimização de código. No entanto, havia incerteza sobre se essa abordagem poderia lidar com todas as variações dos dados dos mainframes e se os testes confirmariam uma taxa de sucesso suficientemente alta na conversão.Outro desafio significativo foi garantir a segurança nas comunicações entre microserviços, especialmente devido às vulnerabilidades em versões antigas do protocolo TLS. As versões anteriores do TLS, como 1.0 e 1.1, eram suscetíveis a ataques POODLE e BEAST, que exploravam fraquezas na implementação do protocolo para interceptar e decifrar dados sensíveis. A complexidade técnica aumentou devido à necessidade de garantir compatibilidade com cifras seguras sem comprometer o desempenho. Além disso, a configuração inadequada de cifras poderia resultar em vetores de ataque adicionais, como ataques de downgrade, onde um atacante força a utilização de uma versão ou cifra mais fraca do protocolo. Considerou-se a configuração do ambiente para utilizar versões mais seguras do TLS (1.2 e 1.3) e a seleção de cifras robustas, priorizando aquelas que ofereciam um equilíbrio entre segurança e desempenho. A implementação exigiria configuração detalhada de servidores e clientes para garantir que apenas cifras seguras fossem utilizadas, além de definir uma política de preferência de cifras. Contudo, havia incerteza sobre se essa abordagem poderia realmente mitigar os riscos sem introduzir novos problemas de desempenho ou compatibilidade.Manter a integridade dos dados em transações complexas entre microserviços foi outro desafio crítico. A principal dificuldade técnica envolveu a gestão de transações que exigiam múltiplas operações em diferentes microserviços, onde falhas parciais poderiam deixar o sistema em um estado inconsistente. Se uma dessas operações falhasse, era necessário garantir que todas as operações anteriores fossem revertidas para manter a integridade dos dados. Uma das soluções propostas foi a implementação de operações de compensação para cada passo da transação, mas isso se mostrou complexo, pois exigia a definição de lógicas de reversão específicas para cada tipo de operação. A abordagem baseada em sagas foi considerada, onde cada transação seria dividida em uma série de passos com operações de compensação definidas. Entretanto, permanecia a dúvida sobre se essa abordagem poderia realmente garantir a integridade dos dados em todas as possíveis condições de falha.A gestão eficaz de chaves de criptografia representou outro desafio técnico complexo. A criptografia AES-256, embora robusta, exigia uma gestão eficaz de chaves para garantir a segurança completa dos dados. A principal dificuldade envolveu a implementação de um sistema de gerenciamento de chaves que pudesse realizar a rotação automática de chaves sem interromper os serviços em execução. Delineou-se a hipótese de criar um serviço personalizado para gerenciamento de chaves que incluísse técnicas avançadas de criptografia e hardware seguro para armazenar as chaves e controles de acesso granulares para garantir que apenas entidades autorizadas pudessem utilizar as chaves. METODOLOGIA: A metodologia de desenvolvimento seguiu uma sequência estruturada para abordar as hipóteses e superar os desafios tecnológicos. Para a integração de sistemas com formatos de dados proprietários dos mainframes, iniciou-se com a análise dos formatos de dados, identificando e documentando todos os formatos proprietários utilizados, incluindo EBCDIC, formatos binários compactos e estruturas de registros hierárquicos. Foram analisadas as diferenças entre EBCDIC e UTF-8, bem como os impactos na conversão de dados numéricos e de datas. Desenvolveram-se algoritmos de serialização e deserialização personalizados para converter dados de EBCDIC para UTF-8, utilizando técnicas de mapeamento de caracteres e normalização de dados. Estruturas aninhadas em JSON/XML foram projetadas para representar campos compostos e registros hierárquicos dos mainframes, utilizando esquemas JSON Schema e XML Schema para validação. Testes de validação foram realizados utilizando conjuntos de dados de teste representativos, verificando a integridade e precisão dos dados convertidos. Testes automatizados com frameworks como JUnit e TestNG garantiram a robustez dos algoritmos de conversão.Na segurança das comunicações entre microserviços, a análise de vulnerabilidades das versões antigas do TLS foi o primeiro passo, seguida pela configuração do ambiente para utilizar TLS 1.2 e 1.3 com cifras robustas como ECDHE, AES-GCM e ChaCha20-Poly1305. Ferramentas de análise de segurança como OpenVAS e Nessus foram utilizadas para identificar possíveis vetores de ataque. Configurou-se o ambiente para utilizar TLS 1.2 e 1.3, priorizando cifras robustas. Realizaram-se testes de desempenho utilizando ferramentas como Apache JMeter e Gatling para avaliar o impacto das novas cifras. Testes de segurança com SSL Labs e Wireshark garantiram que a configuração mitigava vulnerabilidades sem introduzir novos vetores de ataque. A compatibilidade da nova configuração de TLS com todos os microserviços e sistemas legados foi testada em ambientes isolados, ajustando configurações conforme necessário.Para manter a integridade dos dados em transações complexas entre microserviços, analisaram-se as transações, identificando dependências e pontos de falha potenciais. Utilizaram-se diagramas de sequência e fluxogramas para visualizar o fluxo de transações. Desenvolveram-se operações de compensação específicas para cada tipo de operação, utilizando padrões de design como Command e Memento. Implementaram-se operações de compensação com frameworks de transações distribuídas como Apache Camel e Spring Boot. Transações complexas foram divididas em passos menores com operações de compensação definidas, utilizando o padrão de saga. A coordenação entre microserviços foi implementada com orquestradores de sagas como Netflix Conductor e Axon Framework. Testes em condições normais e de falha garantiram que as sagas mantinham a integridade dos dados, utilizando ferramentas de simulação de falhas como Chaos Monkey.Na gestão eficaz de chaves de criptografia, analisaram-se os requisitos de criptografia e desenvolveu-se um serviço personalizado para gerenciamento de chaves, incluindo técnicas avançadas de criptografia e hardware seguro. Identificaram-se e documentaram-se todos os requisitos de criptografia, incluindo tipos de dados a serem protegidos e políticas de acesso. Um serviço personalizado para gerenciamento de chaves foi experimentado, utilizando técnicas avançadas de criptografia. Políticas de rotação automática de chaves foram definidas utilizando frameworks como HashiCorp Vault e AWS KMS, garantindo a substituição regular das chaves sem interrupções. Implementaram-se controles de acesso granulares utilizando RBAC e ABAC para garantir que apenas serviços e indivíduos autorizados possam utilizar as chaves de criptografia. O acesso às chaves foi monitorado e auditado utilizando ferramentas de SIEM como Splunk e ELK Stack. INFORMAÇÃO COMPLEMENTAR: A execução de estudos, pesquisas e experimentos aqui apresentados trouxeram uma série de benefícios significativos para o time de pesquisadores e, por extensão, para o projeto de modernização dos serviços bancários como um todo. Estes benefícios abrangeram diversas áreas, desde a melhoria da eficiência operacional até a garantia de segurança e compliance. A seguir são apresentados alguns benefícios adquiridos:Ao estudar e testar a interoperabilidade entre sistemas mainframe e cloud, a equipe conseguiu desenvolver soluções que permitiram uma integração eficaz entre essas plataformas. Como consequência, foi possível perceber uma maior a flexibilidade operacional, permitindo que sistemas legados e modernos trabalhassem conjuntamente.Otimização de Processos de Modernização: A análise detalhada das estratégias de modernização de mainframe permitiu uma transição mais suave e eficiente para arquiteturas modernas, reduzindo riscos e custos associados à migração, além de minimizar interrupções operacionais.REFERÊNCIAS BIBLIOGRÁFICAS:MIT Technology Review Insights, & Boston Consulting Group. (2023, October 23).Seeking a successful path to core modernization. MIT Technology Review.https://www.technologyreview.com/2023/10/23/1082061/seeking-a-successful-path-to-core-modernization/ RESULTADO ECONÔMICO: Retenção e captação de novos clientes,  apresentação de estruturas modernizadas,  ganhos de produtividade e elevação de competitividade dada a modernização de serviços. RESULTADO INOVAÇÃO: Estruturação de novas arquiteturas que trouxeram jornadas modernizadas, ganhos de escalabilidade,  obtenção de novos conhecimentos para estruturação de aplicações em ambiente cloud. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 14 ID ÚNICO: 109750 NOME: SOLUÇÕES PARA ESTRUTURAÇÃO DE COMPONENTES DE SEGURANÇA EM LOGIN UNIFICADO DESCRIÇÃO: A presente linha de pesquisa propõe o desenvolvimento de um sistema de login unificado, empregando a tecnologia OpenID Connect e soluções da AWS. Este sistema tem como meta fornecer um processo de autenticação seguro e eficiente para os clientes, mitigando riscos associados a ameaças cibernéticas.Para atingir esse objetivo, foram estudados e experimentados componentes para prover a integração de sistemas legados com novas arquiteturas, incluindo eventos e microsserviços, e a implementação de métodos de autenticação avançados. Destacam-se a autenticação multifatorial e a detecção de anomalias com base em inteligência artificial, visando a escalabilidade, alta disponibilidade e conformidade com normas de segurança de dados.Espera-se que, com a implementação deste sistema de login unificado, a empresa aprimore a segurança e a eficiência do processo de autenticação. A experimentação de OpenID Connect, soluções da AWS, autenticação multifatorial e detecção de anomalias por IA deve criar uma plataforma segura e adaptável às necessidades dos clientes, contribuindo para a proteção contra ameaças cibernéticas e melhorando a experiência do usuário.O marco crítico desta linha de pesquisa foi a implementação bem-sucedida de um sistema de login unificado que integra OpenID Connect e soluções específicas da AWS para oferecer autenticação segura e eficiente. Esse sistema incorporou autenticação multifatorial e utilizou inteligência artificial para a detecção de anomalias, abordando diretamente as vulnerabilidades associadas a ameaças cibernéticas.A integração efetiva de sistemas legados com arquiteturas modernas foi realizada por meio de eventos e microsserviços, assegurando a escalabilidade e alta disponibilidade do sistema. Esse processo atendeu estritamente às normas de segurança de dados vigentes. O desenvolvimento desse sistema de login unificado visa fortalecer a segurança contra ameaças cibernéticas e melhorar a experiência do usuário ao simplificar o processo de autenticação. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: OpenID,  unificado,  integrar,  login,  segurança,  arquitetura,  autenticação,  sistema,  criptografia NATUREZA: Serviço ELEMENTO TECNOLÓGICO: A hipótese central desta linha de pesquisa foi a de que a criação de um sistema de login unificado, utilizando OpenID Connect em uma infraestrutura de cloud pública, poderia aprimorar significativamente o processo de autenticação para usuários de variados canais bancários. Este estudo visou comprovar a viabilidade técnica das soluções propostas, superando as deficiências inerentes aos sistemas de autenticação fragmentados. Para isso, foram conduzidos experimentos rigorosos e detalhados, com foco em diversas inovações tecnológicas.Foram realizados experimentos para desenvolver algoritmos de criptografia customizados, com o objetivo de proporcionar uma segurança superior em comparação às soluções tradicionais. A pesquisa inicial envolveu uma análise detalhada de algoritmos de criptografia existentes, como AES, RSA e criptografia homomórfica, identificando suas limitações e vulnerabilidades em cenários de autenticação. Utilizando Python e bibliotecas criptográficas, foi criado um novo algoritmo que combinava criptografia simétrica e assimétrica para garantir a segurança dos tokens JWT, incorporando técnicas de criptografia homomórfica para permitir a análise segura dos dados. A eficácia do novo algoritmo foi avaliada por meio de testes de penetração e análise de vulnerabilidades, que demonstraram resistência a ataques de força bruta, ataques de replay e outras ameaças comuns. Os resultados dos testes indicaram uma redução de 30% no tempo de criptografia e descriptografia, além de um aumento de 50% na resistência a ataques.Um método em Python foi desenvolvido e implementado como uma função Lambda, permitindo a rotação automática de chaves e a aplicação de políticas de acesso dinâmicas. A avaliação do desempenho incluiu a medição da latência de requisição e a rotatividade de chaves, resultando em uma latência média de 200 ms e uma rotação de chaves sem interrupção do serviço.A integração da biblioteca scikit-learn permitiu o desenvolvimento de um modelo de aprendizado profundo para analisar os dados comportamentais em tempo real. Este modelo foi treinado utilizando um conjunto de dados históricos de autenticação, identificando padrões normais e anômalos. Testes de desempenho garantiram que as funções Lambda pudessem escalar automaticamente e lidar com um grande volume de requisições, proporcionando uma validação eficiente das credenciais e uma análise comportamental precisa. A precisão do modelo foi avaliada em 95%, com uma capacidade de processamento de até 1.000 requisições por segundo.Para complementar, foi desenvolvido um sistema de logging que incorporava análise comportamental baseada em inteligência artificial. O sistema de logging coletava e armazenava logs detalhados das tentativas de autenticação. Técnicas de processamento de linguagem natural (NLP) foram implementadas para permitir a análise dos logs e a identificação de padrões suspeitos. Testes de análise, utilizando um conjunto de logs históricos e simulados, avaliaram a precisão do sistema na identificação de padrões suspeitos, resultando em uma detecção precisa de atividades suspeitas. O sistema de logging reduziu a taxa de falsos positivos para 2% e aumentou a detecção de atividades anômalas em 40%.Experimentos foram executados para comprovar a viabilidade da criação de um Gateway, que serviria para rotear todas as requisições de autenticação para as funções Lambda apropriadas, configurando políticas de acesso detalhadas para garantir a segurança das requisições. Um sistema de controle de acesso dinâmico foi desenvolvido para ajustar as políticas IAM em tempo real com base nas análises comportamentais e na avaliação contínua de risco. Testes de segurança garantiram que o Gateway e as políticas de acesso fossem configurados corretamente, proporcionando uma segurança aprimorada e uma experiência de usuário melhorada. O Gateway reduziu as tentativas de acesso não autorizado em 30% e melhorou o tempo de resposta em 20%. DESAFIO TECNOLÓGICO: Ao desenvolver algoritmos de criptografia customizados, o desafio técnico foi garantir que essas operações fossem executadas com latência mínima, mesmo em cenários de alta carga. A complexidade incluiu o gerenciamento de chaves, onde se buscou garantir a segurança das chaves enquanto se minimizava o tempo de acesso. A paralelização também foi um ponto crítico, exigindo a divisão das operações de criptografia para serem processadas em paralelo sem introduzir vulnerabilidades. Além disso, a integração com hardware, utilizando aceleração por hardware como AES-NI, foi considerada para melhorar a performance. A hipótese de solução envolveu a implementação de criptografia simétrica utilizando AES-GCM com aceleração por hardware através de AES-NI. Paralelamente, desenvolveu-se um módulo em Rust para operações críticas de criptografia, utilizando bindings com Python através de FFI (Foreign Function Interface). A ideia foi reduzir a latência ao máximo possível. No entanto, a integração segura entre Rust e Python, garantindo que não houvesse vazamento de memória ou vulnerabilidades de segurança, foi um desafio significativo e requereria testes rigorosos.A rotação automática de chaves e a aplicação de políticas dinâmicas no AWS KMS apresentaram problemas específicos, como a consistência das chaves, onde se buscou garantir que todas as instâncias de serviço utilizassem a chave correta após a rotação. A sincronização de políticas também foi um desafio, exigindo a aplicação de políticas de acesso dinâmicas em tempo real sem causar inconsistências. A latência de atualização foi outro ponto crítico, buscando-se minimizar o tempo de propagação das atualizações de chaves e políticas. A hipótese de solução envolveu o desenvolvimento de um mecanismo de cache distribuído utilizando Redis para armazenar temporariamente as chaves criptográficas e políticas de acesso. Utilizou-se AWS Step Functions para orquestrar a rotação de chaves e a aplicação de políticas, garantindo que a rotação ocorresse de maneira sequencial e controlada.Implementar um sistema de logging que utilizasse técnicas de NLP para análise comportamental em tempo real envolveu desafios específicos, como o volume de dados, onde se buscou processar grandes volumes de logs rapidamente. O treinamento contínuo foi outro ponto crítico, buscando-se re-treinar modelos de NLP continuamente sem interromper o serviço. A precisão do modelo também foi um desafio, onde se buscou garantir que o modelo de NLP mantivesse alta precisão ao longo do tempo. Para o treinamento contínuo, utilizou-se um sistema de aprendizado incremental que atualizasse o modelo de NLP com novos dados periodicamente, sem a necessidade de re-treinar o modelo inteiro. A complexidade de gerenciar a infraestrutura de Kafka e garantir a escalabilidade e a baixa latência do processamento de logs foi significativa e requereria experimentação contínua.A integração de análises comportamentais e algoritmos de aprendizado de máquina foi explorada como uma tentativa para melhorar a detecção de ameaças. A coleta de dados de acesso e a subsequente análise por modelos de aprendizado de máquina, incluindo redes neurais convolucionais (CNNs) e algoritmos de detecção de anomalias, foram postos como possível soluções, que visavam identificar padrões de comportamento suspeitos. A seleção desses modelos foi baseada na sua capacidade de processar grandes volumes de dados e aprender com eles. No entanto, o sucesso da aplicação desses modelos levantava algumas incertezas, dada a dependência de conjuntos de dados de treinamento extensos e representativos. Além disso, a capacidade dos atacantes de adaptar suas estratégias para contornar os padrões aprendidos pelos modelos introduziu uma variável, questionando a capacidade de resposta e adaptação contínua dos algoritmos em um ambiente de ameaças com mudanças constantes. METODOLOGIA: Para superar os desafios, delineou-se uma metodologia de desenvolvimento focada em atividades experimentais. Essa metodologia envolveu uma série de ações e experimentos específicos para tratar cada barreira identificada, com testes rigorosos para confirmação dos resultados.Para os algoritmos de criptografia customizados, as atividades experimentais incluíram a análise de performance, onde se implementaram protótipos de algoritmos de criptografia AES-GCM com e sem aceleração por hardware (AES-NI). Mediu-se a latência e o throughput em diferentes cenários de carga. Desenvolveu-se um módulo em Rust para operações críticas de criptografia e criaram-se bindings com Python utilizando FFI. Realizaram-se testes de integração para verificar a segurança e desempenho, além de auditorias de segurança para identificar possíveis vazamentos de memória e vulnerabilidades Os experimentos e testes envolveram benchmarking para comparar o desempenho entre implementações com e sem aceleração por hardware, testes de carga para avaliar a estabilidade e eficiência do módulo de criptografia, e testes de integração para monitorar o uso de memória e identificar possíveis vulnerabilidades.Para a rotação automática de chaves e a aplicação de políticas dinâmicas no AWS KMS, implementou-se um cache distribuído utilizando Redis para armazenamento temporário de chaves e políticas. Criaram-se fluxos de trabalho com Step Functions para a rotação de chaves e aplicação de políticas dinâmicas, e configurou-se AWS SNS para notificação em tempo real sobre atualizações de chaves. Os experimentos e testes incluíram a verificação da consistência das chaves e políticas em diferentes instâncias de serviço após a rotação, medição do tempo de propagação das atualizações de chaves e políticas, e avaliação da sincronização de políticas de acesso em tempo real, garantindo que não ocorressem inconsistências.No desenvolvimento de funções Lambda escaláveis, implementou-se um SQS para gerenciar requisições de autenticação, combinada à utilização de base de dados NoSQL para armazenamento de estado entre execuções de funções Lambda, e implementou-se um cache local para armazenar temporariamente as credenciais validadas. Os experimentos e testes incluíram a simulação de cenários com grande volume de requisições para avaliar a capacidade de escalabilidade das funções Lambda, verificação do tempo de execução das funções Lambda em diferentes cargas de trabalho, e avaliação da consistência do estado armazenado e do cache local, garantindo a integridade dos dados.Para o sistema de logging com análise comportamental em tempo real, implementou-se Apache Kafka para ingestão e processamento em tempo real dos logs, utilizando spaCy para análise de linguagem natural com um pipeline de pré-processamento otimizado. Desenvolveu-se um sistema de aprendizado incremental para atualização contínua do modelo de NLP. Os experimentos e testes incluíram o processamento de grandes volumes de logs para avaliar a capacidade de Kafka e do pipeline de NLP, validação da precisão do modelo de NLP com dados reais e atualização incremental, e medição da latência do sistema de logging e análise comportamental em tempo real.Configurou-se API Gateway para rotear requisições de autenticação e implementou-se um sistema de controle de acesso dinâmico utilizando AWS IAM e Lambda. As políticas IAM foram ajustadas em tempo real com base em análises comportamentais e avaliação contínua de risco, utilizando Step Functions para orquestrar mudanças. Utilizou-se AWS CloudWatch para monitoramento contínuo e ajuste dinâmico de políticas. Os experimentos e testes incluíram a verificação da propagação instantânea e segura das políticas de acesso, avaliação da latência e eficácia do sistema de avaliação de risco em tempo real, e auditoria de segurança para garantir que as políticas dinâmicas não introduzissem vulnerabilidades. INFORMAÇÃO COMPLEMENTAR: A adoção de um sistema de login unificado utilizando OpenID Connect na AWS representa um avanço significativo na forma como os usuários finais acessam serviços bancários através de diferentes canais. Ao centralizar o processo de autenticação por meio do Amazon Cognito User Pool, configurado para autenticar contra o diretório LDAP do banco, o projeto não só simplificou o acesso para os usuários, eliminando a necessidade de múltiplas credenciais, como também elevou os padrões de segurança e desempenho em comparação com sistemas de autenticação isolados. A utilização de um Amazon Cognito Custom Authentication Flow e triggers de Lambda para a interação com o diretório LDAP foi fundamental para alcançar esse objetivo, permitindo uma integração segura e eficaz que facilita a autenticação dos usuários e a emissão de tokens JWT.A segurança do sistema foi ainda mais fortalecida pelo uso do AWS KMS para criptografar os tokens gerados, juntamente com o desenvolvimento de funções Lambda específicas para validar as credenciais dos usuários e gerar tokens de acesso. Essas medidas, combinadas com a implementação de um API Gateway para gerenciar as solicitações de autenticação e adicionar controles de acesso com políticas IAM, contribuíram para uma arquitetura robusta, capaz de lidar com picos de acesso sem comprometer a performance. A escalabilidade automática proporcionada pelo AWS Lambda e Amazon Cognito, juntamente com a segurança garantida pela criptografia e pelos testes de penetração bem-sucedidos, demonstram o sucesso do projeto em oferecer uma solução eficiente e segura para o acesso a serviços bancários online. RESULTADO ECONÔMICO: Retenção de clientes, maior nível de segurança em transações financeiras e no acesso a contas PF e PJ. RESULTADO INOVAÇÃO: Novas compreensões quanto ao desenvolvimento e aplicação de mecanismos avançados de autenticação multifatorial, de biometria etc., elevação dos níveis de segurança, unificação de jornadas de login e autenticação. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 15 ID ÚNICO: 109751 NOME: CRIAÇÃO DE NOVAS TECNOLOGIAS E COMPONENTES DE SEGURANÇA PARA VIABILIZAR A INTEGRAÇÃO ENTRE SISTEMAS EXTERNOS DESCRIÇÃO: Esta linha de pesquisa teve como objetivo primordial aprimorar a segurança e a eficiência na comunicação entre sistemas cloud AWS e mainframes legados. Através do desenvolvimento de gateways de segurança customizados e protocolos de segurança inovadores, a equipe buscou criar uma solução que pudesse gerenciar as especificidades de criptografia e autenticação, mantendo a integridade e a performance das comunicações. Além disso, o projeto visou facilitar a orquestração unificada de infraestruturas através de adapters personalizados para Infraestrutura como Código (IaC), abordando a complexidade de operar em um ambiente híbrido com tecnologias modernas e legadas.Antes deste desenvolvimento, a integração entre os sistemas cloud e mainframes enfrentava desafios significativos devido à disparidade tecnológica, que impactava tanto a segurança quanto a performance das operações. A comunicação entre esses sistemas era marcada por complexidades de criptografia e autenticação, exigindo soluções manuais ou pouco eficientes que aumentavam a latência e comprometiam a eficácia da segurança. A gestão de infraestrutura e a orquestração de deployments também eram dificultadas pela falta de ferramentas capazes de abranger ambas as tecnologias de forma coesa, resultando em processos lentos e susceptíveis a erros.Com os desenvolvimentos realizados, espera-se alcançar uma integração mais segura e eficiente entre sistemas cloud e mainframes, onde a comunicação e a gestão de segurança são otimizadas através de gateways e protocolos especializados. A introdução de adapters para IaC promete simplificar a orquestração de infraestruturas, permitindo uma gestão unificada que abarca tanto ambientes modernos quanto legados.O marco crítico deste projeto foi a criação e a aplicação bem-sucedida de gateways de segurança customizados, juntamente com protocolos de segurança adaptados, que facilitam a comunicação segura e eficiente entre sistemas em nuvem AWS e mainframes legados. Essa inovação abordou diretamente os desafios de criptografia e autenticação, mantendo a integridade e otimizando a performance das comunicações. Paralelamente, o desenvolvimento de adapters específicos para Infraestrutura como Código (IaC) agregaria um avanço significativo na orquestração de infraestruturas híbridas, simplificando a gestão de ambientes que integram tecnologias modernas e legadas, o que possibilita uma gestão mais eficiente e reduz os riscos de erros operacionais. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Segurança,  gateways,  performance,  sistemas,  desenvolvimento,  ambiente,  criptografia,  autenticação,  integração,  protocolos. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Foi delineada a hipótese de que desenvolver gateways de segurança customizados facilitaria a gestão de segurança entre aplicações na cloud AWS e sistemas mainframe. Esses gateways tratariam as especificidades de criptografia e autenticação de maneira isolada, porém comunicando-se entre si de forma segura.Protótipos foram submetidos a uma série de testes de penetração para avaliar a robustez contra ataques cibernéticos. Em seguida, realizaram-se testes de desempenho para medir a latência e o throughput, além de simulações de carga para verificar a escalabilidade dos gateways. Adicionalmente, foram conduzidos testes de compatibilidade para assegurar que os gateways pudessem interoperar com diferentes sistemas e aplicações.Como resultado, a comunicação entre os sistemas se tornou mais segura, com uma redução significativa em vulnerabilidades conhecidas. No entanto, houve um impacto na performance, com um aumento médio de 15% na latência. Isso indicou a necessidade de otimizações adicionais nos gateways para minimizar a latência sem comprometer a segurança. Foram realizados ajustes na configuração dos gateways e otimizado o código, além de implementar técnicas de caching e balanceamento de carga.Outra hipótese considerada foi a criação de um conjunto de protocolos de segurança intermediários entre os sistemas, convertendo chamadas de segurança modernas em chamadas compatíveis com sistemas legados. Essa abordagem tinha como objetivo harmonizar a segurança entre os sistemas, apesar das dúvidas sobre a complexidade de implementação e o impacto na latência.Foram identificadas as diferenças entre os protocolos de segurança modernos e os legados, focando nos pontos críticos de conversão. Foram desenvolvidos protótipos que incluíam métodos de encapsulamento de chamadas e tradução de tokens de autenticação.Observou-se que a integração de segurança entre os sistemas melhorou significativamente, com apenas 10 das 500 chamadas falhando na conversão. Além disso, observou-se um aumento modesto de 10% na latência, que precisava ser abordado. A equipe simplificou alguns aspectos dos protocolos e implementou técnicas de otimização de performance, como compressão de dados e otimização de algoritmos de conversão.O time técnico propôs a hipótese de que criar adapters personalizados, capazes de traduzir as operações de Infraestrutura como Código (IaC) para comandos compatíveis com o mainframe, facilitaria uma orquestração unificada entre ambientes multi-cloud e legado.Foram desenvolvidos protótipos utilizando uma abordagem modular para facilitar futuras expansões. Esses protótipos foram submetidos a testes de funcionalidade para garantir que os adapters pudessem traduzir corretamente as operações de IaC, testes de desempenho para medir o tempo de tradução de comandos e testes de manutenção para avaliar a facilidade de atualização dos adapters.Os testes iniciais mostraram que os adapters cobriam 85% das operações necessárias, mas algumas operações específicas do mainframe exigiram soluções customizadas. Para melhorar a cobertura e a manutenção, adotou-se uma abordagem modular, permitindo que novos módulos fossem adicionados conforme necessário. Técnicas de automação também foram implementadas para facilitar a atualização dos adapters.Foi considerada a hipótese de codificar uma plataforma de orquestração híbrida que pudesse gerenciar recursos tanto na cloud quanto no mainframe de forma integrada.Foi desenvolvido um protótipo da plataforma, incorporando funcionalidades compatíveis com ambientes modernos e sistemas legados. O protótipo foi submetido a testes de interface para garantir uma experiência de usuário unificada, testes de automação para avaliar a eficácia dos processos de deployment e testes de gestão de configuração para assegurar a correta orquestração dos recursos.Os testes iniciais mostraram que a plataforma conseguia gerenciar eficazmente os recursos em ambos os ambientes, com apenas 25 das 500 operações de deployment falhando. DESAFIO TECNOLÓGICO: Durante a etapa de codificação, o time de especialistas encontrou barreiras significativas ao tentar integrar controles de segurança consistentes entre as aplicações na cloud AWS e os sistemas mainframe. A aplicação de criptografia de ponta a ponta revelou-se complexa devido às diferentes capacidades criptográficas e desempenho entre os sistemas. Além disso, a implementação de uma gestão de identidade unificada tropeçou na incompatibilidade entre os protocolos modernos e os legados.Uma hipótese foi desenvolver ou modificar gateways de segurança que utilizassem criptografia homomórfica parcial para permitir operações sobre dados criptografados sem a necessidade de decriptá-los, ajustando-se às capacidades de processamento dos sistemas legados. Essa solução poderia facilitar a gestão de segurança, mas a viabilidade de manter a performance e a eficácia da segurança entre comunicações era incerta.Outra hipótese considerou desenvolver um conjunto de protocolos de segurança baseados em técnicas de Zero Trust Network Access (ZTNA), que pudessem servir como intermediários entre os sistemas, convertendo chamadas de segurança modernas em chamadas compatíveis com os sistemas legados. Essa abordagem poderia teoricamente harmonizar a segurança entre os sistemas, mas havia dúvidas sobre a complexidade de implementação e o possível impacto na latência. A tradução precisa e eficiente de protocolos de segurança é uma tarefa extremamente complexa e suscetível a erros, especialmente quando se trata de implementar ZTNA em sistemas legados com arquiteturas monolíticas.Pensou-se na ideia de adotar uma plataforma de API de terceiros, integrando-a com extensões customizadas utilizando WebAssembly (Wasm) para melhorar a segurança e a customização. A viabilidade técnica dessa solução se mostrou incerta, dada a complexidade de garantir a compatibilidade dessas extensões com cada atualização da plataforma externa e dos sistemas legados. A dependência de uma plataforma de terceiros pode introduzir vulnerabilidades e problemas de compatibilidade a longo prazo, além de exigir um esforço contínuo de manutenção e atualização das extensões em Wasm.Durante a codificação das soluções de orquestração e automação, a equipe se deparou com a dificuldade de criar scripts e templates de Infraestrutura como Código (IaC) que fossem eficazes tanto na cloud quanto nos sistemas mainframe. A automação de processos de deployment e gestão de configuração revelou-se desafiadora, especialmente pela falta de ferramentas de IaC que suportassem adequadamente o ambiente mainframe.Delineou-se então a hipótese de criar adapters personalizados que utilizassem técnicas de transformação de modelos (Model-Driven Engineering - MDE) para traduzir as operações de IaC para comandos compatíveis com o mainframe, o que poderia permitir uma orquestração unificada. A incerteza aqui residia justamente na capacidade desses adapters de cobrir todas as operações necessárias e na complexidade de mantê-los atualizados. A manutenção e atualização contínua desses adapters podem se tornar um gargalo significativo, especialmente se as ferramentas de MDE não forem suficientemente robustas para lidar com as complexidades dos sistemas legados.Pensou-se também em investir na codificação de uma plataforma de orquestração híbrida que utilizasse técnicas de inteligência artificial, como aprendizado por reforço (Reinforcement Learning - RL), para gerenciar recursos tanto na cloud quanto no mainframe de forma integrada. Essa solução enfrentava incertezas quanto à existência de plataformas que pudessem efetivamente abranger as especificidades dos sistemas legados sem sacrificar as funcionalidades modernas de orquestração. A criação de uma plataforma híbrida eficiente é um desafio monumental, com riscos de incompatibilidades e falhas de integração, além de exigir um treinamento extenso e contínuo dos modelos de RL para garantir uma orquestração eficiente e adaptativa. METODOLOGIA: Para garantir a segurança entre aplicações na cloud AWS e sistemas mainframe, foram desenvolvidos gateways de segurança customizados. Utilizou-se o algoritmo de criptografia AES e o método de autenticação OAuth. Prototipagem inicial e testes de penetração foram realizados para avaliar a robustez dos gateways contra ataques cibernéticos. A latência e o throughput foram medidos através de testes de desempenho, e simulações de carga verificaram a escalabilidade dos gateways. Após identificar um aumento médio de 15% na latência, ajustes na configuração e otimizações de código foram implementados, incluindo técnicas de caching e balanceamento de carga, para minimizar o impacto na performance.Para harmonizar a segurança entre sistemas modernos e legados, foi desenvolvido um conjunto de protocolos de segurança intermediários. Identificaram-se as diferenças entre os protocolos de segurança modernos e legados, focando nos pontos críticos de conversão. Desenvolveram-se protótipos que incluíam métodos de encapsulamento de chamadas e tradução de tokens de autenticação. Testes de integração avaliaram a eficácia na conversão de chamadas de segurança, enquanto testes de desempenho mediram a latência introduzida pelos protocolos intermediários. Simplificações e otimizações, como compressão de dados e otimização de algoritmos de conversão, foram implementadas para reduzir a latência.Um framework de API próprio foi criado com uma abordagem modular para facilitar futuras expansões, incorporando segurança como um componente central desde o início. Testes de funcionalidade garantiram que o framework pudesse ser adaptado conforme necessário, e testes de manutenção avaliaram a facilidade de atualização sem comprometer a agilidade do projeto.Adotou-se uma plataforma de API de terceiros, com desenvolvimento de extensões customizadas para melhorar a segurança e a customização. A viabilidade técnica dessa solução foi estudada, considerando a complexidade de garantir a compatibilidade dessas extensões com cada atualização da plataforma externa e dos sistemas legados. Testes de compatibilidade e integração asseguraram que as extensões funcionassem corretamente com cada atualização.Adapters personalizados foram desenvolvidos para traduzir operações de Infraestrutura como Código (IaC) para comandos compatíveis com o mainframe. Identificaram-se as operações de IaC críticas e desenvolveu-se uma abordagem modular para os adapters. Testes de funcionalidade garantiram a correta tradução das operações de IaC, e testes de desempenho mediram o tempo de tradução dos comandos. Técnicas de automação foram implementadas para facilitar a atualização dos adapters.Finalmente, uma plataforma de orquestração híbrida foi desenvolvida para gerenciar recursos tanto na cloud quanto no mainframe de forma integrada. Analisaram-se as necessidades de orquestração de ambos os ambientes e identificaram-se os pontos de integração críticos. Desenvolveu-se um protótipo da plataforma, seguido de testes de interface para garantir uma experiência de usuário unificada. Testes de automação avaliaram a eficácia dos processos de deployment, e testes de gestão de configuração asseguraram a correta orquestração dos recursos. Adotou-se uma abordagem modular para permitir a integração de novos módulos ou plugins conforme necessário. INFORMAÇÃO COMPLEMENTAR: Ainda sobre os experimentos executados no projeto, é importante destacar que foram desenvolvidos adapters personalizados destinados a atuar como intermediários entre as ferramentas de IaC e os sistemas mainframe. O objetivo era traduzir scripts e templates de IaC em comandos que os mainframes pudessem interpretar e executar. Após a implementação, a equipe realizou uma série de testes de integração e deployment, observando que, embora os adapters permitissem uma maior integração entre os sistemas, a cobertura de todas as operações necessárias e a manutenção constante dos adapters apresentavam desafios significativos.O experimento revelou a necessidade de um processo de atualização contínua para os adapters, a fim de acompanharem as mudanças tanto nos sistemas mainframe quanto nas ferramentas de IaC. A equipe decidiu estabelecer uma colaboração mais estreita com fornecedores de sistemas mainframe para simplificar a manutenção dos adapters. Além disso, foi implementada uma documentação detalhada e um processo de feedback com os usuários para identificar rapidamente necessidades de adaptação. RESULTADO ECONÔMICO: A criação de gateways de segurança customizados e uma plataforma de orquestração híbrida otimizou a integração entre sistemas cloud e mainframe, resultando em eficiências que reduziram custos de manutenção e aumentaram a agilidade no deployment. RESULTADO INOVAÇÃO: A criação de protocolos de segurança e adapters para IaC, facilitou a orquestração unificada entre ambientes cloud e legado. Isso não só melhorou a segurança e a performance, mas também expandiu as possibilidades de integração e automação. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;"NÚMERO: 16 ID ÚNICO: 109752 NOME: DESENVOLVIMENTO DE ARQUITETURA ÚNICA PARA NOVA PLATAFORMA INTEGRADA DE CARTÕES DESCRIÇÃO: A presente linha de pesquisa tem como foco central a criação de uma plataforma integrada de gestão e processamento de cartões que seja eficiente e segura, com uma atenção especial voltada para a prevenção e detecção de fraudes. Para alcançar esse objetivo, o time de especialistas realizou uma série de experimentos e integrações de tecnologias de ponta, como o Spark Streaming e Kafka para processamento de dados em tempo real, além de modelos avançados de Machine Learning (ML), incluindo redes neurais profundas e modelos baseados em árvores de decisão, visando construir um sistema robusto de detecção de fraude que opere em tempo real.Anteriormente a empresa se deparava com desafios significativos relacionados à eficiência e precisão na detecção de fraudes. Esses desafios incluíam altas taxas de falsos positivos ou falsos negativos, o que afetava negativamente a experiência do cliente e a confiabilidade do sistema. Além disso, a capacidade de processar transações em tempo real e de se adaptar rapidamente a novos padrões de fraude possuía oportunidades de melhoria, tendo em vista que boa parte das soluções que compõem esse ecossistema estava em ambiente legado, no mainframe.Para superar esses obstáculos, a equipe despendeu esforços na experimentação do Spark Streaming e Kafka para otimizar o processamento de dados em tempo real, equilibrando o tempo de resposta, o consumo de recursos e o throughput. Diferentes modelos de ML foram avaliados quanto à sua eficácia na detecção de fraudes, resultando na escolha de uma estratégia híbrida que emprega modelos simples para lidar com altas cargas de transações e modelos mais complexos para períodos de menor atividade. Além disso, ajustes nos limiares de decisão dos modelos e a aplicação de técnicas como a ponderação de classes foram realizados para melhorar a sensibilidade e especificidade na detecção de fraudes.Espera-se que a nova plataforma integrada de cartões ofereça um desempenho superior na detecção de fraudes, combinando eficiência, rapidez e precisão. A capacidade de adaptação do sistema a novos padrões de fraude, graças ao mecanismo de feedback contínuo e à estratégia híbrida de modelos de ML, representa um avanço significativo. Espera-se também um equilíbrio melhor entre a performance do sistema e o consumo de recursos, garantindo alta qualidade de serviço sem comprometer a eficiência. Por fim, as evoluções realizadas nos modelos de ML deverão resultar em uma redução significativa de falsos positivos e falsos negativos, melhorando a confiança dos usuários na plataforma e aprimorando a experiência geral do cliente.O marco crítico deste projeto foi a criação e customização dos componentes que incorporam Spark Streaming e Kafka para o processamento de dados em tempo real, juntamente com a definição da estratégia híbrida de modelos de Machine Learning (ML), incluindo redes neurais profundas e modelos baseados em árvores de decisão. Essa abordagem possibilitou um sistema de detecção de fraude altamente eficaz, capaz de operar em tempo real.A superação dos desafios iniciais, como as altas taxas de falsos positivos e falsos negativos, e a limitada capacidade de processamento em tempo real, foi alcançada por meio da otimização do processamento de dados e da implementação de modelos de ML ajustados para maximizar a precisão na detecção de fraudes. A estratégia híbrida de modelos de ML, combinando modelos simples para alta carga de transações e modelos complexos para períodos de menor atividade, juntamente com ajustes nos limiares de decisão e a aplicação de técnicas como ponderação de classes, resultou em uma sensibilidade e especificidade melhoradas na detecção de fraudes. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Experimentos,  Kafka,  modelos,  árvores de decisão,  micro-batches,  dados,  latência,  fraude. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Foram conduzidos experimentos com o Spark Streaming, processando micro-batches de diferentes tamanhos e medindo o impacto no tempo de processamento e na utilização de recursos. Também foram testados diferentes algoritmos de detecção de fraude, avaliando sua eficácia e performance. Esses experimentos trouxeram o entendimento de que micro-batches menores ofereciam respostas mais rápidas, porém com maior consumo de CPU. A escolha do algoritmo de detecção de fraude impactava significativamente a latência,  modelos mais complexos eram mais precisos, porém mais lentos. A equipe optou por uma abordagem híbrida, utilizando modelos mais simples durante picos de carga e modelos complexos em períodos de menor atividade.Para determinar a configuração do Kafka, a equipe realizou experimentos variando o tamanho do ""batch"", ""linger time"" e o número de partições por tópico. Utilizaram-se ambientes de teste que simulavam cargas de transações variáveis, medindo o throughput e a latência. Os resultados indicaram que um maior número de partições melhorava o paralelismo e o throughput, mas também aumentava a latência devido ao overhead de gerenciamento. Ajustes no ""batch size"" e ""linger time"" permitiram um equilíbrio, maximizando o throughput enquanto mantinham a latência dentro de limites aceitáveis.Para avaliar a viabilidade de integrar os modelos de ML com sistemas de pontuação em tempo real, foram realizados experimentos simulando transações em um ambiente controlado. Isso incluiu a avaliação do tempo de resposta dos modelos e sua capacidade de operar sob carga.Foram testados diferentes algoritmos de Machine Learning (ML), incluindo redes neurais profundas e modelos baseados em árvores de decisão, como Random Forest, utilizando conjuntos de dados de transações bancárias com exemplos de fraudes conhecidas para treinar e validar os modelos.Primeiramente, experimentou-se com redes neurais profundas, utilizando uma arquitetura de Rede Neural Convolucional (CNN) com 3 camadas convolucionais e 2 camadas totalmente conectadas. Foram usados 1 milhão de transações bancárias, das quais 10% eram fraudes conhecidas. O conjunto de dados foi dividido em 80% para treinamento e 20% para validação. As métricas de avaliação incluíram acurácia, precisão, recall e F1-Score. Os resultados mostraram uma acurácia de 98.5%, precisão de 97.2%, recall de 96.8% e F1-Score de 97.0%. No entanto, o tempo médio de inferência foi de 200 ms por transação, e o tempo de treinamento foi de 12 horas em uma GPU Nvidia Tesla V100. As redes neurais profundas demonstraram alta capacidade de generalização para detectar padrões complexos de fraude, mas o tempo de inferência elevado foi considerado um desafio para aplicações em tempo real.Em seguida, experimentou-se o modelo Random Forest, configurado com 100 árvores e uma profundidade máxima de 10. Utilizando o mesmo conjunto de dados, o modelo foi treinado e validado, resultando em uma acurácia de 95.3%, precisão de 94.1%, recall de 93.5% e F1-Score de 93.8%. O tempo médio de inferência foi significativamente menor, 50 ms por transação, e o tempo de treinamento foi de 2 horas em uma CPU Intel Xeon. Embora o Random Forest tenha sido mais rápido tanto no treinamento quanto na inferência, houve uma ligeira queda na precisão e recall em comparação com as redes neurais profundas.Os experimentos revelaram que, enquanto as redes neurais profundas ofereciam uma alta capacidade de generalização para detectar padrões complexos, elas exigiam um tempo de treinamento considerável e grandes volumes de dados, além de um tempo de inferência elevado. Por outro lado, o modelo baseado em árvores de decisão (Random Forest) mostrou-se mais rápido para treinar e adaptar-se a novos padrões de fraude, apesar de, em alguns casos, ser menos eficaz na detecção de fraudes altamente sofisticadas. DESAFIO TECNOLÓGICO: A escalabilidade e o tempo de inferência das redes neurais profundas representaram um problema significativo para aplicações em tempo real, devido ao tempo de inferência elevado de 200 ms por transação, com o risco de atrasos na detecção de fraudes. Possíveis soluções incluíam a otimização dos modelos por meio de técnicas de compressão, a migração para uma infraestrutura de computação mais potente e o uso de modelos híbridos, embora cada uma dessas opções apresentasse suas próprias incertezas e desafios.Outro desafio consistiu na necessidade de realizar o treinamento e a atualização contínua dos modelos. O tempo necessário para treinar as redes neurais profundas (12 horas) e os modelos baseados em árvores de decisão (2-3 horas) era uma barreira significativa. Havia incerteza sobre a capacidade de atualizar os modelos em tempo hábil, com o risco de falhas na detecção de novas fraudes. Hipóteses de solução incluíam o aprendizado incremental e o uso de Transfer Learning, cada uma com suas próprias incertezas.Não era sabido se o modelo incremental manteria a mesma precisão e eficácia que um modelo treinado de forma tradicional. Além disso, havia o risco de sobreajuste aos dados mais recentes, ignorando padrões históricos importantes. Sobre o uso de Transfer Learning, a principal incerteza era compatibilidade do modelo pré-treinado com o domínio específico das transações bancárias.Foi desafiador estabelecer uma solução de arquitetura capaz de detectar e prevenir fraudes em tempo real, sem causar atrasos significativos nas transações. Eram desconhecidos os caminhos de resolução do problema, tendo em vista que não foram realizados muitos estudos sobre metodologias de detecção de fraude com cartão de crédito devido à indisponibilidade de conjuntos de dados de transações com cartão de crédito para os pesquisadores. Ashraf, Mohamed et al. (2021). Isso levou os times a buscarem soluções próprias por meio de experimentações e testes de hipóteses, a fim de identificarem soluções possíveis.Foi pensada a hipótese de implementar uma solução baseada em aprendizado de máquina e inteligência artificial, utilizando modelos preditivos e análise comportamental para identificar transações suspeitas. Seriam empregados algoritmos de machine learning, como redes neurais profundas e modelos baseados em árvores de decisão, que aprenderiam continuamente com novos dados de transações, ajustando-se para identificar padrões fraudulentos emergentes. A hipótese era que, ao combinar esses modelos com sistemas de pontuação em tempo real, seria possível avaliar o risco de cada transação instantaneamente. No entanto, a equipe enfrentou a complexidade de treinar esses modelos com conjuntos de dados vastos e altamente desbalanceados, onde as instâncias de fraude são relativamente raras.A construção de modelos de aprendizado de máquina que pudessem adaptar-se dinamicamente a novos padrões de fraude, sem gerar um número excessivo de falsos positivos, representou o segundo grande desafio. Para enfrentá-lo, a equipe concentrou-se em técnicas de aprendizado de máquina, incluindo redes neurais profundas e algoritmos de ensemble, que prometiam maior precisão na identificação de transações fraudulentas.Esses modelos necessitavam de uma quantidade substancial de dados de treinamento etiquetados, além de um processo contínuo de ajuste fino para balancear a sensibilidade e especificidade. A hipótese era que, ao utilizar um sistema de feedback contínuo, onde as transações marcadas manualmente como fraudulentas ou não fraudulentas seriam reintroduzidas no sistema de treinamento, seria possível ajustar os modelos de forma mais eficaz, reduzindo assim os falsos positivos. Contudo, havia uma incerteza inerente quanto à velocidade com que esses ajustes poderiam ser feitos sem comprometer a capacidade do sistema de responder em tempo real, além da preocupação com a possibilidade de introduzir viés nos modelos, o que poderia prejudicar a precisão da detecção de fraudes. METODOLOGIA: -Avaliação comparativa do kafka e outros sistemas de streaming de dados: Foram estudadas as capacidades, limitações e casos de uso do Kafka em comparação com outras tecnologias de streaming, como RabbitMQ e Apache Pulsar, para garantir a escolha mais adequada para ingestão de dados em tempo real.-Análise de viabilidade do apache Spark para processamento em memória: Foi realizada uma avaliação técnica do Apache Spark, incluindo benchmarks de desempenho e análise de casos de sucesso, para determinar sua adequação ao processamento rápido de grandes volumes de dados.-Teste de integração Kafka-Spark: Desenvolveram-se protótipos para testar a integração entre Kafka e Spark, avaliando a latência, throughput e estabilidade do sistema durante o processamento de fluxos de dados em tempo real. Foram criadas também simulações de carga com volumes crescentes de transações para identificar limites de escalabilidade e pontos de falha na arquitetura proposta.Desenvolvimento de Modelos de Machine Learning:-Seleção e treinamento de modelos de aprendizado de máquina: Foram estudados e experimentados diferentes algoritmos, incluindo redes neurais profundas e modelos baseados em árvores de decisão, para identificar aqueles com melhor desempenho na detecção de fraudes.-Experimentos de balanceamento de conjuntos de dados: Foram conduzidos experimentos focados em técnicas de balanceamento de dados, como oversampling de minorias e undersampling de maiorias, para melhorar a eficácia do treinamento de modelos em conjuntos de dados desbalanceados.-Avaliação de falsos positivos e ajuste de modelos: foram desenvolvidos também ciclos iterativos de avaliação e ajuste fino dos modelos de aprendizado de máquina, utilizando feedback real de transações classificadas para reduzir falsos positivos sem sacrificar a sensibilidade na detecção de fraudes.-Otimização da infraestrutura para alta disponibilidade e baixa latência: foram realizados uma série de experimentos para otimizar a configuração de hardware e software, garantindo que a infraestrutura pudesse suportar a análise em tempo real com a menor latência possível.-Validação de modelos em condições de pico de demanda: foram realizados testes de resiliência dos modelos de aprendizado de máquina sob condições extremas de uso, simulando picos de transações para avaliar a eficácia e a velocidade de resposta dos modelos.-Estudos de viés e justiça algorítmica: conduziram-se estudos para identificar e mitigar qualquer viés inerente nos modelos de aprendizado de máquina, assegurando que a detecção de fraudes fosse justa e equitativa. INFORMAÇÃO COMPLEMENTAR: Para otimizar o processamento de dados em tempo real e a detecção de fraudes, foram realizados diversos experimentos envolvendo Spark Streaming, Kafka e algoritmos de Machine Learning (ML). A equipe de engenharia de dados descobriu que micro-batches menores no Spark Streaming ofereciam respostas mais rápidas, mas aumentavam o consumo de CPU em 30%. Para equilibrar eficiência e recursos, desenvolveram um algoritmo de ajuste dinâmico do tamanho dos micro-batches conforme a carga de trabalho.A equipe de cientistas de dados especializados em detecção de fraudes testou redes neurais profundas e Random Forest. As redes neurais profundas alcançaram uma acurácia de 98.5% e um tempo médio de inferência de 200 ms por transação, enquanto o Random Forest obteve uma acurácia de 95.3% com um tempo de inferência de 50 ms por transação. Optou-se por uma abordagem híbrida: modelos mais simples durante picos de carga e modelos complexos em períodos de menor atividade.Colaboradores especializados em infraestrutura de sistemas realizaram testes e experimentos com Kafka, variando ""batch size"", ""linger time"" e o número de partições por tópico. Descobriram que um maior número de partições aumentava o paralelismo e o throughput em 25%, mas também elevava a latência. Ajustes no ""batch size"" e ""linger time"" permitiram maximizar o throughput mantendo a latência dentro de limites aceitáveis.Para integrar os modelos de ML com sistemas de pontuação em tempo real, um time de pesquisadores simulou transações em um ambiente controlado. Redes neurais profundas, utilizando uma arquitetura de Rede Neural Convolucional (CNN) com 3 camadas convolucionais e 2 camadas totalmente conectadas, foram treinadas com 1 milhão de transações (10% fraudes), resultando em um tempo de treinamento de 12 horas em uma GPU Nvidia Tesla V100. O modelo Random Forest foi treinado em 2 horas em uma CPU Intel Xeon. Os experimentos revelaram que redes neurais profundas ofereciam alta capacidade de generalização para detectar fraudes complexas, mas exigiam tempo de treinamento considerável e grandes volumes de dados. O modelo Random Forest, embora ligeiramente menos preciso, mostrou-se mais rápido para treinar e adaptar-se a novos padrões de fraude.Buscou-se encontrar o equilíbrio ideal entre sensibilidade (capacidade de detectar fraudes) e especificidade (capacidade de identificar transações legítimas), ajustando os limiares de decisão dos modelos e aplicando técnicas como ponderação de classes. Ajustar os limiares de decisão dos modelos mostrou-se uma estratégia eficaz para reduzir falsos positivos, permitindo que os modelos fossem mais conservadores ao marcar uma transação como fraudulenta. A ponderação de classes ajudou a compensar o desbalanceamento do conjunto de dados, melhorando a precisão geral dos modelos.Foi descoberto que ajustes frequentes, embora necessários para manter a relevância dos modelos, precisavam ser cuidadosamente gerenciados para não comprometer a performance do sistema em tempo real. A equipe também adotou técnicas de desbiasamento, como análise de sensibilidade e ajuste de conjuntos de dados, para garantir que os modelos permanecessem justos e precisos.Além disso, a simulação de carga de transações ajudou a identificar e corrigir pontos de falha, garantindo que a arquitetura pudesse suportar picos de demanda sem comprometer a qualidade do serviço. Essas atividades experimentais, portanto, contribuíram significativamente para a confiança da equipe na capacidade do sistema de processar milhões de transações diariamente, mantendo a integridade e a segurança dos dados.REFERÊNCIAS BIBLIOGRÁFICAS:Ashraf, Mohamed et al. “A Comparative Analysis of Credit Card Fraud Detection Using Machine Learning and Deep Learning Techniques.” Digital Transformation Technology (2021): n. pag. 267–282. RESULTADO ECONÔMICO: A solução diminuiu significativamente as perdas financeiras relacionadas a fraudes, gerando uma considerável economia anual e fortalecendo a confiança do cliente. RESULTADO INOVAÇÃO: O investimento nas atividades de inovação tecnológica acelerou o processamento de transações e a detecção de fraudes consideravelmente, diminuindo as perdas por fraude e melhorando a segurança dos pagamentos. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC";CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 17 ID ÚNICO: 109753 NOME: DESENVOLVIMENTO DE NOVOS COMPONENTES DE SEGURANÇA E DETECÇÃO DE FRAUDES PARA SOLUÇÕES DE CARTÕES E ATENDIMENTO DESCRIÇÃO: A presente linha de pesquisa busca elevar a capacidade de processamento de dados em tempo real para treinamento de modelos de detecção de fraude. Buscava-se, com isso, aprimorar os mecanismos de criptografia e a eficiência na gestão de tokens, essenciais para proteger dados sensíveis dos clientes. Foi objeto de estudo também integrar esses avanços em sistemas legados, garantindo uma transição suave e segura entre as tecnologias.Em um cenário anterior, o processo de treinamento de modelos e a integração de sistemas enfrentavam gargalos relacionados à eficiência e segurança. A capacidade de processar grandes volumes de dados em tempo real era limitada pela arquitetura da rede neural e pela implementação do código. Além disso, a integração de modelos de aprendizado de máquina em sistemas legados era dificultada por problemas de serialização e desserialização, especialmente ao lidar com diferentes versões de bibliotecas entre os ambientes de desenvolvimento e produção.Espera-se que as atividades de inovação tecnológica desta linha de pesquisa resultem em uma transformação significativa na capacidade de detecção de fraude e na segurança de dados. A otimização do tamanho do lote e da taxa de aprendizagem, juntamente com a implementação de mecanismos de criptografia ponta a ponta e uma gestão de tokens mais eficaz, devem melhorar a precisão dos modelos e acelerar o processo de treinamento. Além disso, a adaptação de modelos de aprendizado de máquina para integração com sistemas legados, considerando a compatibilidade de bibliotecas, visa minimizar os riscos e as ineficiências operacionais.O marco crítico neste projeto foi a conclusão bem-sucedida da primeira integração funcional de mecanismos de criptografia avançada e gestão de tokens com o processamento de dados em tempo real para treinamento de modelos de Machine Learning (ML) destinados à detecção de fraude. Esse ponto de verificação marcou um avanço significativo na infraestrutura do projeto, evidenciando a superação de desafios técnicos e operacionais anteriores, além de estabelecer uma base sólida para futuras iterações e refinamentos. Mahmoud, A., Salem, A., & Elsamahy, E. (2021)Este marco não é uma entrega por si só, mas um momento crítico que demonstrou a viabilidade de integrar novas soluções de segurança e processamento em um ambiente que anteriormente era limitado por arquiteturas de rede neural ineficientes e desafios na integração com sistemas legados. Ele representou o fim de uma importante fase de desenvolvimento e testes, marcando a transição para a etapa de otimização e ajustes finos. Isso envolveu pontos de decisão cruciais, como a escolha das tecnologias de criptografia e gestão de tokens adequadas, além da definição de estratégias para a integração eficaz dessas soluções com os sistemas de processamento de dados em tempo real. Também incluiu a conclusão das tarefas de adaptação dos modelos de ML para garantir sua compatibilidade e desempenho em um ambiente de sistemas legados, destacando-se como um evento significativo no cronograma do projeto. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: TensorFlow,  PyTorch,  treinamento,  modelos,  detecção,  fraude,  dados,  tempo,  arquitetura,  código,  tamanho,  lote,  taxa,  aprendizagem,  precisão,  integração,  aprendizado,  máquina,  sistemas, serialização,  bibliotecas,  desenvolvimento. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para integrar modelos de machine learning com sistemas legados, foram desenvolvidas e testadas APIs RESTful e middlewares. A comunicação eficiente foi alcançada com latência média de 100 ms por requisição, mantendo a compatibilidade com sistemas existentes. Protocolos de segurança, como TLS 1.3 e autenticação OAuth 2.0, foram implementando visando manter a integridade dos dados durante a transferência e processamento, protegendo informações sensíveis contra acessos não autorizados.Os modelos treinados foram submetidos a testes rigorosos em ambientes de produção real, medindo a precisão e recall na detecção de fraudes. Observou-se uma precisão de 98% e um recall de 95%, resultando em uma significativa redução de falsos positivos e negativos. A criação de pipelines de monitoramento e atualização contínua permitiu a adaptação dos modelos a novos padrões de fraude em tempo real, sem necessidade de interrupções, mantendo uma taxa de atualização de 5 minutos.Assim, foram realizados testes sistemáticos e controlados para identificar os hiperparâmetros ideais. Inicialmente, utilizou-se uma abordagem de busca em grade (grid search) para explorar uma ampla gama de valores possíveis para os principais hiperparâmetros, como tamanho do lote, taxa de aprendizagem, número de camadas ocultas e número de neurônios por camada. Em seguida, uma busca aleatória (random search) foi aplicada para refinar ainda mais os hiperparâmetros dentro dos intervalos mais promissores identificados na busca em grade.Durante esses testes, métricas de desempenho foram monitoradas, incluindo a acurácia, precisão, recall, F1-score e a área sob a curva ROC (AUC-ROC). Além disso, o tempo de treinamento e a utilização de recursos computacionais foram registrados para garantir que o modelo tivesse um bom desempenho, além de ser eficiente.Para cada combinação de hiperparâmetros, o modelo foi avaliado usando validação cruzada k-fold, o que permitiu uma avaliação robusta da performance do modelo em diferentes subconjuntos dos dados. Isso ajudou a identificar possíveis problemas de overfitting ou underfitting. Adicionalmente, técnicas de regularização dropout e L2 regularization foram testadas para melhorar a generalização do modelo.Por meio desses experimentos, constatou-se que um tamanho de lote de 64 e uma taxa de aprendizagem de 0,002 proporcionavam um bom equilíbrio entre a precisão do modelo e o tempo de treinamento. Modelos com mais de três camadas ocultas não mostraram melhorias significativas em termos de métricas de desempenho, indicando que uma arquitetura mais simples era suficiente para a tarefa específica de detecção de fraude. A regularização com uma taxa de dropout de 0,5 foi eficaz em reduzir o overfitting sem comprometer a precisão do modelo.Outro ponto é que a integração de modelos de aprendizado de máquina atualizados dinamicamente em sistemas legados trouxe também desafios na serialização e desserialização de modelos, mais especificamente ao lidar com diferentes versões de bibliotecas entre o ambiente de desenvolvimento e produção.Gestão de Tokens: a codificação de tokenização para proteger dados sensíveis dos clientes trouxe compreensões importantes sobre a gestão de tokens. Armazenar, renovar e invalidar tokens mostrou-se uma tarefa complexa, sendo necessário estruturar um serviço dedicado para gerenciá-los eficientemente. Isso incluiu a necessidade de um mecanismo seguro para mapear tokens a dados sensíveis sem expor esses dados em qualquer ponto do processo.A estruturação de dados com a aplicação de bancos de dados in-memory, levaram a um tratamento eficiente para acesso rápido a dados críticos. No entanto, a volatilidade dos dados armazenados em memória levou ao time a necessidade de estabelecer uma estratégia robusta de persistência e recuperação para garantir que os dados não se perdessem em caso de falhas. DESAFIO TECNOLÓGICO: Um desafio crítico presente foi a dificuldade em ajustar hiperparâmetros de forma eficiente devido à alta dimensionalidade do espaço de busca e ao tempo computacional intensivo necessário para realizar buscas em grade e aleatórias. A interação complexa entre diferentes hiperparâmetros, como o tamanho do lote e a taxa de aprendizagem, frequentemente resultava em comportamentos não lineares e imprevisíveis no desempenho do modelo. Por exemplo, durante os experimentos, foi observado que um pequeno ajuste na taxa de aprendizagem poderia levar a uma convergência muito mais rápida ou, inversamente, a um comportamento oscilatório e instável do modelo. Similarmente, variações no tamanho do lote afetavam não apenas a estabilidade do treinamento, mas também a capacidade do modelo de generalizar para novos dados, com tamanhos de lote menores aumentando a variabilidade das atualizações dos gradientes e tamanhos maiores resultando em convergência mais lenta.Para mitigar esses desafios, uma hipótese de solução poderia envolver o uso de técnicas de otimização de hiperparâmetros mais avançadas, como algoritmos de otimização bayesiana, que podem explorar o espaço de busca de maneira mais eficiente e adaptativa. Nos estudos, observou-se que esses algoritmos utilizam modelos probabilísticos para prever o desempenho do modelo com diferentes combinações de hiperparâmetros e, assim, focam a busca nas regiões mais promissoras do espaço de hiperparâmetros. Além disso, a implementação de pipelines de dados mais robustos e a utilização de técnicas de pré-processamento de dados automatizadas poderiam ajudar a lidar com a variabilidade e a qualidade dos dados. Foram exploradas técnicas de balanceamento de dados, como oversampling e undersampling, e métodos de limpeza de dados, que poderiam ser automatizados para garantir que os dados de entrada estejam em um estado ideal para o treinamento do modelo.No entanto, essas hipóteses trouxeram suas próprias complexidades e incertezas. A configuração precisa dos algoritmos de otimização bayesiana era um desafio em si, pois esses algoritmos também possuem seus próprios hiperparâmetros que precisariam ser ajustados. Além disso, a introdução de novos pontos de falha nos pipelines de dados, como dependências adicionais em ferramentas de pré-processamento, aumentou a complexidade e a fragilidade do sistema.A detecção e prevenção de fraudes, mantendo a conformidade com regulamentações estritas como o PCI DSS (Padrão de Segurança de Dados da Indústria de Cartões de Pagamento). A equipe considerou a possibilidade de utilizar aprendizado de máquina para identificar padrões de fraude, mas a integração dessa tecnologia com sistemas legados e a garantia de que os modelos pudessem ser atualizados em tempo real sem afetar a performance do sistema eram questões sem respostas claras. A segurança dos dados dos clientes era uma preocupação constante, e embora a tokenização e a criptografia ponta a ponta fossem hipóteses viáveis, a execução dessas medidas em um ambiente heterogêneo, multiplataforma trazia pontos de incerteza quanto sua viabilidade, o que demandava mais estudos para sua confirmação de viabilidade.O desafio da detecção de fraudes apresentou-se com a necessidade de o sistema aprender e adaptar-se a novos padrões de fraude em tempo real. A hipótese de desenvolver modelos de machine learning, utilizando TensorFlow ou PyTorch, foi considerada. Esses modelos seriam treinados com grandes volumes de dados transacionais e hospedados em ambientes de computação em nuvem. A ideia era que esses modelos pudessem ser atualizados em tempo real, mas a viabilidade de implementação e a eficácia em um cenário de produção permaneceram como questões abertas. METODOLOGIA: Desafios significativos foram enfrentados ao desenvolver um modelo de detecção de fraude. A metodologia adotada envolveu três etapas principais: preparação dos dados, experimentação com hiperparâmetros e avaliação de desempenho.Preparação dos Dados: Dados transacionais do ambiente bancário foram coletados e limpos, aplicando-se técnicas de normalização e codificação de variáveis categóricas. Utilizou-se o SMOTE para lidar com o desbalanceamento de classes, garantindo que os dados estivessem prontos para o treinamento do modelo.Experimentação com Hiperparâmetros: Foram realizados experimentos utilizando busca em grade (grid search) e busca aleatória (random search) para otimizar hiperparâmetros como tamanho do lote e taxa de aprendizagem. Métricas de desempenho, incluindo acurácia, precisão, recall, F1-score e AUC-ROC, foram monitoradas, utilizando validação cruzada k-fold para avaliar a robustez do modelo.Avaliação de Desempenho e Ajustes: Os resultados indicaram que um tamanho de lote de 64 e uma taxa de aprendizagem de 0,001 proporcionavam um bom equilíbrio entre precisão e tempo de treinamento. No entanto, ajustes na taxa de aprendizagem resultaram em comportamentos oscilatórios, exigindo uma abordagem mais adaptativa.Para melhorar a eficiência da otimização de hiperparâmetros, a otimização bayesiana foi implementada, mostrando uma melhoria significativa na eficiência da busca. Além disso, pipelines de dados foram automatizados para melhorar a qualidade e estabilidade do treinamento do modelo. Embora essas soluções tenham mostrado resultados promissores, sua eficácia completa ainda precisa ser validada em um ambiente de produção.Foram realizados estudos sobre a integração de modelos de machine learning, utilizando TensorFlow e PyTorch, com sistemas legados. Focou-se na atualização em tempo real dos modelos sem afetar a performance do sistema. Avaliou-se o impacto das atualizações em tempo real e desenvolveu-se APIs e middlewares para facilitar a comunicação entre os modelos e os sistemas legados. Protocolos de segurança foram implementados para garantir a integridade dos dados.Testes extensivos em ambientes de produção real mediram a precisão, recall e F1-score dos modelos. Desenvolveram-se pipelines para monitorar a performance dos modelos e realizar atualizações contínuas. Analisou-se a capacidade dos modelos de identificar padrões de fraude proativamente.Os resultados mostraram uma melhoria significativa na detecção de fraudes antes que ocorressem danos, mantendo a performance do sistema. A precisão e recall na detecção de fraudes foram altas, reduzindo falsos positivos e negativos.Adoção de Tokenização e Criptografia Ponta a Ponta: Pesquisas detalhadas avaliaram a viabilidade e a eficácia da tokenização e da criptografia ponta a ponta em ambientes heterogêneos e multiplataforma. Estes estudos ajudaram a equipe a entender como essas tecnologias poderiam ser implementadas de maneira eficaz, mantendo a integridade e a segurança dos dados dos clientes em todas as transações.Desenvolvimento de um Sistema de Mensageria e Bancos de Dados In-memory: Foram realizados estudos sobre o uso de Kafka para gerenciamento de fluxo de mensagens em alta velocidade e a implementação de bancos de dados in-memory para acesso rápido a dados críticos. A pesquisa focou na integração e escalabilidade dessas tecnologias em um ambiente complexo, garantindo que as necessidades de rapidez e eficiência transacional fossem atendidas.Criação de APIs e Adaptadores Customizados para Interoperabilidade: Estudos aprofundados sobre a comunicação com diversas redes de cartões e bancos internacionais informaram o desenvolvimento de APIs e adaptadores customizados. Essas pesquisas abordaram a complexidade de implementação e manutenção dessas interfaces, buscando soluções que promovessem flexibilidade e conformidade sem comprometer a performance do sistema. INFORMAÇÃO COMPLEMENTAR: Para além do exposto, é possível destacar que o desenvolvimento das tecnologias apresentadas visou aprimorar a eficiência e a segurança na detecção de fraudes, um ponto extremamente importante em sistemas financeiros. A capacidade de processar grandes volumes de dados em tempo real, essencial para a detecção efetiva de fraudes, foi significativamente melhorada pela otimização da arquitetura das redes neurais e pela implementação de código mais eficiente.Além disso, a iniciativa de integrar mecanismos de criptografia ponta a ponta e aprimorar a gestão de tokens para proteger dados sensíveis dos clientes destaca iniciativa da empresa em evoluir seus processos e mecanismos para garantia de total segurança em suas plataformasA experimentação e integração de modelos de aprendizado de máquina em sistemas legados, superando desafios de serialização e compatibilidade de bibliotecas, destacam o caráter inovador do projeto. Essa abordagem possibilita evoluções importantes para a empresa, bem como gera novas bases de conhecimento e experiência para o desenvolvimento de novos projetos de inovação no futuro.SUBSIDIOS PARA ATIVIDADES DE P&D:Mahmoud, A., Salem, A., & Elsamahy, E. (2021). Real-time machine learning-basedframework for the analysis of banking financial data. In Lecture Notes in Networks and Systems(Vol. 224, pp. 407–421). Springer. RESULTADO ECONÔMICO: A linha de pesquisa apresentada permitiu processar grandes volumes de dados eficientemente, reduzindo o tempo e os custos associados ao treinamento. Ajustes na arquitetura e na implementação resultaram em operações mais ágeis e econômicas. RESULTADO INOVAÇÃO: A introdução de mecanismos de criptografia ponta a ponta e a gestão avançada de tokens possibilitou aumentar a segurança dos dados sensíveis dos clientes, além de otimizar a performance do sistema, equilibrando segurança e eficiência operacional. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
27836;2023;CNPJ: 17192451000170 RAZÃO SOCIAL :BANCO ITAUCARD S.A. ATIVIDADE ECONOMICA :Bancos múltiplos, sem carteira comercial Cd ATIV.ECONOMICA IBGE :K.64.31-0/00 PORTE Demais ID EMPRESA/ANO :27836;NÚMERO: 18 ID ÚNICO: 109754 NOME: SOLUÇÕES AVANÇADAS EM CLOUD PARA ESTRUTURAÇÃO DE SERVIÇOS BANCÁRIOS DISTRIBUIDOS DESCRIÇÃO: Esta linha de pesquisa tem como objetivo obter compreensões técnicas para melhorar a eficiência, confiabilidade e escalabilidade de sistemas distribuídos, através da adoção de estratégias inovadoras de gerenciamento de dados, como CRDTs para resolução de conflitos, particionamento horizontal combinado com réplicas para tolerância a falhas, e a aplicação de protocolos de atomicidade e idempotência, como o uso de logs de transações e o protocolo de commit de duas fases. Além disso, explora-se a integração da tecnologia blockchain com o uso de contêineres Docker e o algoritmo de consenso Proof of Authority para aprimorar a segurança e a flexibilidade operacional. Essas abordagens visam assegurar que sistemas distribuídos possam gerenciar dados de forma eficaz, mantendo a consistência e a disponibilidade em ambientes com atualizações concorrentes e potenciais falhas.Anteriormente, eram enfrentados desafios consideráveis relacionados à sincronização de dados e à tolerância a falhas, que comprometiam sua disponibilidade e escalabilidade. A dificuldade em assegurar a consistência de dados em cenários de atualizações concorrentes, juntamente com a vulnerabilidade a falhas inesperadas, limitava o desempenho e a confiabilidade desses sistemas. A ausência de estratégias eficientes para gerenciar a distribuição e a replicação de dados, bem como para garantir a atomicidade e a idempotência das transações, representava obstáculos significativos para a expansão e a eficiência operacional.Espera-se que os sistemas distribuídos alcancem um novo patamar de eficiência e robustez. A adoção de CRDTs e estratégias de particionamento horizontal com réplicas promete melhorar a disponibilidade e a escalabilidade, enquanto minimiza a latência adicional e assegura a convergência para um estado consistente. A implementação de logs de transações, checkpoints e o protocolo de commit de duas fases deve fortalecer a atomicidade e a idempotência, aumentando a confiabilidade do sistema mesmo diante de falhas. Além disso, a integração da tecnologia blockchain, com ênfase na flexibilidade operacional e na segurança das transações, abre caminhos para a inovação em sistemas distribuídos.O marco crítico deste projeto foi a implementação bem-sucedida e a integração de estratégias de gerenciamento de dados inovadoras, especificamente o uso de CRDTs (Conflict-free Replicated Data Types) para a resolução de conflitos, o particionamento horizontal com réplicas para aumentar a tolerância a falhas, e a aplicação de mecanismos de atomicidade e idempotência, como logs de transações e o protocolo de commit de duas fases. Este ponto de verificação representou uma mudança fundamental na abordagem de gestão de sistemas distribuídos, marcando a transição para uma nova fase de eficiência, confiabilidade e escalabilidade. A integração bem-sucedida dessas estratégias indicou a capacidade do projeto de superar desafios anteriores relacionados à sincronização de dados e tolerância a falhas, estabelecendo uma base sólida para a continuidade do desenvolvimento e aprimoramento do sistema.Além disso, a exploração da tecnologia blockchain, em conjunção com o uso de contêineres Docker e o algoritmo de consenso Proof of Authority, emergiu como um marco crítico, destacando inovações significativas em segurança e flexibilidade operacional. Esta etapa foi crucial para demonstrar o potencial de tais tecnologias em fortalecer a integridade e a confiabilidade dos sistemas distribuídos, ao mesmo tempo em que oferece novas possibilidades para a gestão eficaz de dados em cenários complexos de atualizações concorrentes e falhas potenciais. Este marco não apenas sinalizou a superação de limitações prévias, mas também pavimentou o caminho para futuras inovações e aprimoramentos na gestão de sistemas distribuídos, alinhando-se com os objetivos de longo prazo do projeto de alcançar um patamar superior de eficiência e robustez. TIPO (PA ou DE): DE – Desenvolvimento Experimental AREA: TIC – Software PALAVRAS CHAVE: Sistemas,  dados,  distribuídos,  transações,  réplicas,  desenvolvimentos,  consistência,  estratégias,  falhas,  CRDTs. NATUREZA: Serviço ELEMENTO TECNOLÓGICO: Para resolver conflitos de dados sem intervenção externa, desenvolveu-se um modelo baseado em CRDTs (Conflict-free Replicated Data Types). O experimento envolveu a simulação de atualizações concorrentes em réplicas de dados distribuídas, utilizando um cluster de servidores distribuídos em três data centers distintos. Cada nó aplicou operações de escrita e leitura concorrentes, incluindo incrementos, decrementos e atualizações de registros em um banco de dados distribuído. Realizaram-se 10.000 operações concorrentes em um período de 24 horas.Os resultados mostraram que todas as réplicas convergiram para um estado consistente, apesar das atualizações concorrentes. A consistência eventual foi alcançada em 98,5% das operações dentro de um intervalo de latência de 200 ms a 500 ms, sendo adequada para aplicações onde a tolerância a latências temporárias é aceitável. Observou-se que, embora a consistência eventual não garantisse imediatismo na uniformidade dos dados, ela foi eficaz em cenários onde a latência não era crítica.Estudou-se o impacto de diferentes estratégias de particionamento de dados e tolerância a falhas sobre a performance e a confiabilidade de sistemas distribuídos. Desenvolveu-se um sistema de teste que aplicava particionamento horizontal de dados, distribuindo-os por diversos nós, cada um responsável por uma fração do conjunto de dados. Foi construído um mecanismo de réplica para garantir a tolerância a falhas, acompanhado de benchmarks para avaliar o desempenho sob diferentes cenários de carga e falhas simuladas. O sistema foi testado com um conjunto de dados de 1 TB, particionado em 10 nós, cada um com 100 GB de dados.Realizaram-se simulações de falhas em 20% dos nós, e a carga de trabalho variou de 1.000 a 10.000 operações por segundo. A estratégia de particionamento horizontal, combinada com a réplica de dados, mostrou-se eficiente na melhoria da disponibilidade e da escalabilidade do sistema, com uma disponibilidade de 99,9% e uma redução de latência de 30% em comparação com sistemas sem particionamento. No entanto, o aumento na quantidade de réplicas introduziu latência adicional nas operações de escrita, devido à necessidade de sincronização entre as réplicas. A análise revelou a importância de um balanceamento cuidadoso entre a quantidade de réplicas e a distribuição de carga para otimizar tanto a performance quanto a confiabilidade do sistema.Foi desenvolvido um sistema de rastreamento de transações, utilizando identificadores únicos e uma tabela de controle de estado. Os experimentos envolveram 5.000 transações concorrentes em um ambiente de teste com 5 nós distribuídos. A aplicação de timestamps únicos e logs de transações foi eficaz na prevenção de corrupção de dados, garantindo que apenas a versão mais recente de um dado fosse considerada válida, com uma taxa de sucesso de 99,7%.O sistema de rastreamento de transações demonstrou ser essencial para evitar a duplicação de operações, assegurando que cada transação fosse processada uma única vez, mesmo sob condições de alta carga e instabilidade de rede. A duplicação de operações foi reduzida em 95% em comparação com sistemas sem rastreamento.Foi levado a experimento o protocolo de commit de duas fases (2PC) dentro de um middleware de transação distribuída. Os testes envolveram a execução de 1.000 transações distribuídas entre 10 bancos de dados, avaliando a latência e a consistência dos dados. A latência média das transações foi de 300 ms, com um desvio padrão de 50 ms. Os testes revelaram que, embora o 2PC aumentasse a latência devido à necessidade de comunicação adicional e mecanismos de espera entre os sistemas, ele ofereceu uma garantia robusta de atomicidade em transações distribuídas, com uma taxa de sucesso de 99,8%. A coordenação eficaz entre os sistemas para alcançar um consenso antes de efetivar as mudanças foi essencial para evitar estados inconsistentes entre os bancos de dados envolvidos. DESAFIO TECNOLÓGICO: Garantir a consistência de dados em um ambiente distribuído, especialmente ao lidar com atualizações concorrentes, apresentou desafios significativos. A aplicação de CRDTs (Conflict-free Replicated Data Types) enfrentou dificuldades na implementação devido à complexidade de garantir que todas as réplicas convergissem para um estado consistente. A latência variável entre os nós e a possibilidade de falhas de rede tornaram a consistência eventual um objetivo incerto.A hipótese de solução envolveu a implementação de algoritmos de sincronização mais robustos, como o Algoritmo de Sincronização de Vetores (Vector Clocks), e a utilização de técnicas de gossip para disseminar atualizações de maneira mais eficiente. Além disso, propôs-se a utilização de uma camada de middleware que monitorasse e ajustasse dinamicamente os parâmetros de sincronização com base na carga de rede e na latência observada. No entanto, a eficácia desses algoritmos em cenários de alta carga e falhas frequentes permaneceu uma incerteza, especialmente em ambientes com topologias de rede complexas e dinâmicas.O particionamento horizontal de dados e a implementação de mecanismos de réplica para garantir a tolerância a falhas apresentaram desafios significativos. A distribuição desigual da carga entre os nós resultou em hotspots que afetaram o desempenho do sistema. Além disso, a sincronização das réplicas introduziu latências adicionais nas operações de escrita, comprometendo a performance.A hipótese de solução sugeriu a utilização de algoritmos de balanceamento de carga dinâmicos, como o Algoritmo de Consistência de Hash Dinâmica (Dynamic Consistent Hashing), que redistribuíssem os dados de maneira mais equitativa em tempo real. Propôs-se também a implementação de técnicas de sharding adaptativo, onde a granularidade dos shards poderia ser ajustada dinamicamente com base na carga de trabalho. No entanto, a complexidade de implementar e testar esses algoritmos em um ambiente de produção levantou dúvidas sobre sua viabilidade prática, especialmente em termos de sobrecarga computacional e impacto na latência das operações.Garantir a atomicidade e idempotência das transações distribuídas foi outro desafio crítico. A aplicação de timestamps únicos e logs de transações enfrentou problemas de precisão e sincronização, especialmente em ambientes com alta latência de rede. A hipótese de solução propôs a utilização de relógios lógicos, como o Algoritmo de Relógio de Lamport, e a implementação de um protocolo de sincronização de tempo mais preciso entre os nós, utilizando técnicas de NTP (Network Time Protocol) aprimoradas com PTP (Precision Time Protocol).Além disso, sugeriu-se a implementação de um sistema de checkpointing distribuído que permitisse a recuperação de estados consistentes em caso de falhas. Contudo, a precisão e a escalabilidade desses relógios lógicos em um ambiente distribuído de larga escala permaneceram incertas, levantando dúvidas sobre a capacidade de garantir a atomicidade e idempotência de maneira consistente, especialmente em cenários com alta variabilidade de latência de rede.A implementação do algoritmo de consenso Proof of Authority (PoA) apresentou desafios relacionados à segurança e à integridade das transações. Embora o PoA oferecesse vantagens em termos de velocidade e consumo de recursos, a centralização do poder de validação em um número limitado de nós levantou preocupações sobre vulnerabilidades a ataques e falhas de segurança.A hipótese de solução envolveu o desenvolvimento de um sistema de rotação de autoridades, onde os nós de validação seriam selecionados de maneira rotativa com base em critérios de performance e confiabilidade.No entanto, a eficácia desses mecanismos em mitigar riscos de segurança sem comprometer a performance permaneceu uma incerteza. A complexidade adicional introduzida pela rotação de autoridades e pela verificação contínua das ações dos nós poderia afetar negativamente a eficiência do sistema. METODOLOGIA: Para garantir a consistência de dados em ambientes distribuídos, foram implementados algoritmos de sincronização robustos, como o Algoritmo de Sincronização de Vetores (Vector Clocks). Além disso, desenvolveu-se uma camada de middleware para monitoramento dinâmico da latência de rede e ajuste dos parâmetros de sincronização. Os experimentos delineados incluíram a criação de um ambiente de testes simulado com diferentes topologias de rede e variações de latência para avaliar a eficácia dos algoritmos de sincronização. Executaram-se testes de carga para observar o comportamento dos dados replicados sob condições de alta concorrência e falhas de rede. Para confirmação, realizaram-se testes de consistência eventual, verificando se todas as réplicas convergiam para o mesmo estado após um período de tempo, e analisaram-se logs de transações para identificar conflitos e medir o tempo de resolução dos mesmos.No que tange ao particionamento de dados e à tolerância a falhas, foi implementado o Algoritmo de Consistência de Hash Dinâmica, e desenvolveram-se técnicas de sharding adaptativo para ajustar a granularidade dos shards com base na carga de trabalho. Foram simulados diferentes cenários de carga para testar a eficácia dos algoritmos de balanceamento de carga e sharding adaptativo, bem como o monitoramento do desempenho do sistema em termos de latência, throughput e distribuição de carga entre os nós. Para confirmação, realizaram-se testes de desempenho com cargas de trabalho variáveis para avaliar a distribuição de carga e identificar hotspots, além de analisar logs de operações de escrita e leitura para medir a latência introduzida pela sincronização de réplicas.Para garantir a atomicidade e idempotência em transações distribuídas, implementaram-se relógios lógicos, como o Algoritmo de Relógio de Lamport, e protocolos de sincronização de tempo aprimorados com NTP e PTP. Desenvolveu-se também um sistema de checkpointing distribuído para recuperação de estados consistentes em caso de falhas. Os experimentos incluíram a criação de um ambiente de testes distribuído para avaliar a precisão e a escalabilidade dos relógios lógicos, bem como a simulação de falhas de rede e nós para testar a eficácia do sistema de checkpointing na recuperação de estados consistentes. Para confirmação, realizaram-se testes de precisão de timestamps em diferentes cenários de latência de rede e verificou-se a atomicidade e idempotência das transações através da análise de logs de transações e estados recuperados.A criação do protocolo de commit de duas fases (2PC) foi otimizada através da redução de etapas de comunicação e da implementação de técnicas de pré-commit. Desenvolveu-se um protocolo híbrido combinando 2PC com técnicas de consenso baseadas em Paxos ou Raft. Os experimentos incluíram a simulação de transações distribuídas para avaliar a latência e a eficiência do protocolo 2PC otimizado, além de testes de carga para medir o impacto das otimizações na performance do sistema. Para confirmação, realizaram-se testes de latência de transações em diferentes cenários de carga e falhas de rede, além de analisar logs de transações para verificar a consistência dos dados e a eficácia das otimizações.Para o desenvolvimento do algoritmo de consenso Proof of Authority (PoA), foi desenvolvido um sistema de rotação de autoridades baseado em critérios de performance e confiabilidade. Os experimentos incluíram a avaliação da performance do sistema de rotação de autoridades em diferentes cenários de carga e confiabilidade dos nós. Para confirmação, realizaram-se testes de segurança para identificar vulnerabilidades e medir a eficácia dos mecanismos de verificação e auditoria, além de analisar logs de performance e confiabilidade dos nós para avaliar a eficácia do sistema de rotação de autoridades. INFORMAÇÃO COMPLEMENTAR: O desenvolvimento desta linha de pesquisa possibilita avanços significativos no desenvolvimento e estruturação de sistemas distribuídos na empresa, destacando-se o uso de CRDTs para uma eficaz resolução de conflitos e garantia de consistência de dados sem intervenção externa, mesmo em cenários de atualizações concorrentes. A adoção de estratégias de particionamento horizontal e réplicas aumenta a tolerância a falhas e a escalabilidade, enquanto a implementação de logs de transações e o protocolo de commit de duas fases fortalecem a atomicidade e a idempotência das operações. Além disso, a integração da tecnologia blockchain, com o uso de contêineres Docker e o algoritmo de consenso Proof of Authority, introduz uma nova dimensão de segurança e flexibilidade operacional. RESULTADO ECONÔMICO: A implementação de CRDTs e réplicas diminui custos operacionais ao melhorar eficiência e escalabilidade dos sistemas, reduzindo a necessidade de manutenção e otimizando recursos. RESULTADO INOVAÇÃO: A integração de CRDTs e blockchain aprimora a gestão de dados e segurança em sistemas distribuídos, possibilitando inovações e aplicações mais robustas e seguras. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: TIC;CICLO MAIOR QUE 1 ANO: Não ATIVIDADE PDI CONTINUADA ANO ANTERIOR :
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 1 ID ÚNICO: 113224 NOME: Bag of Money DESCRIÇÃO: Experimentações, testes rápidos de tecnologias, metodologias para aperfeiçoar um processo interno, pesquisas em conhecimento básico que está sendo explorado e testes de conceitos. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: FlexCop,nanoaditivação,bioestratigrafia,IoT,robótica,caracterização de fluido,integridade de duto,revestimento avançado,detector aquoso,cold spray,termossifão,eficiência naval,hidrocinética,biodiversidade,petrofísica 4D,duto submarino,nanotubo,CCS NATUREZA: Processo ELEMENTO TECNOLÓGICO: Coluna de produção em compósito com linhas elétricas e fibra ótica embutidas, substituindo o aço tradicional. Novos catalisadores baseados em óxidos mistos para desidrogenação oxidativa de etano a eteno. Adesivo epóxi nanoaditivado para reparos estruturais em compósitos. Automação e digitalização de acervos bioestratigráficos usando ferramentas tecnológicas e robóticas. Automação e otimização de processos de caracterização de fluidos, com digitalização eficiente de dados. Sistema de fluido para redução de APB e mitigação de transientes térmicos. Revestimento epóxi com nanopartículas luminescentes para detecção de trincas. Técnicas avançadas de inspeção e ensaios não destrutivos para tubulações sujeitas à corrosão. Sistema de Informações Geográficas (SIG) para planejamento e logística de matérias-primas renováveis em biorefino. Revestimento DLC (Diamond Like Carbon) antiincrustante e resistente à erosão. Sistemas de monitoramento com baterias de eletrólitos ativáveis por contaminação aquosa. Polímeros dielétricos com permeabilidade reduzida para revestimentos de cabos elétricos submarinos. Tecnologia cold spray com alumínio para mitigação de corrosão em equipamentos topside. Nova tecnologia de transferência de calor com vantagens sobre sistemas tradicionais de casco e tubos. Integração de tecnologias para aumento de eficiência e redução de emissões em embarcações, incluindo propulsão, hidrodinâmica e captura de CO2. Gerador de energia de baixa potência utilizando hidrocinética de correntes marinhas em águas profundas. Métricas para avaliação do impacto líquido na biodiversidade em ambientes terrestres e marinhos. Estudos de petrofísica e caracterização de amostras de rocha usando microtomografia em tempo real com o Sirius. Análise de fadiga em dutos submarinos considerando efeitos de trincheira e gaps pequenos em solo erosivo. Uso de efeito piezelétrico (ultrassom) para remoção de água e redução de corrosividade em sistemas de transporte e armazenamento de hidrocarbonetos. Pesquisa de substitutos para abatimento de acidez em dessalgadoras de petróleo. Melhoria das propriedades elétricas de cabos híbridos de nanotubos de carbono/fibra de carbono. Qualificação de basaltos para uso em CCS (Captura e Armazenamento de Carbono).Estas inovações abrangem uma ampla gama de áreas, incluindo materiais avançados, nanotecnologia, automação, energias renováveis, técnicas de monitoramento e inspeção, e soluções para redução de impacto ambiental. Destacam-se o uso de compósitos e nanomateriais para melhorar o desempenho e a durabilidade de equipamentos, a aplicação de tecnologias digitais e robóticas para otimizar processos e gerenciar dados, e o desenvolvimento de soluções para aumentar a eficiência energética e reduzir emissões de gases de efeito estufa.Várias dessas inovações têm potencial para transformar significativamente as operações na indústria de petróleo e gás, tornando-as mais eficientes, seguras e ambientalmente sustentáveis. Por exemplo, o uso de materiais compósitos e revestimentos avançados pode prolongar a vida útil dos equipamentos e reduzir a necessidade de manutenção, enquanto as tecnologias de monitoramento em tempo real e automação podem melhorar a segurança operacional e reduzir custos.Além disso, o foco em tecnologias para energias renováveis e redução de emissões demonstra um compromisso com a transição energética e a sustentabilidade ambiental. Isso é evidenciado pelo desenvolvimento de soluções para biorefino, captura de CO2 e geração de energia a partir de correntes marinhas.A integração de tecnologias avançadas, como microtomografia em tempo real e uso de inteligência artificial para análise de dados, também indica uma tendência para operações mais precisas e baseadas em dados, o que pode levar a melhorias significativas na eficiência e na tomada de decisões. DESAFIO TECNOLÓGICO: 1) Prova de conceito de projeto de engenharia de uma estrutura em composito com a camada externa com as instrumentações elétricas e hidraulicas existentes e seus respectivos conectores.2) Atividade, seletividade e estabilidade de catalisador para a reação de desidrogenação oxidativa de etano a eteno.3) ncorporação de nanomaterias de carbono funcionalizados em resinas adesivas epóxi utilizadas em reparos por compósitos4) Emprego de automação para redução de riscos de conferência e cadastro de informações.5) Metodologias defasadas, sem emprego de automação, na caracterização de fluidos traz consumo excessivo de solventes e reagentes e tempo elevado demandado para execução de ensaios (HH elevado).6) Desenvolvimento de formulação de fluido de completação de poço com baixa condutividade térmica e/ou com característica termocontrátil. 7) Funcionalização das nanopartículas em resina epóxi8) Desenvolvimento de novas tecnologias em desenvolvimento na área de ensaios não destrutivos e inspeção de equipamentos. 9) Determinação do espaço amostral para inspeção, espessura mínima e vida remanescente de tubulações sujeitas a fenômenos de corrosão.10) Obtenção de informações de qualidade e confiáveis para elaboração de uma ferramenta com uma base de dados confiável. Mapear e adquirir novas ferramentas tecnologicas visando aprimorar a qualidade e profundidade das análises da informação. Identificar dentre o material levantado o escopo de estratégia de desenvolvimento aquela que simultâneamente irá trazer a melhor visão de negócio para a empresa e resolver problemas relevantes das áreas operacionais da companhia que lidam com este tipo de matéria-prima.11) Desenvolvimento de material para revestimento de tubulares, antiincrustane e resitente à erosão.12) Materiais autoenergizáveis com a presença de contaminação aquosa13) Polímeros dielétricos com nanoaditivos para reduzir a permeação de fluidos sob pressão de LDA superiores a 1000m14) Técnica com baixa maturidade tecnológica e infraestrutura ainda indisponível no Brasil.15) Desenvolvimento de fornecedores e novos tipos de fluidos.16) Avaliação da eficiência e sinergia de diferentes tecnologias de  embarcação comerciais e desenvolvidas internamente, dentre elas propulsão, hidrodinâmica, captura de CO2, entre outros) para aumento de eficiência e redução de emissão de gases do efeito estufa.17) Desenvolvimento de turbinas submarinas de baixa potência voltadas a maiores correntes submarinas. 18) Ausência de metodologicias para avaliação de impacto líquido de biodiversidade adpatadas às especificidades brasileiras e do setor de óleo e gas.19) Redução das incertezas na caracterização de reservatórios da Petrobras, estudos de caracterização de reservatórios para captura de CO2.20) Elaboração de uma nova metodologia para cálculo de fadiga de VIV em vãos livres submarinos que contemplem o efeito de trincheira e vãos pequenos.21) Provar o conceito da aplicabilidade de um dispositivo, disruptivo para indústria de óleo e gás, de remoção de água por meio do efeito de ultrassom (efeito piezelétrico).22) O neutralizante empregado atualmente traz transtornos às refinarias devido à sua composição. O teste de outras substâncias para esta finalidade dá alternativas ao refinador e permite pesar custo e desempenho do produto.23) Cabo elétrico composto de um núcleo de fibra de carbono (promotora da resistência mecânica) revestida por filamentos contínuos de nanotubos de carbono, principais responsáveis pela elevada conduvidade elétrica.24) Capacidade de estocagem e reatividade dos basaltos para realizar o armazenamento METODOLOGIA: Prototipagem, testes laboratoriais e de campo, desenvolvimento de materiais avançados, automação, análises químicas e físicas. Inclui parcerias com universidades, adaptação de métricas, simulações computacionais e estudos de viabilidade. INFORMAÇÃO COMPLEMENTAR: PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Esforço resultante de 24 planos de trabalho nas áreas de Exploração e Produção, Desenvolvimento da Produção, Downstream, Midstream e Sustentabilidade no exercício 2023. RESULTADO ECONÔMICO: Ainda não se tem um estudo sobre a valoração do projeto, isso é exatamente umas das atividades que ainda será desenvolvida. RESULTADO INOVAÇÃO: Novos materiais, tecnologias e processos para otimizar produção, reduzir custos e impacto ambiental. Avanços em monitoramento, integridade e eficiência energética. Soluções para captura de CO2, biodiversidade e combustíveis marítimos. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Transversal;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto ""FlexCop"" visa desenvolver uma coluna de completação de poço utilizando materiais flexíveis. O projeto avançou com o levantamento de parâmetros, elaboração de memorial descritivo e assinatura de termos de cooperação com instituições como COPPE, SENAI e BH. O ""Adesivo epóxi nanoaditivado"" atingiu os requisitos para o TRL 4 após a extensão do termo de cooperação e reprogramação das entregas, sendo posteriormente encerrado. O projeto de ""Novas tecnologias para automação de processos analíticos"" progrediu com pesquisa bibliográfica, especificação de equipamentos, implementação robótica e criação de funcionalidades no sistema Geoqview. O ""Fluido para atenuar APB"" iniciou testes de prova de conceito com diversos aditivos para reduzir condutividade e/ou expansividade térmica em fluidos de perfuração e completação. O projeto ""LUMINOSO"" busca detectar trincas por meio de revestimento epóxi nanoaditivado. O projeto avançou com a definição de hipóteses, assinatura de contrato e caracterização do projeto. A ""Análise de Integridade de Tubulações"" utilizou a Teoria de Valores Extremos para analisar dados de PIG e determinar a espessura mínima de dutos submarinos. O ""Sistema de Informações Geográficas para Matérias Primas Renováveis"" foi interrompido após não encontrar um cliente interessado, apesar do levantamento acadêmico e de mercado realizado. O projeto ""Revestimento DLC antiincrustante resistente à erosão"" estabeleceu parcerias com UFRJ, UFES e UTFPR para desenvolvimento e testes. Os ""Detectores de contaminação aquosa"" avançaram com a seleção de uma proposta do IPT em parceria com a USP. O projeto de ""Nanoaditivação de polímeros dielétricos"" para umbilicais submarinos iniciou com o estabelecimento de hipóteses e princípios. O ""Uso do Cold Spray com Alumínio sem Jateamento em Topside"" progrediu com o levantamento do estado da arte e comparação com processos convencionais. O projeto de ""Levantamento de oportunidade de utilização de termossifão"" avançou do scouting inicial até a elaboração da valoração. As ""Métricas para avaliação de impacto líquido de biodiversidade"" progrediram com revisão bibliográfica, prospecção de parceiros e aprovação de oportunidades. O projeto ""Petrofísica 4D"" realizou o projeto de infraestrutura e mapeamento de testes necessários. O ""Estudo de Novas Metodologias para Dutos Rígidos Submarinos"" iniciou com a contratação do JIP FIST – Spans in Scour Trenches Fase 2 da DNV. A ""Aplicação de Ultrassom na redução de água e potencial corrosivo de hidrocarbonetos"" avançou com a definição de objetivos, planejamento experimental e assinatura de termo de cooperação. O projeto ""Cabo híbrido nanotubos de carbono/fibra de carbono - Fase 2"" elaborou e aprovou a OP1872 para continuar as pesquisas sobre cabos elétricos de alta eficiência. O ""Potencial de captura e armazenamento de carbono (CCS) por mineralização em reservatórios vulcânicos"" progrediu com a assinatura de termo de cooperação, participação em eventos, atividades de campo e workshops. Estes projetos demonstram o amplo espectro de pesquisa e desenvolvimento da Petrobras, abrangendo desde melhorias em processos de perfuração e completação até inovações em materiais, automação, sustentabilidade e eficiência energética. A empresa está claramente investindo em tecnologias de ponta e colaborando com diversas instituições acadêmicas e parceiros industriais para impulsionar a inovação no setor petrolífero. A maioria dos projetos está em fases iniciais ou intermediárias de desenvolvimento, com ênfase na definição de conceitos, estabelecimento de parcerias e realização de testes preliminares. Isso sugere um pipeline robusto de inovações que podem trazer benefícios significativos para as operações da Petrobras no futuro próximo.																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 2 ID ÚNICO: 113225 NOME: Entrega Potencial DESCRIÇÃO: Prover soluções disruptivas nas diversas áreas de PD&I da indústria de óleo e gás. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Biomarítimo,SHM,nanotubo,ancoragem flutuante,energia offshore,polímero semicondutor,eletrificação submarina,MicroGrid,eólica flutuante,biodiversidade,SGEES,RFID,CCUS,injetor autônomo,resíduo sustentável,asfalteno,escoamento,hidrato,VFD compacto NATUREZA: Processo ELEMENTO TECNOLÓGICO: Combustíveis com baixa pegada de carbono para o setor marítimo, utilizando bio-óleo.  Patch adesivo para monitoramento de saúde estrutural (SHM) em reparos com materiais compósitos. Cabo híbrido de nanotubos de carbono/fibra de carbono com alta condutividade elétrica. Sistema de casco e ancoragem para geração de energia eólica flutuante em águas profundas. Geração nuclear de eletricidade usando Small Modular Reactors (SMR) para unidades de produção offshore. Novos polímeros conjugados para dispositivos fotovoltaicos mais eficientes e duráveis. Cabos submarinos AC e DC de alta tensão para transmissão de energia elétrica offshore. Desenvolvimento conceitual de Microgrid para UEPs no campo de Búzios. Conceito de produção de energia elétrica a partir de fonte eólica offshore para projetos de desenvolvimento da produção. Avaliação integrada de garantia de escoamento, focando em comportamento reológico e propriedades interfaciais de emulsões complexas. Técnicas de restauração ecológica em vegetação costeira e metodologias para cálculo do impacto líquido à biodiversidade.  Sistema de geração nuclear submarina utilizando micro-reatores nucleares (MNR) para operação em águas profundas. Sistema inovador de geração de energia elétrica in situ, aproveitando correntes de alta pressão e/ou alto teor de CO2. Aplicação de tecnologia RFID em ambiente submarino para identificação remota de equipamentos e dutos. Projeto de hub de CCUS (Captura, Utilização e Armazenamento de Carbono) no Estado do Rio de Janeiro. Sistema submarino autônomo para injeção de água do mar, independente de plataformas de produção. Unidade de Processamento Sustentável de Resíduos (UPSR) para economia circular e eficiência energética. Coleção de referência e banco de dados de imagens 3D de microfósseis (foraminíferos e ostracodes). Metodologia QCR para medidas de precipitação e deposição de asfaltenos. Método HITC (Hydrate Induction Time of Crudes) para avaliação do tempo de indução de hidratos. VFD Compacto de Superfície para aplicação em unidades offshore, com foco em redução de dimensões e peso.  Estas inovações abrangem uma ampla gama de áreas, incluindo energia renovável, materiais avançados, nanotecnologia, energia nuclear, monitoramento ambiental, e soluções para desafios específicos da indústria offshore. Destacam-se as soluções para geração de energia limpa (eólica, nuclear, aproveitamento de correntes de alta pressão), o uso de materiais avançados para melhorar eficiência e durabilidade, e o desenvolvimento de tecnologias para monitoramento e operação em ambientes submarinos desafiadores. Muitas dessas inovações têm o potencial de transformar significativamente as operações na indústria de petróleo e gás, tornando-as mais eficientes, seguras e ambientalmente sustentáveis. Por exemplo, o desenvolvimento de combustíveis com baixa pegada de carbono e sistemas de geração de energia limpa demonstram um compromisso com a redução de emissões de gases de efeito estufa.  Além disso, o foco em tecnologias como CCUS, restauração ecológica e processamento sustentável de resíduos indica uma forte ênfase na sustentabilidade ambiental e na economia circular. A integração de tecnologias avançadas, como RFID submarino e sistemas autônomos, aponta para uma tendência de operações mais eficientes e seguras em ambientes offshore desafiadores. Em resumo, estas inovações representam um esforço abrangente para modernizar e tornar mais sustentável a indústria de petróleo e gás, abordando desafios técnicos, operacionais e ambientais através da aplicação de tecnologias de ponta e abordagens interdisciplinares. Elas demonstram um compromisso com a transição energética, a eficiência operacional e a responsabilidade ambiental. DESAFIO TECNOLÓGICO: Redução da pegada de carbono em combustíveis marítimos para atender às regulamentações da IMO. Desenvolvimento de sistemas de monitoramento em tempo real para reparos com compósitos, utilizando nanomateriais de carbono. Criação de cabos elétricos com núcleo de fibra de carbono e revestimento de nanotubos de carbono para maior condutividade. Otimização de turbinas eólicas flutuantes para condições do pré-sal. Adaptação de tecnologias para embarcações de produção de óleo e gás, considerando restrições de espaço e peso. Desenvolvimento de polímeros semicondutores com maior eficiência e estabilidade. Criação de barreiras metálicas contra ingresso de água em cabos de alta tensão para uso em águas profundas. Estudo de Microgrids para campos de produção, incluindo múltiplas UEPs e fontes externas de energia. Fornecimento de energia sem emissão de carbono para campos com déficit energético, incluindo avaliação de riscos e custos de geração eólica offshore. Solução para bloqueio de linhas submarinas por emulsões de petróleos parafínicos e formação de hidratos. Desenvolvimento de técnicas de recomposição vegetal para diferentes cenários de campo. Criação de sistemas submarinos remotos operando a 2.000 m de profundidade com mínima intervenção. Aproveitamento de energia de correntes de alta pressão para geração elétrica submarina, reduzindo emissões de gases de efeito estufa. Superação de limitações de comunicação por radiofrequência em água salgada. Desenvolvimento de projeto piloto de captura, transporte e armazenamento geológico de CO2 (CCUS) em formação salina profunda. Desafios em controle autônomo de sistemas submarinos, independência de plataformas e suprimento de energia. Processamento sustentável de resíduos com geração de produtos de alto valor agregado. Organização física e digital de coleções de referência de microfósseis. Desenvolvimento de sistemas de supervisão para aquisição e tratamento de sinais. Otimização de procedimentos para repartida de poços após paradas. Aplicação de tecnologia de Carbeto de Silício em variadores de média tensão e alta potência, e qualificação de VFD compacto para instalação externa. METODOLOGIA: Testes em escala laboratorial, piloto e de campo, modelagem numérica, desenvolvimento de protótipos, avaliações técnico-econômicas e parcerias com universidades e institutos, caracterização de materiais, simulações computacionais. INFORMAÇÃO COMPLEMENTAR: PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Esforço resultante de 21 planos de trabalho nas áreas de Exploração e Produção, Desenvolvimento da Produção, Downstream, Midstream e Sustentabilidade no exercício 2023. RESULTADO ECONÔMICO: Aumento de receitas com bunker de baixo carbono, produção adicional de óleo. Redução de custos em manutenção, energia e operações submarinas. Otimização da produção e eficiência energética. Potencial para novos negócios em CCUS e gestão de resíduos. RESULTADO INOVAÇÃO: Avanços em energia renovável, eficiência energética e descarbonização. Melhorias em processos de CCUS. Desafios em: monitoramento, manutenção e geração submarina, automação e otimização de operações, restauração ecológica e redução de resíduos. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Transversal;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto ""Formulação para biomarítimos"" estudou o potencial de combustíveis com bio-óleos para o setor marítimo, visando atender às metas de redução de emissões da IMO. O projeto ""Patch SHM"" avançou no sistema de aquisição e processamento de dados para verificar a sensibilidade dos sensores de nanotubos de carbono. O projeto ""Cabo Híbrido Nanotubos de Carbono/Fibra de Carbono"" buscou desenvolver um protótipo de cabo elétrico híbrido. Apesar de desafios na adesão dos nanotubos à fibra de carbono, o projeto continuará sob um novo processo. O projeto ""Estudo Conceitual para Casco e Ancoragem de Sistema de Geração de Energia Eólica Flutuante"" realizou testes em modelo de escala reduzida no LabOceano. O projeto de ""Desenvolvimento da geração nuclear em instalação topside"" iniciou com a negociação de um Termo de Cooperação com o IPEN. O projeto ""Polímero Semicondutor"" avançou na síntese e caracterização de polímeros para dispositivos eletrônicos. O projeto ""Eletrificação Submarina"" explorou tecnologias de cabos submarinos de transmissão de energia. O projeto ""Estudo Conceitual de MicroGrid para UEPs de Búzios"" passou por várias etapas, incluindo ensaios em escala reduzida de um modelo de FOWT. O projeto ""Eólica flutuante para suprimento a projetos de DP"" realizou um scouting e RFI para conhecimento do mercado. O projeto ""Avaliação Integrada de garantia de escoamento"" focou em estudos sobre interação emulsão-parafinas e implantação de modelos cinéticos. O projeto de ""Técnicas para recuperação de vegetação costeira"" incluiu o processamento de imagens aéreas e a elaboração de propostas para recuperação e ganho em biodiversidade. O projeto ""Desenvolvimento de solução nuclear em instalação submarina"" iniciou com a negociação de um Termo de Cooperação com o IPEN. O projeto ""Sistema de Geração de Energia Elétrica Submarina"" estabeleceu uma parceria com a UNIFEI para experimentos em escala piloto. O projeto ""ETIQUETAGEM SUBMARINA ROBUSTA COM TECNOLOGIA RFID"" demonstrou a viabilidade da tecnologia RFID para aplicações submarinas. O projeto ""Hub de CCUS em Cabiúnas"" conduziu estudos sobre monitoramento de CO2 e caracterização de reservatórios salinos profundos. O projeto ""Injetor Autônomo"" definiu o conceito de um sistema submarino autônomo para injeção de água do mar. O projeto ""Desenvolvimento de Unidade de Processamento Sustentável de Resíduos"" mapeou potenciais parceiros e definiu requisitos técnicos. O projeto ""Desenvolvimento de Modelo para Deposição de Asfaltenos"" iniciou o processo de contratação para testes laboratoriais e modelagem. O projeto ""HITC"" focou na revisão bibliográfica e construção da matriz de testes para formação de hidratos em sistemas dominados pelo óleo. O projeto ""VFD Compacto de Superfície"" analisou tecnologias para reduzir custos e garantir a integridade das operações em plataformas. A maioria dos projetos está em fases iniciais ou intermediárias, indicando um forte pipeline de inovações futuras. Há um destaque para tecnologias sustentáveis e de baixo carbono, como biocombustíveis marítimos, energia eólica offshore, captura e armazenamento de carbono, e energia nuclear submarina, refletindo a preocupação com mudanças climáticas e a transição energética. 																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 3 ID ÚNICO: 113226 NOME: Engenharia de Reservatórios & Elevação e Escoamento DESCRIÇÃO: Engenharia de ReservatóriosProver soluções tecnológicas com foco em Engenharia de Reservatórios e Geomecânica visando:- Embasar os estudos de simulação de fluxo com dados experimentais, otimização da lavra dos hidrocarbonetos e análise econômica dos empreendimentos de produção do E&P,- Desenvolver soluções inovadoras para o monitoramento ou gerenciamento da injeção e drenagem de fluidos nos reservatórios, visando antecipar e maximizar o fator de recuperação durante a produção das acumulações de petróleo,- Monitorar deformações no assoalho marinho de modo a subsidiar a calibração de Modelos Geomecânicos necessários para o gerenciamento de geohazards naturais e decorrentes da drenagem do reservatório.- [EV-02468] Apoiar a Petrobras no desenvolvimento do CCUS (Carbon Capture, Usage & Storage), através da caracterização de formações geológicas que possam armazenar CO2 de forma segura.- [EV-00503] Confecção da Base de Dados CCUS Brasil, Desenvolver projeto para implantação de planta de demonstração de BioEnergy with CCS (BECCS), utilizando correntes de CO2 oriundas da produção de biocombustíveis em refinarias e/ou usinas de etanol, avaliando a possibilidade de CO2-EOR, Revisão da Análise do Ciclo de Vida (ACV) do etanol.Elevação e EscoamentoDesenvolver tecnologias, produtos químicos e procedimentos operacionais capazes de garantir o escoamento de petróleo e gás em diferentes cenários de produção dos campos da Petrobras.- [EV-02223] Desenvolvimento e implantação de metodologia para acompanhamento da qualidade de inibidores de incrustação e monitoramento da injeção- [EV-02529] Desenvolver solução tecnológica para maximização da produção em ativos de E&P que utilizam bombeio centrífugo submerso submarino (BCSS)- [EV-02800] Desenvolvimento e implantação de metodologia expedita para monitoramento de elementos químicos associados a incrustação inorgânica- [EV-02238] Desenvolvimento e implantação de tecnologia para otimização de tratamentos químicos de squeeze de inibidor de incrustação no pré-sal. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Reserva, reservatório, pré-sal, escoamento, hidratos, atrito, válvulas químicas, gas lift, CCUS, CCS, descarbonização, BECCS, biorefino, ACV, offset, net-zero, incrustação, inibidores, água produzida, sulfato, carbonato, bário, cálcio, BCSS, squeeze NATUREZA: Processo ELEMENTO TECNOLÓGICO: 1) Desenvolvimento de metodologias para otimizar a produção de petróleo.2) Desenvolvimento de tecnologias inovadoras de elevação e escoamento, caracterizados por procedimento de prevenção da formação de hidratos em linhas submarinas, válvulas de gás lift e mandril de inejção química ambas elétricas, produtos inibidores de hidratos e produtos redutores de atrito para cenários de produção offshore.3) GIS CCUS Brasil: Plataforma on-line interativa de acesso aberto e acessível a todos, integrando o Sistema de Informações Geográficas Brasileiro com dados nacionais abrangentes sobre fontes estacionárias, reservatórios geológicos, áreas de proteção e infraestrutura para transporte de CO2.4)  Evitar a injeção de inibidores de incrustação fora de especificação e acompanhar o residual de inibidor de incrustação nas correntes. Essas duas entregas visam diminuir a frequência de eventos de bloqueio por incrustação salina nos sistemas de produção.5)  Avaliação de novos produtos visando otimização da entrega e posicionamento do inibidor de incrustação para aplicação em reservatório (squeeze) 6)  Avaliação completa de geoengenharia de reservatórios à luz do armazenamento de CO2, com (i) identificação de sítios para armazenamento, (ii) caracterização de rochas e interação rocha-fluido, e (iii) simulação de escoamento de fluidos, com atenção especial às questões de segurança e contenção da pluma de CO2 em subsuperfície.7)  Tecnologias alternativas para medição simultânea de íons inorgânicos associados a incrustação salina. Estas visam diminuir a frequência de eventos de bloqueio por incrustação salina nos sistemas de produção. DESAFIO TECNOLÓGICO: 1) Dificuldades para a caracterização e modelagem geológica e de fluxo visando redução de incertezas de produção e Reservas.2) Risco das decisões de investimento associado à incerteza de reservatório.3) Dificuldade na quantificação do ganho de produção com a utilização da técnica de injeção alternada de água e gás (WAG)..4) Ausência de metodologia de otimização da produção de campos de petróleo sob injeção de água e gás.5) Atualmente, não é possível avaliar a qualidade dos produtos injetados em campo, nem acompanhar o residual nas correntes de processo. A proposta envolve a disponibilização de ferramentas expeditas para análises em campo na implantação.6) Entender, modelar e generalizar o impacto dos novos produtos propotos nas métricas de perfomance dos tratamentos de squeeze, p ex tempo de vida úitil7) Nas simulações numéricas de reservatório voltadas para estimar a produção de óleo e gás, são simuladas dezenas de anos e estão representados um conjunto reduzido de fenômenos como interação do CO2 com os fluidos e a rocha. Essas simulações não atendem completamente os requisitos dos estudos de armazenamento de CO2 em subsuperfície, que cobrem centenas de anos e uma maior quantidade de fenômenos físico-químicos. 8) O tempo simulado e os fenômenos de interação do CO2 com os fluidos e a rocha reservatório, nas simulações numéricas de reservatório de petróleo voltadas à produção de óleo e gás, não satisfarem inteiramente os requisitos de uma simulação voltada para estudos de armazenamento de CO2 em subsuperfície costumam ter duração de centenas de anos. Outra diferença marcante está na dimensão dos modelos que nos estudos de armazenamento de CO2 em aquíferos salinos costumam ser diversas vezes maiores, quando comparada aos modelos numéricos usados na exploração de petróleo. Além disso, fenômenos como interação do CO2 com os fluidos e a rocha reservatório precisam ser modeladas tornando o processo de simulação por vezes proibitivo. Elevação e Escoamento9) Há incertezas quanto a medição dos elementos químicos associados a incrustação salina feitas em campo, por conta das limitações da técnica utilizada, frente as diferentes matrizes de água produzidas. Em função disso, metodologias analíticas alternativas estão sendo desenvolvidas e avaliadas.    10)  A produção de poços situados a grandes distâncias do sistema de produção (FPSO ou Plataforma) exige o desenvolvimento de novas tecnologias de elevação e escoamento e produtos químicos capazes de suportar os desafios de longos tie backs em cenários de águas profundas (alta pressão) e teores elevados de CO2 e ainda garantir o escoamento do petróleo e do gás, gerando valor ou viabilizando jazidas marginais.  METODOLOGIA: 1) Avaliação de métodos para a determinação da saturação em meios porosos, identificando vantagens e desvantagens, cenários de aplicação, integração com outros métodos e aplicação a mudança de escala.2) Disponibilizadas tecnologias para melhorar a caracterização e modelagem geológica e de fluxo aplicáveis aos processos de exploração e de desenvolvimento da produção, reduzindo as incertezas e diminuindo os riscos do negócio.3) Desenvolvida metodologia para quantificar o ganho de produção e otimização da injeção WAG no Pré-Sal considerando incertezas geológicas e operacionais.4) Análise de dados, análise de desempenho técnico-econômico, análise de ciclo de vida.5) Modelagem de cenários geológicos e seu impacto no comportamento da pluma de CO2, Desenvolvimento de métodos rápido para estimativa de injetividade de CO2 em subsuperfície, Elevação e Escoamento6) Desenvolvimento e qualificação de procedimentos operacionais, produtos químicos inibidores de hidratos e redutores de atrito, válvulas e mandris de poços, em cenários de produção de óleo e gás offshore.7) Desenvolvimento de modelo numérico e ferramenta computacional para análise térmica detalhada em motores elétricos de Bombas Centrífugas Submersas Submarinas (BCSS)8) Métodos de campo para o acompanhamento da qualidade de inibidores de incrustação e para o monitoramento da injeção do produto químico, a fim de minimizar os problemas associados à incrustação salina.9) Desenvolvimento de métodos de campo para o monitoramento de sulfato, bicarbonato e outros elementos químicos na água produzida com o objetivo de minimizar problemas associados à incrustação salina.10) Análises em laboratório, em diferentes escalas, para quantificar retenção e liberação dos inibidores de incrustação para os diferentes modos de aplicação (produtos) avaliados INFORMAÇÃO COMPLEMENTAR: PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Natureza: Produto e Processo.Esforço resultante de 7 planos de trabalho na área de Engenharia de Reservatórios no exercício 2023. RESULTADO ECONÔMICO: Reservas e Reservatórios: aumento de produção, antecipação de receita, redução de CAPEX/OPEX. VPL de R$ 100 MM e geração de offsets. Elevação e Escoamento: maior eficiência operacional, aumento de receita e vida útil dos ativos e menos intervenções. RESULTADO INOVAÇÃO: Aumento do fator de recuperação e redução de incertezas na exploração, plataforma GIS CCUS Brasil (dados sobre CO2), soluções químicas para remoção de líquidos em linhas de produção, Reservatórios para armazenamento de CO2, Redução de incrustações. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto ""Implantação do Projeto BECCS Brasil"" trabalhou juntamente à PUCRS no desenvolvimento da Plataforma GIS CCUS Brasil, já disponível de forma online e gratuita no endereço: https://www.pucrs.br/ipr/plataforma-gis-ccus-brasil/. O projeto ""Soluções químicas para remoção de líquido em linhas de produção e serviço"" começou com o planejamento das atividades e a apresentação das soluções químicas avaliadas em laboratório. Diversos marcos foram criados através de checklists, incluindo proposta de valor, conhecimento do mercado, aplicação de tecnologia, hipóteses de base, caracterização, e conceitos básicos. O projeto conceitual foi documentado, e a fase inicial de desenvolvimento das soluções foi concluída. O plano de implantação foi apresentado aos clientes, e a pré-qualificação de todos os produtos químicos a serem utilizados na operação de SGN foi realizada. A entrega foi disponibilizada conforme o marco criado automaticamente. No projeto ""Métodos de avaliação em campo da qualidade e eficiência de inibidores de incrustação a base de fosfonatos"", foram desenvolvidas metodologias alternativas para a determinação em campo de íons com potencial de incrustação, seguidas pela validação do CRL-5 pela Área de Inteligência e Integração com o Negócio. No projeto ""Squeeze Life Enhancers para o Pré-Sal"", foram realizadas uma avaliação preliminar em laboratório e uma request for information (RFI). Os resultados obtidos nessas etapas permitiram a definição de um plano para a implantação da tecnologia no Campo de Búzios, pela Petrobras. O projeto ""Quantificação do potencial volumétrico e segurança no Armazenamento de CO2"" avançou com a criação de marcos através de checklists, abordando hipóteses e princípios, aplicação de tecnologia, conhecimento do mercado, e caracterização. Estudos de reservatórios e monitoramento foram concluídos tanto para Cabiúnas onshore quanto offshore. A validação da proposta de valor foi realizada após análises adicionais solicitadas pelo cliente. O levantamento dos conceitos básicos do processo de modelagem para CCS foi um resultado importante, seguido pela consideração de entregáveis de simulação de reservatórios e a assinatura de um TC com a UFRN e um JIP com a SINTEF. O projeto ""Ferramenta Computacional de Análise Térmica para Motores de BCSS"" começou com uma revisão bibliográfica e formulação matemática, focando no estudo aprofundado sobre o aquecimento de motores, modelos para geração de calor, problemas conjugados e transferência de calor, além do desenvolvimento da camada limite térmica. Em seguida, foram prototipados e documentados os algoritmos ou funções básicas, com a busca de dados publicados e modelos que complementassem as medições e simulações. Posteriormente, os algoritmos foram executados e testados em laboratório, culminando na definição do plano de implantação. O projeto ""Metodologias alternativas para determinação em campo de íons com potencial de incrustação"" começou com uma revisão bibliográfica de estudos, experimentais e teóricos, das técnicas espectroscópicas e dos sistemas de amostragem aplicadas para análise de amostras aquosas. Os dados coletados dessa revisão subsidiaram a linha de trabalho para o desenvolvimento das metodologias analíticas para uso em campo."
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 4 ID ÚNICO: 113227 NOME: Geofísica e Petrofísica DESCRIÇÃO: Prover soluções em Geofísca e Petrofísica através da pesquisa e desenvolvimento em Exploração de Petróleo. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Imageamento sísmico, Inversão sísmica, imagem sísmica, geofísico NATUREZA: Processo ELEMENTO TECNOLÓGICO: Tecnologias para melhorar a qualidade da imagem sísmica. DESAFIO TECNOLÓGICO: Caracterização de reservatórios. METODOLOGIA: Inversão, imageamento, modelo de velocidade. INFORMAÇÃO COMPLEMENTAR: PB - Pesquisa Básica, PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Natureza: Produto e Processo.Esforço resultante de 4 planos de trabalho na área de Geofísica e Petrofísica no exercício 2023. RESULTADO ECONÔMICO: Os ganhos econômicos do PD&I, previstos para desenvolvimento da área de Geolfísca e Petrofísica, decorrem da expectativa de aumento de eficiência operacional, e redução de CAPEX. RESULTADO INOVAÇÃO: Novas tecnologias na área de Geofísica e Petrofísica que contribuem para a redução de custos de investimento (CAPEX) na exploração do pré-sal. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto ""Data Augmentation vinculados ao Modelo Petrossísmico"" iniciou com o estabelecimento de hipóteses e princípios. No projeto ""Estudos de Zona de Invasão"" foram identificados gargalos na modelagem de invasão através do UTapWeLs. Uma lista de todos os possíveis gargalos foi elaborada, visando identificar os pontos de melhoria no workflow. Além disso, foram obtidos resultados iniciais de modelos de invasão com o UtapWels, o que permitiu uma análise mais aprofundada dos resultados e a identificação de possíveis melhorias. Houve também um alinhamento com a UT sobre as melhorias no simulador de invasão, visando aprimorar ainda mais os resultados. Compromissos formais foram assumidos com os parceiros e o processo de contratação para o desenvolvimento do simulador foi iniciado. No projeto ""Inversão Acústica 4D"" foram realizadas análises dos time-shifts disponíveis na Petrobras, visando avaliar a resposta dos diferentes time-shifts disponíveis na companhia. Além disso, foi realizada uma revisão bibliográfica dos métodos existentes de inversão acústica 4D. Foram realizados testes em dados sintéticos e em dados reais, utilizando os dados selecionados. Os parâmetros da inversão foram ajustados para melhorar os resultados. Foi confeccionado um protótipo de ferramenta para testes, visando aprimorar ainda mais os resultados. Por fim, foi proposto um fluxo de trabalho para a produção, visando otimizar o processo de inversão acústica 4D. No projeto “Aprendizado de Máquina aplicado ao processamento Geológico”, foi realizada uma revisão da literatura, focada no aprendizado das terminologias do processamento geológico, seguida da seleção, tratamento dos dados de interesse, abrangendo dados sintéticos públicos e os dados disponibilizados pela Petrobras. Foram elaborados quatro benchmarks, sendo eles: L1 –  Segmentação Sísmica L2 -  Imputation in Well Log Data L2 –Well Log Lithology Classification: A Benchmark on Machine Learning Approaches L3 – Benchmark – Palinspastica. Foi ainda iniciada a construção de algoritmos de aprendizado de máquina para disponibilizar a 1ª versão de modelos nas três linhas do projeto.																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 5 ID ÚNICO: 113228 NOME: Geologia para Exploração e Reservatórios DESCRIÇÃO: Responsável por conceber, gerir e implantar o portfólio de entregas de valor para o negócio através de Pesquisa, Desenvolvimento & Inovação e através de Assistência Técnico-Científica no tema de Geologia, incluindo Geologia Estrutural, Geoquímica, Sedimentologia & Petrografia, Estratigrafia, nos âmbitos da exploração e reservatórios. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Geoquímica, Geologia Estrutural e Magmatismo, Geologia de Reservatórios, Bioestratigrafia, Sedimentologia e Petrologia, CCUS. NATUREZA: Processo ELEMENTO TECNOLÓGICO: 1) Caracterização de zonas hidrotermais associadas a rochas ígneas e a zona de falha, com detalhamento dos efeitos térmicos e definição de sweet spots. Como:- Utilização de geotermômetros de baixa temperatura e isótopos para rastreio de fluidos hidrotermais (Isótopos agrupados),- Avaliação da característica isotópica das zonas hidrotermais associadas a intrusão e a zona de falha,- Modelagem numérica para variação de temperatura a partir da intrusão contemplando convecção,- Aferição com dado de rocha do modelo numérico,- Validação da modelagem da zona de dano de falhas associada a processos hidrotermais.2) Melhor entendimento sobre a anisotropia de permeabilidade e conectividade hidráulica de reservatórios fraturados e redução de incertezas em modelos geológicos e previsão da produção. Como: Continuar o aperfeiçoamento do programa (FRACz – Workflow de modelagem da zona de falhas) desenvolvido em ambiente Petrel e patenteado (NI 21/059) no âmbito do projeto anterior, EV-00761 (implantada em junho/22). Assim,  novas implementações como calibração com dados de poços, atributos sísmicos e dados de produção, para a modelagem de fraturas e outros modelos geológicos afins. 3) Acrescentar o parâmetro de modelagem numérica às interpretações de emplacement, aumentando o grau de confiabilidade das mesmas. Como: estimar o emplacement de lavas com base quantificação da taxa de nucleação de cristais em amostras de poço. Esse modelo será calibrado com informações provenientes de amostras de afloramento e de poços cujo emplacement é conhecido. Com esse objetivo, será utilizada, de maneira pioneira na companhia, o programa CSD (crystal size distribution). 4) Projetar, construir e testar um protótipo constituído de um conjunto de sensores a laser para detecção de dióxido de carbono (CO2) e metano (CH4) que possam simular e futuramente detectar possíveis vazamentos de CO2 e CH4 em fundo oceânico. EV-02524: gerar modelos de fácies geológicas com as diversas arquiteturas deposicionais conhecidas do Pré-Sal de uma maneira rápida e condicionada aos dados. 5) Desenvolvimento de novos módulos no DionisosFlow (Software desenvolvido em parceria com o IFPEN) com a capacidade de modelar computacionalmente, através de simulações numéricas baseadas em processos, as condições físicas e químicas que controlaram a deposição e evolução inicial dos reservatórios Pré-sal. A partir destas simulações será possível prever a distribuição das fácies e da mineralogia dos reservatórios Pré-sal, e das características petrofísicas associadas, e parametrizar modelos geoestatísticos e de estatística multi-ponto. DESAFIO TECNOLÓGICO: 1) O desafio que se apresenta é ter êxito em quantificar e qualificar aquilo que ainda é subestimado nas bacias brasileiras: o impacto do hidrotermalismo associado a rochas ígneas e a zonas de falha. Dentro desse contexto, destaca-se a dificuldade na escolha de áreas com material disponível e acesso a métodos para atingir os objetivos de caracterização hidrotermal. 2) Reduzir os riscos associadas a modelagem de fratura com novos algoritmos. 3) Os desafios consistem em obter êxito em quantificar e definir as diferenças entre as curvas de cristalização de litotipos originados em diferentes ambientes (subaéreos e subaquosos). Adiciona-se a isso a dificuldade de acesso aos dados já existentes para atingir os objetivos propostos. 4) Construção de detectores que seja baratos e que consigam operar em diferentes condições (ex. lãmina d´água). 5) Qualidade da aquisição das imagens para treinamento das redes neurais. 6) As reações químicas são extremanete complexas e interrelacionáveis, sendo necessário o desenvolvimento de algorítimos que permitam um bom desempenho computacional para testar diversos cenários e pertir a construção de curvas probabilísticas. METODOLOGIA: 1) Caracterização de rochas e feições estruturais através de uma ampla gama de métodos, sísmica, modelagem numérica preditiva e análise de isótopos agrupados. 2) Criação de algotitmos associados à processos geológicos observados na natureza ou atravpes de realização de simulações laboratoriais. EV-02338: Caracterização de amostras de poços, afloramento e modelagem geológica.  3) Serão testado vários métodos detecção dos gases alvos e será avaliado o de melhor custo benefício (ex. laser). 4) Para treinar essa rede neural se faz necessária a geração de diversas imagens de treinamento confiáveis, provenientes por exemplo da modelagem forward realizada em softwares comerciais (Dionisos, GPM). Dessa forma essa EV tem o objetivo de fornecer imagens de modelos geológicos do ativo confiáveis para serem testadas no GeoGAN. 5) Levantamento de reações químicas da literatura e realização de experimentos laboratoriais para calinbarção das reações e construção. Calibração dos dados com observações mineralógicas das rochas. INFORMAÇÃO COMPLEMENTAR: PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Natureza: Produto e Processo.Esforço resultante de 6 planos de trabalho na área de Geologia para Exploração de Reservatórios. RESULTADO ECONÔMICO: Ganhos econômicos em Geologia para Exploração: redução do risco exploratório. PD&I em CCUS visa garantir segurança operacional em projetos futuros e descomissionamentos, potencialmente reduzindo custos e riscos associados a essas operações. RESULTADO INOVAÇÃO: Moodelagem geológica 3D, detecção de zonas de dano e processos hidrotermais. Melhorias na confiabilidade de modelos baseados em imagens sísmicas. Sensor para projetos CCUS. Aumento da produtividade e precisão na análise de reservatórios. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto “Hidrotermalismo associado a rochas ígneas e a zona de falha” avançou na compreensão da história térmica e cronológica da bacia, destacando a qualidade dos dados dos poços estudados e planejando novas análises de termometria de baixa temperatura. Estudos focaram nos fluidos hidrotermais e sua influência nas características permoporosas das rochas, discutindo processos de plastificação e deformação volumétrica nas falhas e enviando amostras para análises isotópicas. Realizou-se a caracterização do hidrotermalismo e modelagem numérica e determinística associada a zonas de falhas, com resultados publicados. No projeto “Modelagem da zona de dano (FRACz): aperfeiçoamento e melhoria”, a ferramenta FRACz foi atualizada para a versão 6.0 e entregue com treinamento. Dados estruturais de testemunhos foram inseridos no Painel de Análogos da Companhia e uma prova de conceito foi criada para dados de relações de escala entre rejeito de falhas e espessura da zona de dano. Testes do FRACz em modelo geológico sintético foram executados e um formulário de pesquisa foi elaborado para obter sugestões de melhorias. O projeto ""Determinação de emplacement de magmas a partir da análise quantitativa da distribuição do tamanho dos cristais (método CSD)"" estabeleceu uma estratégia de implantação, destacando a correta interpretação do ambiente de colocação das rochas ígneas. Foram planejadas análises nos laboratórios do CENPES e externos, focando nas diferentes condições de resfriamento das rochas ígneas. Uma equipe foi criada, reuniões estratégicas realizadas, e lâminas delgadas confeccionadas para caracterizar as principais feições texturais. O MVP foi entregue ao cliente, com modelos preliminares aplicados em rochas das bacias sedimentares. O projeto “Desenvolvimento de detectores de CO2 em fundo oceânico para monitoramento de CCUS” iniciou em 2023 com o objetivo de projetar, construir e testar um protótipo de sensores para detecção de CO2 e CH4 no leito marinho. A ferramenta Scouting identificou 8 empresas internacionais, mas nenhuma oferecia uma solução integrada. Criou-se uma oportunidade no Ambiente de Competitividade para selecionar um parceiro de pesquisa. No projeto ""Modelagem Forward para auxílio das redes neurais a serem testadas no GeoGAN"", o plano de implantação foi construído com a área cliente e a testagem do protótipo foi realizada. A implantação foi concluída com a entrega das imagens de treinamento da modelagem forward no GPM para testagem das redes neurais do GeoGAN. A primeira versão do GPM foi disponibilizada aos usuários, com verificação, validação e disponibilização concluídas. Finalmente, o projeto ""DionisosFlow: Presalt lacustrine carbonate and Mg-clay"" avançou na revisão de dados e definição dos parâmetros-chave e algoritmos, continuando também o desenvolvimento de detectores de CO2 em fundo oceânico e resultando na seleção de parceiros internacionais.																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 6 ID ÚNICO: 113229 NOME: Geração de Energia DESCRIÇÃO: P&D em Geração de energia e descarbonização dos escopos 1, 2 e 3, incluindo tecnologias para abatimento de emissões e diversificação rentável. Atendendo frente relacionadas às áreas de Clima, Exploração e Produçào, Refino, Gás e energia.  TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Energias renováveis, eólica, solar, termelétrica, UTE, geotérmica, energia de correntes, fotovoltaica, aerogerador, módulo fotovoltaico, descarbonização, baixo carbono. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Desenvolvimento e certificação de um aerogerador onshore com potência igual ou superior a 7 MW. DESAFIO TECNOLÓGICO: Desenvolvimento e disponibilização de aerogerador de porte na faixa de 7MW ou mais com caracteristicas apropriadas às condições do Brasil visando a redução de custos da energia gerada em função do ganho de escala. METODOLOGIA: Desenvolvimento e projeto de novo aerogerador onshore com potência de 7.0 MW ou mais, com rotor (pás) de 172 metros ou mais e utilização de caixa de engrenagem de média velocidade, fabricação e instalação de uma unidade protótipo para testes em bancada e em campo para aferição das características definidas em simulação e posterior certificação independente de tipo.  INFORMAÇÃO COMPLEMENTAR: Natureza: Produto e Processo.Esforço resultante de 1 plano de trabalho na área de Geração de Energia no exercício 2023. RESULTADO ECONÔMICO: Royalties de vendas do novo aerogerador, redução de custos na geração eólica com produto de maior porte no mercado brasileiro e ganhos pela aplicação do conhecimento adquirido pela empresa em projetos eólicos.  RESULTADO INOVAÇÃO: Desenvolvimento de projeto e protótipo de aerogerador onshore de 7MW, incluindo a fabricação dos componentes do protótipo a ser instalado em campo para realização de certificação de tipo segundo as normas técnicas do setor eólico. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto de desenvolvimento de um aerogerador onshore de 7MW tem avançado conforme o cronograma do projeto. Inicialmente, foi definido o site onde o protótipo será instalado, escolhendo-se o local da empresa STATKRAFT em Brotas de Macaubas, na Bahia. Ao final de 2023, iniciada a montagem da estrutura de testes Back to Back (B2B) na fábrica da WEG em Jaraguá do Sul, marcando o início dos testes principais dos componentes internos da nacele operando de forma integrada.																								
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 7 ID ÚNICO: 113230 NOME: Logística, Petróleo e Produtos DESCRIÇÃO: Prover soluções nas áreas de Comercialização de produtos e petróleos, bem como Logistica e Transporte relacionados as áreas de exploração, produção de petróleo, de refino e gás natural. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Dimensionamento e integridade de ativos logísticos, competitividade do gás, planejamento e programação marítimos, descarbonização do setor marítimo NATUREZA: Processo ELEMENTO TECNOLÓGICO: 1. Ferramenta computacional para previsibilidade de mercado e competitividade de gás natural.2. Modelos e pensamento competitivo ajustados a transição de mercado de gás liberalizado e diversificação de contratos.3. Sistema para otimização da programação marítima de embarcações supridoras do tipo PSV, por meio de gestão da velocidade e re-roteirização.4. Otimização de transporte aéreo para plataformas.5. Metodologia multifásica para evitar formação de borras em tanque de petróleo.6. Técnicas avançadas de reparo em dutos.7. Modelagem integrada de dispositivos para descarbonização do setor marítimo.8. Desenvolvimento de modelo de planejamento para transporte marítimo do futuro. DESAFIO TECNOLÓGICO: 1. Desenvolver modelos matemáticos e de simulação para determinar a melhor localização e a dimensão de ativos logísticos, reduzindo custos operacionais e garantindo o atendimento da demanda futura2. Modelar com técnicas de equilíbrio, para médio e longo prazo, o mercado de gás natural brasileiro considerando a competição entre agentes e transição de mercado liberalizado.3. Aumentar a aderência entre planejamento e realização nas atividades logísticas que demandam recursos de transporte e movimentação de cargas.4. Melhorar a previsibildiade de decolagens e pousos em plataformas em virtudes de condições climáticas e oceanográficas.5. Maior disponibilidade de tancagem e flixibilidade de planejamento e programação, bem como menor geração de resíduos.6. Tecnicas mais agéis, com maior confiabilidade e segurança reparo em dutos.7. Modelagem integrada de dispositivos por tipo de embarcação, entendendo suas sinergias e interações, levando a um diagnóstico de ações por tipo de embarcação. 8. Modelo para planejamento de transporte marítimo tendo em vista as incertezas de mercado e tecnológicas, como por exemplo presença de corredores verdes, combustíveis alternativos. METODOLOGIA: 1. Modelagem matemática utilizando teorias microecônomicas de competição (Cournot e equilíbrio de Statckelberg) com teoria dos jogos.2.. Simulação por eventos discretos de processos realizados em armazéns e portos.3. Modelagem fluido dinâmica computacional.4. Modelagem estocástica e dinâmica, através do uso de solvers numéricos e ferramentas computacionais.5.  Desenvolvimento de algoritmos de simulação e otimização integrados para planejamento em logística de óleo e gás offshore e estudo comparativo de benchmark de operações e instalações logísticas em atividades de óleo e gás offshore. INFORMAÇÃO COMPLEMENTAR: Natureza: Produto e Processo.Esforço resultante de 9 planos de trabalho na área de Logística & Produtos no exercício 2023. RESULTADO ECONÔMICO: Os ganhos econômicos do PD&I, previstos para desenvolvimento da área de Logística e produtos, decorrem da expectativa de aumento da eficiência operacional, aumento de receita com novos produtos e redução OPEX. RESULTADO INOVAÇÃO: Avanços em transporte e logística de petróleo e gás: redução de custos, maior confiabilidade e segurança. Modelo de previsão para gás natural, técnicas de valoração em mercados competitivos e metodologias para eficiência logística offshore e onshore. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto ""Modelos Avançados para Competição em Mercado de Gás"" começou com a formulação da hipótese de base e o conhecimento do mercado. Os parceiros foram mobilizados, e um plano inicial de entregas foi estabelecido. A valoração da oportunidade foi definida, e um plano de trabalho foi elaborado. Entrevistas com gestores foram realizadas, e o módulo 1 foi concluído. Conjuntos de dados para o Toy Estocástico e o Modelo Determinístico foram preparados, e a modelagem matemática do Modelo Determinístico foi realizada. O projeto ""HELI FX - Previsor de Movimentos em Helideques"" iniciou com o planejamento da EV e a definição das áreas de interesse. O entendimento do problema e a caracterização da entrega foram seguidos pela formulação da hipótese de pesquisa e a análise de sistemas de mercado. Análises preliminares para uma unidade marítima foram realizadas, e um plano de trabalho foi elaborado. O termo de cooperação foi assinado, e as atividades do TC foram iniciadas. O projeto ""Processos mecanizados e de alta produtividade para soldagem de dutos terrestres"" foi iniciado em setembro de 2023. Já definiu prazos e custos para a implantação da entrega e o escopo com ICT. No projeto ""Implantar metodologia multifásica para uso dos impelidores de tanques"", o problema ou oportunidade a ser resolvido foi descrito, seguido pela aplicação da tecnologia e o mapeamento e análise de soluções de mercado. O conhecimento do mercado foi consolidado. A assinatura do TC foi realizada e o desenvolvimento do procedimento experimental material e numérico foi iniciado. O projeto ""Tecnologias para aumento de eficiência em embarcações – Classe 1"" começou com a identificação das partes interessadas e a definição da metodologia a ser aplicada. Um termo de cooperação com ICT foi elaborado, e a valoração do cliente foi validada. O termo de cooperação foi assinado e um contrato de consultoria foram assinados. A caracterização da rota de navegação foi realizada e um relatório sobre tecnologias para navios foi enviado. O projeto ""Tecnologias para aumento de eficiência em embarcações – Classe 2"" seguiu um caminho similar ao do projeto ""Tecnologias para aumento de eficiência em embarcações – Classe 1"", com a identificação das partes interessadas, definição da metodologia, elaboração do termo de cooperação, validação da valoração do cliente, assinatura do termo de cooperação e do contrato de consultoria e a caracterização da rota de navegação. O projeto ""Cenários futuros para a redução de emissões GEE no transporte marítimo"" começou com a identificação das partes interessadas e a definição da metodologia a ser aplicada. Foi elaborado o escopo e feita a especificação técnica para a consultoria de avaliação de cenários. Posteriormente, foi elaborado um plano de trabalho e definida a valoração do cliente. Foi contratada uma consultoria para Pensamento Sistêmico e na validação do relatório da comissão de licitação. As partes interessadas foram mobilizadas e engajadas no projeto. O levantamento dos modelos mentais dos atores foi realizado, consolidando os avanços do projeto. O projeto ""Planejamento integrado das operações simultâneas da UEP"" envolveu a pesquisa em bases documentais e a descrição do problema. Entendido o problema foram identificadas as partes interessadas, e valorada a entrega de valor. Soluções de mercado foram analisadas. Recursos, operações e responsáveis foram mapeados e fontes de dados e sistemas foram identificados. A maturidade do processo e da ferramenta foi analisada, e dados dos PLSVs, RSVs, SDSVs, NTs, UMS, PSVs e helicópteros foram analisados. Condições de simultaneidade e restrições operacionais foram levantadas e uma análise dos recursos com a UEP foi realizada. Recursos, conflitos, gargalos e oportunidades foram priorizados. O projeto ""Reparos Alternativos em Dutos – Fase 2"" foi iniciado em setembro de 2023, e avançou com as simulações numéricas dos reparos propostos.																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 8 ID ÚNICO: 113231 NOME: Processamento de Gás, Química e Fertilizantes DESCRIÇÃO: Prover soluções tecnológicas para agregação de valor na área do processamento de gás, G&E, petroquímica e fertilizantes com foco em ganhos de eficiência energética, confiabilidade dos ativos, redução de custos de produção e desenvolvimento de produtos com maior valor agregado. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Gás natural, petroquimica, fertilizantes CO2, catálise, metanol, COF Desidrogenação oxidativa, ODH, Etano, Etano, óxidos mistos. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Novos catalisadores para conversão de CO2 a metanol. DESAFIO TECNOLÓGICO: Seletividade e eficiência de catalisadores de conversão de CO2 para metanol, bem como a escalabilidade de sua síntese e viabilidade técnica e econômica do novo processo. METODOLOGIA: 1. Revisão bibliográfica, realização de experimentos em escala de bancada, seleção de catalisadores, scale-up, termo de cooperação. INFORMAÇÃO COMPLEMENTAR: Natureza: Produto e Processo.Esforço resultante de 1 plano de trabalho na área de Processamento de Gás, Química e Fertilizantes no exercício 2023. RESULTADO ECONÔMICO: Novas tecnologias em gás, fertilizantes e petroquímica agregam valor. Destaque para receitas com venda de correntes ricas em olefinas (eteno, propeno, butenos) e metanol de baixo carbono, ampliando portfólio e potencializando ganhos econômicos.  RESULTADO INOVAÇÃO: 1- desenvolvimento de novos catalisadores e de um novo processo termocatalítico de conversão de CO2 a metanol. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto ""Partida de unidade de conversão de CO2 a metanol utilizando redes covalentes orgânicas (COF)"" apresentou avanços ao longo do ano de 2023. A pesquisa via cooperação tecnológica conseguiu êxito no preparo de redes covalentes (COF) com elevada área específica (964 m2/g), alta cristalinidade e razoável resistência térmica (280°C). Todavia, os testes catalíticos de desidrogenação utilizando COF como suporte apresentaram resultados aquém do esperado, razão pela qual optou-se por trabalhar, num primeiro momento, com suporte de zeólita USY, para num momento seguinte retomar o uso do suporte COF na reação de produção de metanol. Ao término dos experimentos, em junho de 2023, os catalisadores trimetálicos suportados em USY, a base de níquel, ferro e cério, não apresentaram atividade e estabilidade satisfatórias, sofrendo com a rápida formação de coque e bloqueio de seus microporos. Por não ter alcançado os resultados esperados e pela perspectiva da aplicação de COF como suporte catalítico ainda estar muito distante de um possível uso em processo industrial, optou-se por encerrar o projeto em dezembro de 2023.																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 9 ID ÚNICO: 113232 NOME: Produtos Sustentáveis DESCRIÇÃO: Desenvolvimento de tecnologias para produtos sustentáveis.                     1. Produção de Hidrogênio a partir de matérias primas renováveis2. Desenvolvimento do JET A com intensidade de carbono reduzida (LCAF)3. Reuso de RSU de plástico para economia circular do plástico através da integração ao refino                                                                                                                                                    4. Identificar e avaliar a oferta e a sustentabilidade de novas cargas renováveis para processamento em unidades de refino TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: 1.Hidrogênio, Biometano, Etanol, Óleo Vegetal, Nafta Verde 2.LCAF, intensidade de carbono, ciclo de vida, CORSIA 3.Economia circular, plástico, pirólise, Refino 4.Biorrefino, diesel renovável, BIOQAV, matérias primas. NATUREZA: Processo ELEMENTO TECNOLÓGICO: 1. Catalisador e Configuração de processos2. Viabilização da certificação formal do ciclo de vida da produção de JET A do poço ao tanque.                                                                                                                                                     3. Estudo de tecnologias para processamento do plástico e ajustes no coprocessamento para reinserção de resíduos plásticos de RSU, principalmente plásticos inservíveis não passíveis de reciclagem tradicional na cadeia de valor de produtos do refino.  Estudos de tecnologias de processamento, como por exemplo a pirólise, que permitam o coprocessamento em unidades de refino, para a produção de novos produtos petroquímicos ou outros produtos do refino com conteúdo circular. Um exemplo é o o uso de plástico flutuante ( um passivo ambiental) como matéria prima para produção de novos produtos petroquímicos ou do refino através do coprocessamento em refinarias de petróleo                                                                                                                                                4. Identificação de matérias primas graxas renováveis, com menor IC, como alternativa ao óleo de soja refinado. Desenvolvimento de tecnologias de tratamento das cargas para enquadramento às especificações do Refino. Desenvolvimento de tecnologias para disponibilização em grande volume de MP, por exemplo OGR. Validar a adaptabilidade das unidades de tratamento para as matérias-primas alternativas (MPA) e prever alterações de processamento. Estabelecer metodologia laboratorial para validação das MPAs para prevenir problemas operacionais ou desativação prematura do catalisador.  DESAFIO TECNOLÓGICO: 1. Evitar desativação do catalisador2. Infraestrutura necessária nas plataformas e unidades de refino para medições das emissões de gases de efeito estufa (GEE)                                                                                                                                                                                                     3. Enquadramento das correntes de plastico (RSU ou lixo flutuante) processadas ( ex. óleo de pirólise) em relação a contaminantes  e compostos derivados de matéria orgânica (oxigenados) para coprocessamento em refinaria, sem prejudicar o desempenho os catalizadores                                                                                                                                      4.Disponibilizar matéria prima graxa alternativa, com baixo IC, para substituir parcialmente o óleo de soja, reduzindo custo da carga e aumentando a sustentabilidade dos biocombustíveis gerados. METODOLOGIA: 1. Desenvolvimento de catalisador e teste em unidade piloto2. Levantamento dos dados de emissões, cálculo das intensidades de carbono dos óleos e da produção de JET A a partir da modelagem das UEPs e refinarias no sistema GABI, identificação de gaps na infraestrutura de medição de GEE e avaliação técnica da implementação de ações de mitigação de GEE.                                                                                                                        3. Estudos em bancada, piloto e protótipo ( 1t/dia) do processamento do plástico inservível, para geração de correntes com carbono circular, avaliadas por métodos analiticos quanto a sua composição em hidrocarbonetos, metais, oxigenados, cloro, entre outros. Estudos de processos de limpeza e tratamento destas correntes, além de testes em bancada e piloto quanto a compatibilidade de diferentes % de mistura, para coprocessamento em refino.                                                                                                           4. Estudos de bancada de tratamentos  fisico químicos e enzimáticos de MP graxas para produção de biocombustíveis. Avaliação por métodos analiticos quanto a sua composição, teor de fosfatos e possíveis contaminantes que possam prejudicar os catalisadores do refino. Estudos de processos de limpeza e tratamento destas correntes. Mapeamento de matérias primas disponíveis e potenciais para viabilização da redução do IC das cargas do refino. Desenvolvimento de software de logistica reversa de OGR, entre outros. INFORMAÇÃO COMPLEMENTAR: PB - Pesquisa Básica e PA - Pesquisa Aplicada.Natureza: Produto e Processo.Esforço resultante de 4 planos de trabalho na área de Produtos sustentáveis no exercício 2023. RESULTADO ECONÔMICO: Créditos de carbono na produção de hidrogênio, JET A de baixo carbono com maior valor agregado, derivados com conteúdo circular e biocombustíveis sustentáveis. Potencial para mercados nacionais e internacionais, atendendo mandatos de descarbonização. RESULTADO INOVAÇÃO: Inovações em catálise e processos para produção de hidrogênio e combustíveis de baixo carbono. Tecnologias para reciclagem de plásticos e uso de óleos e gorduras residuais. Software de logística reversa e identificação de matérias-primas sustentáveis DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :""No projeto """"Produção de H2 com baixa pegada de carbono através do coprocessamento de correntes renováveis em UGH"""" foram avaliados os custos e as emissões de CO2 do uso de etanol, nafta verde e biogás para a produção de H2. Um pedido de patente foi depositado no INPI relativo a um processo de produção de H2 a partir de matéria-prima renovável, que pode incluir etanol, glicerina, óleo vegetal e outras, integrado com a produção de Bio-QAV. Este depósito permite qualificar para a obtenção de incentivos fiscais ao projeto, reduzindo seus custos. Além disso, foram realizadas corridas exploratórias de integração da produção de Bio-QAV com a geração de carga renovável para a produção de H2, resultando em um pedido de patente depositado nos EUA. Finalmente, foi consolidada a avaliação técnica do uso de diferentes matérias-primas renováveis para a produção de H2, incluindo biometano, biogás, nafta verde e etanol, com a realização de experimentos em escala de laboratório quando necessário.O projeto """"Matérias-Primas Alternativas para o Biorefino - OGR e outras matérias graxas"""" teve diversos marcos importantes, como a passagem de TRL4 em 2023. Foram validados os protocolos para pré-tratamento de triglicerídeos (TG) para enquadramento de carga para planta dedicada (rota química e enzimática). Foi aprofundado o entendimento do mercado de óleo e gorduras residuais, com o desenvolvimento de softwares para logística reversa do OGR. Foram avaliadas amostras com potencial comercial para aplicação imediata em unidades de coprocessamento, de forma a avaliar possíveis contaminantes e necessidade de pré-tratamentos. Posteriormente, foram identificados parceiros de negócio, definindo aspectos comerciais e tecnológicos para o fornecimento de carga. A caracterização das amostras e a validação laboratorial do seu uso para processamento pela Rota HVO/SAF foram realizadas, culminando na validação laboratorial dos tratamentos das diversas cargas para processamento pela rota HEFA. Por fim, em 2023 foi concluída a primeira etapa do mapeamento de potenciais matérias primas alternativas (MPA), com foco na primeira planta dedicada de rota HEFA, definindo as 10 MPA de maior potencial nacional. O projeto de """" Economia Circular - Reuso de RSU de plástico"""" vem estudando a  reciclagem terciária do plástico inservível, como lixo flutuante ou o plástico do RSU. em 2023, foi iniciada parceria com a UFRJ, que inclui estudos em unidade protótipo de pirólise.  Foram avaliados locais para implantação da tecnologia, fornecedores e possíveis parceiros para a implantação, considerando a etapa de produção do óleo de pirólise e integração ao parque de refino. Foram identificadas duas empresas brasileiras, que já realizam a pirólise de RSU, incluindo o combustível derivado de resíduos (CDR), utilizando tecnologia própria. Termos de Acordo (TEA) estão sendo assinados para aprofundar o conhecimento dos produtos gerados. Além disso, foram realizadas análises de amostras de óleo de pirólise, caracterizando as frações geradas a partir da destilação e formulação de correntes ou produtos. A partir dos testes de incorporação de correntes de plástico em produtos asfálticos foi depositada patente de Asfalto com conteúdo circular. O projeto """"Comercialização de JET A com pegada de Carbono reduzida - LCAF"""" avançou com a disponibilização de uma proposta de modelo de quantificação da intensidade de carbono (IC) da produção de JET A. Esta atividade envolveu a quantificação do ciclo de vida do JET A, conforme as regras previstas pelo CORSIA, desmembrando os parâmetros que compõem a intensidade de carbono do E&P e do Refino, com implementação de sistemática de otimização das alocações de petróleo para a certificação do LCAF.""																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 10 ID ÚNICO: 113233 NOME: Tecnologia de Poços e Sondagem DESCRIÇÃO: Liderar, planejar e executar a pesquisa e o desenvolvimento de tecnologias de Engenharia de Poço nas áreas de perfuração, completação, barreiras de segurança, planejamento de recursos físicos, cimentação, fluidos, estimulação e produtividade de poços, pautadas em Segurança de Processo, Confiabilidade e redução de custos, integrando as competências necessárias na Petrobras, comunidade de C&T e indústria, a fim de implantar as soluções tecnológicas em conjunto com as Áreas de Negócio. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Poço, Construção, Intervenção, Abandono NATUREZA: Processo ELEMENTO TECNOLÓGICO: Disponibilizar tecnologias para a otimização do planejamento de recursos e redução dos custos unitários e totais da construção de poços marítimos (perfuração e completação). Disponibilizar tecnologias para a redução do custo de operação (OPEX) e abandono (ABEX) de poços marítimos. DESAFIO TECNOLÓGICO: Redução de custos de intervenção através de aumento de confiabilidade de sistemas, Redução de custos de abandono através de desenvolvimento de tecnologias em equipamentos e fluidos,Monitoramento e coleta de dados em tempo real em poços para aumento de produção e garantia de integridade. METODOLOGIA: A metodologia aplicada no desenvolvimento dos projetos elegíveis varia de acordo com o contexto de aplicação, maturidade atual, existência de parceiros de mercado e a tipologia. Em essência, os projetos são desenvolvidos a partir de uma análise de mercado, com foco na identificação da maturidade (TRL) atual e sua aderência aos requisitos necessários. Diante dessa fotografia são definidas rotas necessárias para atingimento dos objetivos propostos. Essa etapa é iterativa pois é necessária a identificação de parceiros (universidades e empresas) para definição de prazos e custos. Finda a fase de definição e contratação dos parceiros, inicia-se a pesquisa propriamente dita com as fases de avaliação de conceitos, construção e validação em escala de laboratório, avaliação em escala de subsistema/sistema e avaliação em campo.No caso da tipologia de software, temos algumas diferenças.  A metodologia de pesquisa utilizada para desenvolvimento do software PWO Custos (Acompanhamento Financeiro e Orçamento de POCOS automáticos) inclui:1) revisão da literatura, para identificar estudos existentes e lacunas, 2) definição do problema de pesquisa, 3) escolha da abordagem de pesquisa (qualitativa, quantitativa ou mista) e seleção de instrumentos de coleta de dados, 4) planejamento do experimento, 5) coleta e análise dos dados, 6) avaliação e discussão dos resultados, 7) conclusão e sugestões de melhorias futuras.2) Para a rota de desenvolvimento metodológico, dedicamos atenção ao problema de Execução de Início de Poços sem Uso de Sondas de Perfuração.Para tal desenvolvemos um aparato experimental capaz de simular o comportamento do solo frente ao revestimento condutor de um Poço de Petróleo. Onde focamos:- Início de Poço Jateado- Início de Poço Perfurado e Cimentado3) Para a rota de redução de CAPEX, está em desenvolvimento a EV de Perfuração em Carretel, que possui o potencial de redução significativamente o tempo de construção de poço, para tal, serão desenvolvidos 3 grandes equipamentos:- Removedor alternativo de rocha- BHA tipo tractor- Coluna de perfuração flexível4) Para a rota de redução de OPEX, estão em desenvolvimento os projetos de robótica para eliminar uso de sonda para light workover, desenvolvimento de válvulas elétricas para injeção química e gás lift.5) Para a roda de ABEX, estão em desenvolvimento o uso de plasma para a destruição de coluna de produção e remoção de acessórios.   INFORMAÇÃO COMPLEMENTAR: PB - Pesquisa Básica, PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Natureza: Produto e Processo.Esforço resultante de 12 planos de trabalho na área de Tecnologia de Poços e Sondagem no exercício 2023. RESULTADO ECONÔMICO: Os ganhos econômicos do PD&I, previstos para desenvolvimento da área de Engenharia de Poço, decorrem da expectativa de aumento de eficiência operacional, redução de OPEX, redução de CAPEX e redução de ABEX. RESULTADO INOVAÇÃO: Avanços em perfuração e completação offshore reduzem CAPEX, OPEX e ABEX, melhorando confiabilidade e integridade operacional. Novas tecnologias otimizam sistemas de produção, impactando positivamente custos e eficiência.  DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto ""DATS"" alcançou marcos importantes, como a formulação de hipóteses e princípios, caracterização do projeto, e conhecimento do mercado. Revisões dos conectores ANM/TH e do sistema de monitoramento foram feitas em colaboração com o mercado. A tecnologia foi aprovada em SELEPOÇO, com reuniões de incorporação ao SEAP. A reformulação do Termo de Cooperação (TC) com a UTFPR foi crucial, dependendo de aprovação em CTO e prestação de contas. A assinatura do TC para desenvolver o sistema de feedthrough entre ANM/TH e ferramentas de instalação, além do conector de fundo para descida em dual trip, enfrentou desafios nas especificações técnicas e definição de rotas com empresas parceiras. O projeto aderiu ao JIP SEAFOM. O projeto ""Robin - Light Workover Autônomo"" definiu hipóteses e princípios básicos, formulou a aplicação da tecnologia e avaliou a prontidão do mercado. A proposta de valor foi estabelecida e termos de cooperação foram assinados. O projeto ""Perfuração em Carretel"" assinou um TC com a UFRJ para desenvolver a modelagem da dinâmica da perfuração em carretel, com os primeiros modelos evoluídos na parceria. O projeto “Plasma Milling” trata do uso de plasma para corte de colunas, focando em atividades de abandono. Em 2023, concluiu o TRL 3 com a entrega do projeto conceitual, incluindo equipamentos de fundo e superfície. O uso do plasma reduz o tempo operacional, mas os equipamentos de superfície requerem redesenho das instalações, sendo um ponto crítico para a continuidade do projeto. No projeto “PAA - Remoção da Coluna e Acessórios”, a empresa Clearwell foi mapeada para desenvolver uma rota de corte utilizando Lança Térmica, com negociação do escopo e modelagem contratual. O projeto ""VGL-e: Válvula de Gás Lift Elétrica"" desenvolveu a primeira versão da especificação técnica, assinou um TC sob gestão da Shell e atingiu o TRL-2 após análises de modos de falha e efeitos, requerendo um plano de testes abrangente. O projeto “MIQ-e: Mandril de Injeção Química Elétrico” visa à eletrificação completa do poço. Em 2023, diligências com empresas parceiras resultaram na contratação da SLB ao final do ano. O projeto ""PWO Custos: Acompanhamento Financeiro e Orçamento de Poços Automáticos"" definiu um parceiro e plano de trabalho, desenvolveu um motor de cálculo com regras customizadas e integrou a versão alfa com diversos sistemas. A versão beta foi lançada para orçamento executivo e acompanhamento financeiro, com novas integrações previstas. A contratação de um novo TC, Smart Cost, foi iniciada. No projeto de “Fluido de Completação Estimulante”, houve revisão de tecnologias aplicáveis, levantamento bibliográfico e histórico de materiais. Estudos preliminares indicaram cenários adequados para tratamento de baixo volume, alinhados com o reservatório. A formulação de fluidos foi mapeada e um estudo preliminar publicado, aguardando retorno de empresas e universidades. O projeto de ""Ferramentas Ultrassônicas para Remoção de Obstruções em Poços"" avançou significativamente com a caracterização, conhecimento do mercado, formulação de hipóteses e aplicação de tecnologia. No projeto ""Fluido e-CSB para Workover"", a formulação base do fluido foi definida e aprovada nos ensaios de performance conforme padrões da companhia. No projeto “Início de Poço sem Sonda”, foi assinado um TC com a COPPE/UFRJ e realizada vistoria no laboratório e campo experimental. Ajustes de SMS foram feitos e a matriz de testes remanescente para análise dinâmica de condutores jateados foi definida.																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 11 ID ÚNICO: 113234 NOME: Tecnologia de Refino DESCRIÇÃO: Prover soluções tecnológicas para agregação de valor na área de Processos de REFINO, com foco em ferramentas de tomada de decisão, ganhos de eficiência operacional e energética, confiabilidade e rendimento de produtos de maior valor agregado. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Refino, tecnologia, Digital Twin, Eficiência Energética, Confiabilidade NATUREZA: Processo ELEMENTO TECNOLÓGICO: 1. Catalisador de FCC contendo zeólita com sítios ativos mais acessíveis possibilitando uma maior conversão de fundos e maior seletividade a LCO e Gasolina2. Modelo para otimização do sistema catalítico de uma UFCC com adequação de hardware em um sistema com mais de uma formulação de catalisador virgem3. Desenvolvimento de aditivos de catalisadores de FCC com maior acessibilidade para aumento da produção de propeno4 – Aplicação de métodos de inteligência artificial para diagnosticar a confiabilidade de feixes de trocador de calor, utilizando dados de processo.5 – Desenvolver modelo matemático-computacional para prever a ocorrência de erosão e corrosão eletroquímica em linhas de escoamento de unidades do refino.6 – Desenvolver modelos de iteração fluido-estrutura para simulação de escoamento em feixe de tubos de caldeira7 - Desenvolver algoritmo para diagnóstico do estado atual dos compressores, visando eficiência energética e confiabilidade.8 - Desenvolver ferramenta para gestão do ciclo de vida de sistemas de automação industrial.9 - Desenvolver soluções de otimização de mistura de produtos visando maximizar as receitas/rentabilidade de unidades do refino.10 - Desenvolver soluções de otimização de mistura em linha de correntes que compõem o pool de óleo diesel em uma refinaria 11 - Avaliar a influência da da qualidade de cargas e condições operacionais das unidades de coqueamento retardado na morfologia do coque obtido em refinarias12- Garantir melhor desempenho do catalisador de FCC através do aumento da atividade da sua matriz ativa13 - Desenvolver soluções de processo baseadas em emprego de aditivos antioxidantes para garantia de estabilidade de cargas de unidade de HDT de Médios. DESAFIO TECNOLÓGICO: 1 -  Achar o valor ótimo da acessibilidade dos sítios ativos da zeólita uma vez que a maior acessibilidade dos sítios ativos em geral implica em redução do número dos mesmos2 - Garantir que o modelo de previsão seja assertivo para que a otimização do sistema catalítico traduza em ganhos de receita. Ter disponível no prazo da solução uma opção de hardware de carregamento de catalisador que atenda às necessidades deste desenvolvimento3. Processo com alto rendimento na formação de mesoporos. Garantir desempenho do aditivo após desativação4 – Otimização da rotina de inspeção de feixes de trocadores de calor, baseado em diagnóstico de confiabilidade dos feixes fornecido por métodos de inteligência artificial5 – Permitir avaliar de forma quantitativa a formação de corrosão em escoamentos em linha de processos de refino que possam ocasionar alteração de condições operacionais.6 – Determinar a capacidade máxima de vibração nos feixes de tubos das caldeiras, evitando paradas não programada por danos causados por vibração7 - Monitoração preditiva do desempenho de compressores instalados em unidades operacionais8 - Desenvolver solução digital para suportar a manutenção e o planejamento da gestão de ativos dos sistemas de automação9 - Desenvolver modelos não lineares representativos do comportamento da mistura de produtos/derivados de refinaria.10 - Desenvolver modelos não lineares representativos do comportamento da mistura de correntes que compõem o pool de óleo diesel de uma refinaria.11 - Identificar e correlacionar propriedades físico-químicas das cargas e de parâmetros operacionais na qualidade do coque obtido em undades industriais de coqueamento retardado.12 - Modificar a matriz com rota de baixo custo para aumento de conversão de fundos.13 - Identificar e validar aditivo antioxidante com desempenho adequado para minimizar formação de depósitos em cargas de HDT de Médios contendo componentes instáveis, tais como, correntes na faixa de destilação do óleo diesel provenientes da unidade de coqueamento retardado. METODOLOGIA: 1 - Revisão bibliográfica, testes de validação de conceito em escala de bancada e piloto, seleção de catalisadores, avaliação de cargas2 - Desenvolvimento do modelo de ML em termo de cooperação e prospecção de hardware que atenda à demanda de múltiplos compartimentos com capacidade suficiente às necessidades3 - Testes de zeolitas com diferentes graus de cristalinidade de modo a poder avaliar o efeito do grau de acessibilidade dos sítios ativos da zeólita bem como o número dos sítios propriamente dito, scale-up4 - Desenvolvimento de sistema de diagnóstico da confiabilidade de feixes de trocadores de calor baseado na análise de dados de processo via uso de métodos de inteligência artificial 5 - Incorporação de modelos matemáticos para predição de erosão e corrosão eletroquímica em escoamentos em linha na plataforma de simulação fluidodinâmica MFSIM.6 - Desenvolvimento de ferramenta computacional para simulação das iterações fluido-estrutura7 - Desenvolvimento de ferramenta computacional para diagnóstico em tempo real de compressores instalados em unidades operacionais.8 - Desenvolver solução digital para diagnóstico da saúde dos sistemas de automação de unidades industriais9 - Realizar otimização em tempo real da mistura de produtos reduzindo perdas de produtos/derivados de refinarias.10 - Realizar otimização em tempo real da mistura de correntes que compõem o pool de óleo diesel de uma refinaria, reduzindo perdas de produção.11 - Realizar experimentos em escalas de bancada, piloto e industrial de modo a identificar a relação entre propriedades de carga e condições de processo com a qualidade e morfologia do coque obtido.12 - Realização de experimentos de síntese e avaliação catalítica em escalas de laboratório, piloto e scale-up13 - Realização de testes em bancada representativos da adição de produto químico para melhoria de estabilidade de cargas de HDT de Médios. INFORMAÇÃO COMPLEMENTAR: Natureza: Produto e Processo.Esforço resultante de 13 planos de trabalho na área de Processos de Refino no exercício 2023. RESULTADO ECONÔMICO: Implantação de novas tecnologias ou melhoramento de tecnologias existentes que agreguem valor na área de processos de refino. RESULTADO INOVAÇÃO: Insumos para testes em bancada e em escala piloto e outras análises laboratoriais que suportam a pesquisa, desenvolvimento e inovação no tema Processos de Refino. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :No projeto ""Catalyst as a Service Fit for Purpose"" definiu-se o fornecedor do carregador de catalisador, disponibilizaram-se modelos de machine learning para previsão de rendimentos, e elaborou-se um relatório de viabilidade técnica do protótipo. No projeto ""Análise de interação fluido-estrutura em banco de tubos de caldeiras"" concluiu-se a definição das premissas de modelagem e ajustes nas premissas de valoração. No projeto ""Utilização de Inteligência Artificial Visando o Aumento da Confiabilidade e Diminuição de Custos em Inspeção Baseada em Risco"" concluíram-se os resultados de inspeção e análises comparativas de tubos das refinarias. No projeto ""Modelagem matemática e computacional de processos de corrosão e erosão em superfícies metálicas"" definiram-se as premissas de modelagem e ajustaram-se as premissas de valoração. No projeto  ""Desenvolvimento zeólita ZSM-5 de maior acessibilidade para aumento de propeno"" prepararam-se zeólitas em laboratório, adiou-se o scale-up, prepararam-se e caracterizaram-se aditivos, e realizou-se avaliação catalítica. No projeto ""Aumento de desempenho de catalisadores de FCC através de desenvolvimentos em zeólita Y"" prepararam-se e modificaram-se zeólitas, realizaram-se testes de estabilidade, caracterizações e análises de resultados. No projeto ""Metodologia para avaliação da modernização de compressores"" realizou-se planejamento, alinhamento com áreas clientes, especificação de caso de REVAMP, mapeamento de mercado, elaboração de estratégia, priorização de backlog, modelagem de metodologia e codificação de algoritmo. No projeto ""Asset performance management aplicado a Sistemas de Automação Industrial"" selecionou-se ICT parceira, elaborou-se especificação de requisitos, compararam-se soluções de mercado, realizou-se planejamento e celebrou-se termo de cooperação. No projeto ""Otimização multibatelada de mistura de produtos"" elaborou-se modelo representativo, realizaram-se avaliações de uso do otimizador, definiram-se diretrizes para modelos de qualidade e consolidaram-se regras de mistura. No projeto ""Otimização de mistura em linha de diesel"" preparou-se material e abriu-se edital de licitação da tecnologia a ser avaliada. No projeto ""Avaliação da influência da qualidade da carga e condições operacionais da UCR na morfologia do coque"" iniciaram-se corridas em planta piloto e analisou-se operação de unidade industrial. No projeto ""Modificação de matriz para aumento de conversão de fundos"" prepararam-se amostras em laboratório com modificações de metais e Si, e iniciaram-se caracterização e medidas de acidez. No projeto ""Estabilidade de cargas para Unidade de HDT Médios (U-2500) do GásLub"" assinou-se TEA com fornecedor de aditivo anti-oxidante e iniciaram-se testes laboratoriais de envelhecimento. Esses projetos demonstram um foco significativo em melhorias de processos, otimização de catalisadores, desenvolvimento de novas metodologias e aplicação de tecnologias avançadas como inteligência artificial e machine learning na indústria petroquímica. Os avanços relatados incluem desde preparações em escala laboratorial até implementações em unidades industriais, passando por modelagens computacionais, testes em plantas piloto e desenvolvimento de ferramentas de otimização. Muitos dos projetos envolvem parcerias com fornecedores, ICTs (Instituições Científicas, Tecnológicas e de Inovação) e áreas clientes internas, demonstrando uma abordagem colaborativa para a inovação. Além disso, há um foco claro na melhoria da eficiência operacional, aumento da confiabilidade dos equipamentos e processos e otimização do uso de recursos. Os projetos abrangem diversas áreas do refino, incluindo unidades de FCC (Fluid Catalytic Cracking), HDT (Hidrotratamento), coqueamento retardado, e sistemas de automação industrial. Há também um foco significativo no desenvolvimento e melhoria de catalisadores, especialmente zeólitas, visando aumentar o rendimento de produtos desejados como propeno. 																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 12 ID ÚNICO: 113235 NOME: Tecnologia de Superfície / Topside DESCRIÇÃO: Projetos de pesquisa e desenvolvimento de tecnologias nas áreas de operação, projeto, fabricação, construção e montagem naval e industrial de sistemas Topside, para o aumento de produtividade, qualidade e redução de custos, integrando as competências necessárias na Petrobras, comunidade de C&T e indústria a fim de implantar as soluções tecnológicas em conjunto com as Áreas de Negócio. Envolve desenvolvimento de equipamentos e ferramentas digitais voltadas a esta macro temática. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Processamento, Tratamento, Medição, Naval, Projeto, Otimização, Eficiência. NATUREZA: Processo ELEMENTO TECNOLÓGICO: Para a temática em questão, referem-se a novos equimentos, uso de novas técnicas e novas ferramentas digitais nas áreas de operação, projeto, fabricação, construção e montagem naval e industrial de sistemas Topside DESAFIO TECNOLÓGICO: 1. Eficiência de separação/tratamento, de modo a se reduzir o footprint das plantas de tratamento de fluidos (O/A/G) em unidades estacionárias de produção (UEPs) garantindo simultanemanete a qualidade dos fluidos para exportação,2. Incremento da confiabilidade e acurácia dos sistemas de medição de correntes produzidas (O/A/G),3. Redução do consumo energético para processamento e medição de fluidos em ambiente offshore METODOLOGIA: 1. Simulação e modelagem de fenômenos e processos,2. Prova de Conceito de metodologias, equipamentos, produtos químicos e arranjos de processo em escala de laboratório (TRL 1-3),3. Validação em escala piloto de metodologias, equipamentos, produtos químicos e arranjos de processo (TRL 4-6),4. Avaliação e disponibilzação de tecnologias em escala de processo industrial (TRL 7-9). INFORMAÇÃO COMPLEMENTAR: PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Natureza: Produto e Processo.Esforço resultante de 22 planos de trabalho na área de Tecnologia de Superfície / Topside no exercício 2023. RESULTADO ECONÔMICO: Os ganhos econômicos do PD&I, previstos para desenvolvimento da área de Engenharia de Processamento e Instalação de Superfície, decorrem em geral da expectativa de aumento de eficiência operacional e na redução de CAPEX das novas unidades. RESULTADO INOVAÇÃO: Avanços em equipamentos de superfície, processos em UEPs e sistemas de medição. Produtos químicos mais eficientes e otimização do escoamento de fluidos. Melhorias na robustez e segurança dos sistemas de produção offshore. DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :O projeto “Robô escalador para pintura e soldagem em locais remotos” iniciou com levantamento do estado da arte, validação de componentes e construção de modelo de prova de conceito. O projeto “RPAS (Drone aéreo) para Inspeção Visual e Medição de Espessura de tanques (Projeto Beebot)” avançou com seleção de proposta, aquisição de equipamentos e validação em laboratório. O projeto “Inclusão de alternativa de tecnologia de desaeração de água do mar por membranas contatoras nos projetos de afretadas e próprias.” iniciou planejamento de testes em escala piloto em parceria com a 3M. O projeto “Dispositivo Ultrassônico para Limpeza de Incrustações em Equipamentos  e Tubulações de Plataformas de Petróleo e Gás” progrediu com testes de campo, validação do plano de implantação e definição de plataformas piloto. O projeto “Sistema de Tratamento e Monitoramento de Óleo de Mancais e Monitoramento e Selos” apresentou primeiro protótipo funcional com testes em bancada experimental. O projeto “Antiespumante com baixo teor de silício para cenário pré-sal” definiu rota estratégica para desenvolvimento e implantação, incluindo processo de pré-qualificação. O projeto “Sistema de Riser de Captação de Água para Grandes Profundidades” avançou com projeto conceitual, mapeamento de fornecedores e testes experimentais. O projeto “Redução da Ociosidade de PLSVs (BANIT)” realizou testes de manobras com novos limites operacionais e primeira implantação. O projeto “[DT ANC] - Módulo 08: Sistema de Integração e Gerenciamento dos Algoritmos do Gêmeo Digital de Ancoragem” desenvolveu protótipos, validou arquitetura e realizou implantação para P-75. O projeto “Requisitos para recebimento de componentes, tubulação e equipamentos trabalhados a quente em superduplex” realizou experimentos em escala protótipo/demonstração. O projeto “Desenvolvimento de Metodologias de Inspeção de Tubulações com Reparos em Compósito” iniciou cooperação com LAMEF (UFRGS) e realizou ensaios preliminares. O projeto “OADS (Oil Anomalies Diagnosis System)” concluiu protótipo e iniciou fase piloto. O projeto “UBUNTU - Robô colaborativo para manutenção de detectores de F&G” avançou com simulações, projeto mecânico, fabricação e testes em ambiente relevante. O projeto “Sistema portátil de medição de vibração sem contato para aplicação em unidades industriais com pisos não inerciais” realizou caracterização inicial de incertezas e desenvolveu atuadores passivos e ativos. O projeto “Neutralizadores de Vibração Viscoelásticos - Fase 2” avançou no levantamento de estado da arte e definição de premissas técnicas. O projeto “Limpeza Ultrassônica de Incrustações em Flotadores e Hidrociclones em Plataformas de Petróleo” progrediu com seleção de parceiro para cooperação. 																								
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 13 ID ÚNICO: 113236 NOME: Tecnologia em SMS DESCRIÇÃO: P&D nos mercados de Segurança (processo e ocupacional), Meio Ambiente e Saúde, com o SMS como incorporador ou sponsor ativo. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Redução de HHER (homem hora exposto ao risco) redução de resíduo. NATUREZA: Processo ELEMENTO TECNOLÓGICO: 1 - Implementação de um sistema para supervisão e apoio a guindastes baseado em inteligência artificial para análise de riscos e alertas com possível implantação em guindaste real.2 - Desenvolver uma forma mais eficiente de realizar a limpeza dos tanques, recuperar o óleo das borras e destinar de forma segura os demais componentes, em especial metais3 - Implementação de kit de dispositivos ativos de segurança para o processo de hidrojateamento em ultra alta pressão, com desenvolvimento de um protótipo para integração a pistolas de hidrojateamento e caixas de potência, constituindo um sistema que visa aprimorar a segurança e eficiência dessa atividade em unidades operacionais da indústria em geral. 4-Validação de metodologia de inspeção de visual de Coral Sol atrás ve miniROVS 5- Construção de modelos de crescimento de bioincrustação e resistencia de embarcações 6- Geração de produtos digatais para apoiar a curadoria das informações geradas em campanhas de inspeção e identificação automatica das espécies invasoras.7 - Desenvolvimento de ferramenta robótica capaz de realizar remoção de macroincrustação de casco de embarcações offshore. 8 - Sistema autônomo de apoio às operações de contenção e recolhimento de óleo derramado, que emprega visão computacional, inteligência de enxame e sensores avançados, DESAFIO TECNOLÓGICO: 1 - Desenvolvimento de sistema para identificação automática de riscos para auxílio na operação de guindastes2 - Promover uma  limpeza de tanques que supere o emprego de força manual e diminuir os custos com destinação de borras com catalisador  para coprocessamento3 -  Desenvolvimento de sistema de monitoramento ativo da atividade de hidrojateamento com intervenção automática na operação em caso de riscos aos trabalhadores. 4-Utilização de robotótica submarina para substituir a atividade de risco relacionada a inspeção visual de coral sol realizadas atualmente por mergulhadores5-Criação de modelos para melhorar a compreenção dos fenômenos de crescimento de bioincrustação e resistencia das embarcações que auxiliarão na compreenção de fenômenos relacionados ao consumo de combustível e emissões6-Construão de produtos digitais para aramazenamento de informações auxiliarão na melhoria da gestão ambital e dos processos de gestão internos7 - Desenvolver sistema capaz de se fixar no casco mesmo em condições meteorológicas offshore, realizar a remoção com contenção sem perda de material orgânico para o ambiente. 8- Desenvolvimento do sistema autônomo de apoio ao lançamento/recolhimento de barreiras de contenção de óleo bem como a integração de navegação autômoma em ambiente offshore . METODOLOGIA: 1 - Identificação e modelagem de elementos e cenários presentes nas atividades de operação de guindastes, proposição de arranjo de sensores capazes de perceber os elementos e cenários, implementação de modelos de digital twin capazes de representar computacionalmente os elementos e cenários, desenvolvimento de algoritmos de segmentação, rastreio e predição de movimento dos elementos presentes nos diferentes cenários, , integração de predições do cenário com simulação do guindaste de forma a prever situações de risco à profissionais e instalações, e proposição de sinalizador  de alarmes de situação de risco ao operador.2 - Foram realizados testes de bancada para solubilizar o catalisador com um quelante ou através de um ataque ácido, reduzindo a viscosidade e permitindo a remoção da borra.3 - Análise da atividade e de seu histórico de sinistros, definição dos eventos monitoráveis, especificação dos sensores e microprocessadores adequados, integração dos dispositivos, compatibilização do sistema às condições severas de uso e adequação do mercado.4-Foram realizados levantamento bibliográficos para a construção dos modelos de crescimento da bioincrustação e resistencia baseados em lógica fuzzi, campanhas de inspeção de teste da tecnologia de miniROV no Porto do Açu e na Baia de Guanabara, construção da arquitetura do produto digital. 5 - Foi utilizado metodologias ágeis para o desenvolvimento de unidade robótica com prototipação de conceitos seguidos de teste de bancada para validação experimental de parâmetros. 6 - TC de desenvolvimento de solução com a UFRJ com levantamento de soluções, simulações numéricas, testes em escala piloto e em escala de protótipo até o nível semi-comercial. INFORMAÇÃO COMPLEMENTAR: PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Natureza: Produto e Processo.Esforço resultante de 5 planos de trabalho na área de Tecnologia em SMS no exercício 2023. RESULTADO ECONÔMICO: Resultados em segurança e meio ambiente: recuperação de óleo gera ganhos financeiros, redução de HHER melhora gestão ambiental, mitigação de riscos de multas por derramamento. Benefícios econômicos alinhados à responsabilidade socioambiental.   RESULTADO INOVAÇÃO: Segurança e eficiência: robótica para movimentação de cargas e inspeção submarina, limpeza de tanques otimizada, hidrojateamento seguro e contenção de vazamentos. Redução de HHER, riscos e impacto ambiental. Economia de combustível em embarcações.  DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :No projeto ""Pistola Segura para Hidrojato - Eliminação de Acidentes Graves"", a elaboração da notificação de invenção foi realizada e submetida pelo sistema SIAPI. A especificação de uma nova pistola de hidrojato com sensores de segurança foi reprogramada devido ao atraso na conclusão do MVP do kit de adaptadores e patente. O pedido de patente para o sistema e método de segurança para pistolas de hidrojateamento foi depositado. O projeto foi divulgado no evento CCipra 2023 e apresentado no congresso. Com a alteração na sistemática de envolvimento de interessados, foi necessário definir um novo plano de implementação para a tecnologia. A conclusão do protótipo e testes em ambiente laboratorial foi realizada, e um RFI para o mercado foi emitido para identificar potenciais fornecedores. O projeto ""iCrane - Guindaste Inteligente e Seguro"" teve a documentação do projeto conceitual, incluindo a descrição do sistema, postergada devido à necessidade de verificação de testes de integração dos dispositivos e módulos que compõem a solução. Os testes de integração, inicialmente previstos para serem realizados nas instalações da P-58, foram reprogramados para testes laboratoriais representativos devido a requisitos e restrições de TIC. O projeto ""Robô submarino para remoção de incrustações"" começou com a emissão de um relatório do estado da arte dos métodos e tecnologias. A pesquisa do estado da arte foi iniciada, seguida pela avaliação de potenciais parceiros, incluindo empresas norueguesas e sul-coreanas, para encurtar a rota de desenvolvimento do robô. A avaliação das opções com o jurídico para a entrada da Eco Subsea no TC foi realizada. Um relatório conceitual do robô e do SMTE foi emitido. O projeto ""Monitoramento Visual Diverless"" teve diversos avanços ao longo do tempo. Inicialmente, foi realizada a assinatura do Termo de Cooperação (TC), seguida pela montagem de um experimento completo para substituir a inspeção de mergulho, incluindo a obtenção de imagens e um laudo de um biólogo externo à Petrobras. A definição da arquitetura de referência e requisitos técnicos para os produtos digitais foi postergada devido ao atraso na assinatura do TC, assim como o planejamento da sustentação desses produtos. A análise preliminar das embarcações, a integração dos dados e a curadoria das informações também foram adiadas pelo mesmo motivo. A validação da arquitetura dos produtos digitais, a prova de conceito das variáveis críticas e o relatório de análise biológica e energética seguiram o mesmo destino. A análise do contrato atual (LOEP) foi realizada para entender as obrigações de inspeção e a possibilidade de determinar uma nova forma de inspecionar. O desenvolvimento do modelo de crescimento de bioincrustações e perda de eficiência, o plano de implantação acordado com o cliente, o planejamento da aquisição de informações das embarcações e a integração dos dados, bem como o planejamento das campanhas de desenvolvimento dos modelos, foram todos postergados. O planejamento preliminar da análise dos dados e cálculos, as campanhas de aplicação em campo e a apresentação dos resultados dos modelos aos clientes também foram adiados. O primeiro uso do modelo de contratação pelo cliente e o planejamento e alinhamento para proposição de medidas seguiram o mesmo caminho. O projeto ""Sistema autônomo de apoio a contenção"" teve a abertura de uma oportunidade no Ambiente de Competitividade. Está em fase inicial de avaliação da compatibilidade da tecnologia disponível, e posterior desenvolvimento dos gaps tecnológicos.																								"
28610;2023;CNPJ: 33000167000101 RAZÃO SOCIAL :PETROLEO BRASILEIRO S A PETROBRAS ATIVIDADE ECONOMICA :Fabricação de produtos do refino de petróleo Cd ATIV.ECONOMICA IBGE :C.19.21-7/00 PORTE Demais ID EMPRESA/ANO :28610;NÚMERO: 14 ID ÚNICO: 113237 NOME: Tecnologia Submarina DESCRIÇÃO: Liderar, planejar e executar o desenvolvimento de pesquisas tecnológicas na área de riser, dutos flexíveis, dutos rígidos e equipamento e processamento submarinos, controle e elétrica submarina, bem como o desenvolvimento de métodos para análise estrutural, de integridade e de instalação de dutos, risers e equipamentos submarinos, controle e elétrica submarina integrando as competências necessárias na Petrobras, comunidade de C&T e indústria. TIPO (PA ou DE): PA – Pesquisa Aplicada AREA: Outros PALAVRAS CHAVE: Submarina, Elétrica, Dutos, Risers, Equipamentos NATUREZA: Processo ELEMENTO TECNOLÓGICO: 1. Ferramentas submarinas de aquecimento para dissociação de hidratos externos.2. ferramentas para monitoramento de flexíveis3. Novos conceitos de arvore de natal molhada elétrica de baixo custo4. Metodologias para otimização do tempo de inspeção baseadas em risco IBR-FLEX5. Processamento de Imagens para inspeção e monitoramento da integridade de sistemas submarinos6. robotica para substituição de atividades de mergulho com Manipulação Avançada7. Modelo Logístico para definir fluxo otimizado do descomissionamento de linhas submarinas8. Sistema de Geração de Energia Elétrica Submarina por aproveitamento de pressão do reservatório - SGEES9.Conector Mecânico para Risers de Produção/Injeção10. Mecanismo diverless de 2ª geração para suportação de risers flexíveis (BSDL-SI & TSUDL)11. Desenvolvimento de Cabo MLE de 4,6/8kV e de Lower Connectors para Skid-BCSS12. Integridade de Juntas Dissimilares em Equipamentos Submarinos. DESAFIO TECNOLÓGICO: 1. Formação de hidratos em conectores submarinos, garantir o aquecimento da estrutura do equipamento submarino, evitando danos  a vida marinha e a infraestrutura submarina.2. Monitorar parametros usados em avaliação de integridade de dutos flexiveis (alagamento, arames rompidos, etc)3. Altos custos de arvore de natal molhada elétrica de baixo custo4. Realizar inspeções em periodicidade mais adequada com base em avaliações de risco5. Maior confiabilidade na analise de resultados de inspeção e monitoramento da integridade de sistemas submarinos6 Reduzir atividades de mergulho com Manipulação Avançada7. Otimizar fluxo do descomissionamento de linhas submarinas8. Gerar energia elétrica no fundo do mar aproveitando a diferença de pressão entre o reservatorio e o ambiente submarino. 9. Desenvolvimento de conector mecânico para utilização em dutos rígidos submarinos de Produção/Injeção possibilitando intercabialidade e flexibilidade10. desenvolvimento de mecanismo automatizados para suportação de risers visando a redução de atividades de mergulho11.Utilização do material de isolamento MLE para ambientes submarino. 12. Desenvolvimento de metodos e processo para verificação dantegridade de Juntas Dissimilares em Equipamentos Submarinos, considerando diferentes ligas. METODOLOGIA: 1. Desenvolvimento de prototipo de novas ferramentas junto aos fornecedores e as Universidades.2. Estudos conceituais da aplicação submarina, testes em planta piloto.3. Estudos conceituais e testes laboratoriais de aplicação de revestimento especializado4. Testes de Campo para validar tecnologias. INFORMAÇÃO COMPLEMENTAR: PA - Pesquisa Aplicada e DE - Desenvolvimento Experimental.Natureza: Produto e Processo.Esforço resultante de 10 planos de trabalho na área de Tecnologia Submarina no exercício 2023. RESULTADO ECONÔMICO: A antecipação de receita, maior segurança e eficiência, redução de perdas, ABEX, CAPEX, OPEX e impacto ambiental. Aumento de receitas e diminuição de homem-hora exposto ao risco, otimizando operações e resultados financeiros. RESULTADO INOVAÇÃO: Novas Tecnologias submarinas: ferramentas, proteção corrosiva, monitoramento e gestão de integridade. Energia alternativa, softwares de análise e novos equiptos. Robôs de inspeção e sistemas que reduzem mergulhos, aumentando segurança e eficiência.   DESCRIÇÃO RH:  DESCRIÇÃO MATERIAIS:  SETOR PARA ANALISE DO PROJETO: Química e Farmácia;"CICLO MAIOR QUE 1 ANO: Sim ATIVIDADE PDI CONTINUADA ANO ANTERIOR :No projeto “Sistema de ANM elétrica”, a segunda fase adicionou exigências gerenciais, levando à revisão das documentações técnicas e à criação de um grupo de trabalho para avaliar riscos do desenvolvimento. O objetivo é desenvolver três protótipos para teste-piloto em campo como ""stand alone"". O projeto ""Tecnologia de Dissociação de Hidrato Interno em Sistemas Submarinos"" avançou com a definição da hipótese de base e princípios, além de caracterização do mercado. A aplicação da tecnologia foi alinhada com o mercado e clientes, e a proposta do plano de implantação do sistema piloto foi validada, indicando viabilidade com a utilização de uma embarcação já contratada pela Petrobras. O projeto ""Estrutura têxtil anti-incrustante diverless (tecido anti-incrustante)"" completou a Fase 1, realizando testes em laboratório e definindo a estratégia de negócio. A Fase 2 começou com a assinatura do Termo de Cooperação, abordando desenvolvimento conceitual, provas de conceito e testes. A etapa informacional, finalizada em 2023, envolveu entender o ciclo de vida do produto, identificar necessidades do cliente e priorizar requisitos. A fase conceitual está prevista para terminar em 2024. O projeto ""Integridade de Juntas Dissimilares em Equipamentos Submarinos"" está sendo reavaliado devido a limitações de recursos humanos e complexidade. No projeto ""Conector Mecânico para Risers de Produção/Injeção"", a RFI concluiu que não existe produto de mercado que atenda às especificações do pré-sal, necessitando desenvolvimento interno. O projeto ""Modelo de taxa de crescimento de trinca em SCC-CO2 e fadiga da/dN em SCC-CO2"" teve o Termo de Cooperação P&D assinado com o LAMEF/UFRGS em setembro de 2023. A revisão bibliográfica e resultados prévios foram consolidados, apesar de atrasos na assinatura do Termo de Cooperação de Infraestrutura. O projeto ""Soluções Avançadas para o Monitoramento e a Identificação de Falhas em Sistemas Elétricos Submarinos"" foi dividido em duas frentes. A primeira focou no aprimoramento de um sistema remoto de diagnóstico preditivo de falhas em conjuntos de bombeio submarinos, iniciando a análise de dados e integração com sistemas supervisórios. A segunda desenvolveu uma ferramenta específica para diagnóstico e localização de falhas em cabos elétricos submarinos, com avanços significativos no hardware e modelagem de sistemas elétricos submarinos. O projeto ""Efeito do consumo do CO2 no anular de duto flexível"" avançou na descrição de entregas, premissas e potencial valor econômico-financeiro. Conceitos básicos foram estabelecidos e uma pesquisa documental foi realizada. A valoração foi finalizada e a validação do consumo de CO2 em teste de pequena escala está em andamento, dependendo da contratação do novo Termo de Cooperação. O projeto ""Conexão Diverless de Risers Rígidos em Bocas de Sino"" iniciou com estudos numéricos para avaliação e confirmação das hipóteses e conceitos básicos. A prospecção de mercado identificou soluções alternativas. A qualificação do TiPT está em andamento, com seleção inicial de materiais poliméricos. O projeto ""MICROSEAL - Mecanismo de Inspeção, Coleta e Recolhimento Otimizado com Selagem de Anular de Linhas"" avançou com a revisão do projeto conceitual e avaliação de viabilidade técnica. Um protótipo está sendo fabricado para testes em câmara hiperbárica, seguidos por campanhas de testes de campo para validação da tecnologia, visando a implantação na rotina de serviço de recolhimento de dutos.																								"
