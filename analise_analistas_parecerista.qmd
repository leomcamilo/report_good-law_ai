---
title: Analise
author: Leonardo Camilo
execute:
    echo: false
    warnings: false
    message: false
jupyter: python3

---


# An√°lise Detalhada: Analistas vs Pareceristas - Lei do Bem 2021


Este notebook analisa a rela√ß√£o entre as decis√µes dos analistas (fase
DO) e dos pareceristas (fase Parecer) nos projetos da Lei do Bem.

Perguntas a serem respondidas: 1. Taxa de aprova√ß√£o por analista 2. Taxa
de concord√¢ncia DO ‚Üí Parecer 3. Revers√µes de decis√£o (N√£o Recomendado ‚Üí
Recomendado) 4. Dispers√£o de √°reas por analista 5. Fichas individuais
dos top 15 analistas 6. Modelo preditivo de aprova√ß√£o no Parecer

# An√°lise de Decis√µes no Processo da Lei do Bem (2021): Analistas e Pareceristas

## 1. Introdu√ß√£o

Esta an√°lise aprofundada explora as decis√µes tomadas por analistas e
pareceristas no √¢mbito do programa Lei do Bem do MCTI, um dos principais
instrumentos de fomento √† inova√ß√£o no Brasil. O estudo se baseia em um
volume expressivo de dados, compreendendo 13.198 projetos analisados
durante o ano de 2021. O objetivo geral √© identificar padr√µes,
inconsist√™ncias e insights nas avalia√ß√µes, visando otimizar a
efici√™ncia, a isonomia e a previsibilidade do processo de concess√£o de
incentivos fiscais para Pesquisa e Desenvolvimento (P&D).

## 2. Metodologia

A an√°lise foi conduzida a partir de um banco de dados consolidado
contendo informa√ß√µes detalhadas sobre cada um dos 13.198 projetos do
ano-base de 2021. As fontes de dados incluem os pareceres t√©cnicos
emitidos tanto pelos analistas (fase DO) quanto pelos pareceristas (fase
Parecer), al√©m de metadados dos projetos, como √°reas de conhecimento e
informa√ß√µes textuais descritivas.

As t√©cnicas aplicadas envolveram desde a estat√≠stica descritiva, para
quantificar taxas de aprova√ß√£o e concord√¢ncia, at√© a an√°lise de
dispers√£o para entender a distribui√ß√£o de projetos entre os analistas.
Um dos pilares da metodologia foi o processamento de linguagem natural
(PLN), utilizado para extrair insights dos campos textuais. Para isso, a
biblioteca NLTK (Natural Language Toolkit) foi empregada para processar
os textos em portugu√™s, aplicando a remo√ß√£o de stopwords (palavras
comuns como ‚Äúde‚Äù, ‚Äúpara‚Äù, ‚Äúcom‚Äù) para focar nos termos mais relevantes
de cada projeto.

O conjunto de stopwords foi composto por 207 palavras do NLTK e um
conjunto customizado de 71 termos espec√≠ficos do dom√≠nio da Lei do Bem,
totalizando 278 palavras removidas para garantir uma an√°lise mais limpa
e focada. Al√©m disso, foi desenvolvido um modelo de Machine Learning
para explorar a capacidade preditiva dos dados textuais em rela√ß√£o √†
aprova√ß√£o final dos projetos.

### An√°lise de Analistas vs Pareceristas - Lei do Bem 2021

**Data de An√°lise:** 18/07/2025 **Ano Base dos Dados:** 2021  
**Total de Projetos:** 13.198

``` {python}
#| echo: false
# %% Imports e Configura√ß√£o
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder
import re
from collections import Counter
import nltk

# Baixar recursos necess√°rios do NLTK
print("Verificando recursos do NLTK...")
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Baixando punkt...")
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("Baixando stopwords...")
    nltk.download('stopwords')

print("‚úÖ Recursos NLTK prontos!")

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Configura√ß√µes
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-whitegrid')
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

# Configurar fonte para melhor visualiza√ß√£o em PDF
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 12
plt.rcParams['axes.labelsize'] = 10
plt.rcParams['xtick.labelsize'] = 9
plt.rcParams['ytick.labelsize'] = 9
plt.rcParams['legend.fontsize'] = 9
plt.rcParams['figure.dpi'] = 300

# %% Configura√ß√£o de stopwords em portugu√™s
# Carregar stopwords padr√£o do NLTK em portugu√™s
stop_words_nltk = set(stopwords.words('portuguese'))

# Adicionar stopwords espec√≠ficas do dom√≠nio Lei do Bem
stop_words_dominio = {
    # Verbos comuns
    'realizar', 'desenvolver', 'criar', 'implementar', 'utilizar', 'aplicar', 'melhorar',
    'analisar', 'estudar', 'avaliar', 'testar', 'produzir', 'fabricar', 'construir', 'usar', 'uso'
    
    # Preposi√ß√µes e conectivos adicionais
    'atrav√©s', 'mediante', 'partir', 'base', 'forma', 'modo', 'tipo', 'fase', 'etapa',
    'objetivo', 'objetivos', 'finalidade', 'prop√≥sito', 'meta', 'metas',
    
    # Termos muito gen√©ricos
    'novo', 'nova', 'novos', 'novas', '√°rea', '√°reas', 'setor', 'setores',
    'atividade', 'atividades', 'trabalho', 'trabalhos', 'servi√ßo', 'servi√ßos'
}

# Combinar todas as stopwords
todas_stopwords = stop_words_nltk.union(stop_words_dominio)

print(f"üìä Total de stopwords:")
print(f"   - NLTK portugu√™s: {len(stop_words_nltk)}")
print(f"   - Dom√≠nio espec√≠fico: {len(stop_words_dominio)}")
print(f"   - Total combinado: {len(todas_stopwords)}")
```

    Verificando recursos do NLTK...
    ‚úÖ Recursos NLTK prontos!
    üìä Total de stopwords:
       - NLTK portugu√™s: 207
       - Dom√≠nio espec√≠fico: 44
       - Total combinado: 251

``` {python}
# %% Fun√ß√µes de processamento de texto simplificadas
def limpar_texto(texto):
    """Limpa e normaliza o texto"""
    if pd.isna(texto) or not texto:
        return ""
    
    # Converter para string e min√∫sculas
    texto = str(texto).lower()
    
    # Remover caracteres especiais, mantendo espa√ßos e letras
    texto = re.sub(r'[^a-z√°√†√¢√£√©√®√™√≠√Ø√≥√¥√µ√∂√∫√ß√±\s]', ' ', texto)
    
    # Remover espa√ßos m√∫ltiplos
    texto = re.sub(r'\s+', ' ', texto)
    
    return texto.strip()

def analisar_texto_stopwords(texto, stopwords_set):
    """
    Analisa um texto e retorna estat√≠sticas sobre stopwords
    """
    if pd.isna(texto) or not texto:
        return {
            'texto_original': '',
            'texto_sem_stopwords': '',
            'palavras_total': 0,
            'palavras_sem_stopwords': 0,
            'stopwords_encontradas': [],
            'palavras_relevantes': [],
            'reducao_percentual': 0
        }
    
    # Converter para string primeiro para garantir
    texto_str = str(texto)
    
    # Limpar texto
    texto_limpo = limpar_texto(texto_str)
    
    # Tokenizar
    palavras = texto_limpo.split()
    palavras_total = len(palavras)
    
    # Filtrar stopwords
    palavras_sem_stop = [p for p in palavras if p not in stopwords_set and len(p) > 2]
    stopwords_encontradas = [p for p in palavras if p in stopwords_set]
    
    # Calcular redu√ß√£o
    reducao = ((palavras_total - len(palavras_sem_stop)) / palavras_total * 100) if palavras_total > 0 else 0
    
    # Preparar texto original para retorno (truncado se muito longo)
    texto_original_truncado = texto_str[:200] + '...' if len(texto_str) > 200 else texto_str
    
    return {
        'texto_original': texto_original_truncado,
        'texto_sem_stopwords': ' '.join(palavras_sem_stop),
        'palavras_total': palavras_total,
        'palavras_sem_stopwords': len(palavras_sem_stop),
        'stopwords_encontradas': stopwords_encontradas,
        'palavras_relevantes': palavras_sem_stop[:20],  # Top 20 palavras
        'reducao_percentual': reducao
    }

def processar_campos_texto(df, campos, stopwords_set):
    """
    Processa m√∫ltiplos campos de texto e retorna an√°lise de stopwords
    """
    resultados = {}
    
    for campo in campos:
        if campo not in df.columns:
            print(f"‚ö†Ô∏è Campo {campo} n√£o encontrado")
            continue
            
        print(f"üìù Processando {campo}...")
        
        # Analisar cada texto
        analises = df[campo].apply(lambda x: analisar_texto_stopwords(x, stopwords_set))
        
        # Extrair m√©tricas
        resultados[campo] = {
            'df_analise': pd.DataFrame(list(analises)),
            'reducao_media': np.mean([a['reducao_percentual'] for a in analises]),
            'palavras_media_original': np.mean([a['palavras_total'] for a in analises]),
            'palavras_media_filtrado': np.mean([a['palavras_sem_stopwords'] for a in analises])
        }
        
        # Contar palavras mais frequentes ap√≥s filtro
        todas_palavras_relevantes = []
        for analise in analises:
            todas_palavras_relevantes.extend(analise['palavras_relevantes'])
        
        contador_palavras = Counter(todas_palavras_relevantes)
        resultados[campo]['top_palavras'] = contador_palavras.most_common(20)
        
        # Contar stopwords mais comuns
        todas_stopwords = []
        for analise in analises:
            todas_stopwords.extend(analise['stopwords_encontradas'])
        
        contador_stopwords = Counter(todas_stopwords)
        resultados[campo]['top_stopwords'] = contador_stopwords.most_common(20)
    
    return resultados
```

## 1. Carregamento e Prepara√ß√£o dos Dados

``` {python}
# %% Carregamento dos dados
print("Carregando dados...")

# Carregar o arquivo principal com todos os dados
df = pd.read_csv('csv_longo/projetos_lei_do_bem_2023_DETALHADO_LINHA_UNICA.csv', sep=';', encoding='utf-8')

print(f"Total de projetos carregados: {len(df):,}")
print(f"Total de colunas: {len(df.columns)}")

# Colunas relevantes para a an√°lise
colunas_analise = [
    # Identifica√ß√£o
    'lst_idprenchimentosituacaoanalise', 'lst_norazaosocial', 'lst_nrcnpj',
    'lst_noatividadeeconomica', 'daproj_nritem',
    
    # Projeto
    'daproj_noprojeto', 'daproj_dsprojeto', 'daproj_dsareaprojeto',
    'daproj_dspalavrachave', 'daproj_dselementotecnologico',
    'daproj_dsdesafiotecnologico', 'daproj_dsmetodologiautilizada',
    
    # Analista (DO)
    'do_saat_idunicopessoaanalise', 'do_taaproj_notipoavaliacaoanalise',
    
    # Parecerista
    'p_taaproj_notipoavaliacaoanalise',
    
    # Valores
    'do_aat_vltotaldeclarado', 'do_aat_vltotalparecer'
]

# Filtrar apenas colunas existentes
colunas_existentes = [col for col in colunas_analise if col in df.columns]
df_analise = df[colunas_existentes].copy()

print(f"\nColunas selecionadas para an√°lise: {len(colunas_existentes)}")

# Definir campos de texto para an√°lise de stopwords
campos_texto_analise = [
    'daproj_dsprojeto',              # Descri√ß√£o do projeto 
    'daproj_dselementotecnologico',  # Elemento tecnol√≥gico
    'daproj_dsdesafiotecnologico',   # Desafio tecnol√≥gico
    'daproj_dsmetodologiautilizada'  # Metodologia utilizada
]

# Filtrar apenas campos existentes
campos_texto_existentes = [campo for campo in campos_texto_analise if campo in df_analise.columns]
print(f"\nCampos de texto para an√°lise: {len(campos_texto_existentes)}")

# %% An√°lise de stopwords
print("\nüîç AN√ÅLISE DE STOPWORDS EM PORTUGU√äS")
print("=" * 60)

# Processar campos
resultados_stopwords = processar_campos_texto(df_analise, campos_texto_existentes, todas_stopwords)

# Mostrar estat√≠sticas gerais
print("\nüìä ESTAT√çSTICAS DE REDU√á√ÉO POR CAMPO:")
print("-" * 60)
for campo, resultado in resultados_stopwords.items():
    nome_campo = campo.replace('daproj_', '').replace('ds', '').replace('_', ' ').title()
    print(f"\n{nome_campo}:")
    print(f"  ‚Ä¢ Palavras m√©dias (original): {resultado['palavras_media_original']:.1f}")
    print(f"  ‚Ä¢ Palavras m√©dias (filtrado): {resultado['palavras_media_filtrado']:.1f}")
    print(f"  ‚Ä¢ Redu√ß√£o m√©dia: {resultado['reducao_media']:.1f}%")
    
    # Top 5 palavras relevantes
    print(f"  ‚Ä¢ Top 5 palavras relevantes:")
    for palavra, freq in resultado['top_palavras'][:5]:
        print(f"    - {palavra}: {freq:,} ocorr√™ncias")

# %% Visualiza√ß√£o de stopwords
# Criar visualiza√ß√£o comparativa
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

for i, (campo, resultado) in enumerate(resultados_stopwords.items()):
    if i < len(axes):
        ax = axes[i]
        
        # Dados para o gr√°fico
        df_vis = resultado['df_analise']
        
        # Histograma de redu√ß√£o percentual
        df_vis['reducao_percentual'].hist(bins=30, ax=ax, color='skyblue', edgecolor='navy')
        ax.set_title(f'Redu√ß√£o por Stopwords\n{campo.replace("daproj_", "")}')
        ax.set_xlabel('Redu√ß√£o Percentual (%)')
        ax.set_ylabel('Frequ√™ncia')
        ax.axvline(resultado['reducao_media'], color='red', linestyle='--', 
                  label=f'M√©dia: {resultado["reducao_media"]:.1f}%')
        ax.legend()
        ax.grid(True, alpha=0.3)

# Remover subplots vazios
for j in range(len(resultados_stopwords), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.suptitle('An√°lise de Redu√ß√£o por Stopwords nos Campos de Texto', y=1.02, fontsize=16)
plt.show()

# %% Top palavras mais relevantes (ap√≥s remover stopwords)
print("\nüèÜ TOP 20 PALAVRAS MAIS RELEVANTES POR CAMPO")
print("=" * 60)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

for i, (campo, resultado) in enumerate(resultados_stopwords.items()):
    if i < len(axes):
        ax = axes[i]
        
        # Top 10 palavras para o gr√°fico
        palavras = [p[0] for p in resultado['top_palavras'][:10]]
        frequencias = [p[1] for p in resultado['top_palavras'][:10]]
        
        # Gr√°fico de barras horizontal
        y_pos = np.arange(len(palavras))
        ax.barh(y_pos, frequencias, color='green', alpha=0.7)
        ax.set_yticks(y_pos)
        ax.set_yticklabels(palavras)
        ax.invert_yaxis()
        ax.set_xlabel('Frequ√™ncia')
        ax.set_title(f'Top 10 Palavras Relevantes\n{campo.replace("daproj_", "")}')
        ax.grid(axis='x', alpha=0.3)
        
        # Adicionar valores nas barras
        for j, v in enumerate(frequencias):
            ax.text(v + 10, j, str(v), va='center')

# Remover subplots vazios
for j in range(len(resultados_stopwords), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# %% Preparar dados processados para an√°lise posterior
print("\nüíæ Criando DataFrame com textos processados...")

# Criar novas colunas com textos limpos (sem stopwords)
for campo, resultado in resultados_stopwords.items():
    df_analise[f'{campo}_limpo'] = resultado['df_analise']['texto_sem_stopwords']
    df_analise[f'{campo}_num_palavras_limpo'] = resultado['df_analise']['palavras_sem_stopwords']

# Criar texto combinado limpo para an√°lises posteriores
textos_combinados_limpos = []
for idx in range(len(df_analise)):
    texto_combinado = ' '.join([
        str(df_analise[f'{campo}_limpo'].iloc[idx]) 
        for campo in campos_texto_existentes 
        if f'{campo}_limpo' in df_analise.columns
    ])
    textos_combinados_limpos.append(texto_combinado)

df_analise['texto_combinado_limpo'] = textos_combinados_limpos

print(f"‚úÖ DataFrame expandido com textos processados!")
print(f"   Novas colunas criadas: {len([col for col in df_analise.columns if 'limpo' in col])}")

# Exemplo de texto processado
print("\nüìù EXEMPLO DE PROCESSAMENTO:")
print("-" * 60)
idx_exemplo = 0
for campo in ['daproj_dsprojeto']:
    if campo in df_analise.columns and f'{campo}_limpo' in df_analise.columns:
        print(f"\n{campo}:")
        print(f"Original: {str(df_analise[campo].iloc[idx_exemplo])[:150]}...")
        print(f"Limpo: {str(df_analise[f'{campo}_limpo'].iloc[idx_exemplo])[:150]}...")
        print(f"Redu√ß√£o: {resultados_stopwords[campo]['df_analise']['reducao_percentual'].iloc[idx_exemplo]:.1f}%")
print("Carregando dados...")

# Carregar o arquivo principal com todos os dados
df = pd.read_csv('csv_longo/projetos_lei_do_bem_2023_DETALHADO_LINHA_UNICA.csv', sep=';', encoding='utf-8')

print(f"Total de projetos carregados: {len(df):,}")
print(f"Total de colunas: {len(df.columns)}")

# Colunas relevantes para a an√°lise
colunas_analise = [
    # Identifica√ß√£o
    'lst_idprenchimentosituacaoanalise', 'lst_norazaosocial', 'lst_nrcnpj',
    'lst_noatividadeeconomica', 'daproj_nritem',
    
    # Projeto
    'daproj_noprojeto', 'daproj_dsprojeto', 'daproj_dsareaprojeto',
    'daproj_dspalavrachave', 'daproj_dselementotecnologico',
    'daproj_dsdesafiotecnologico', 'daproj_dsmetodologiautilizada',
    
    # Analista (DO)
    'do_saat_idunicopessoaanalise', 'do_taaproj_notipoavaliacaoanalise',
    
    # Parecerista
    'p_taaproj_notipoavaliacaoanalise',
    
    # Valores
    'do_aat_vltotaldeclarado', 'do_aat_vltotalparecer'
]

# Filtrar apenas colunas existentes
colunas_existentes = [col for col in colunas_analise if col in df.columns]
df_analise = df[colunas_existentes].copy()

print(f"\nColunas selecionadas para an√°lise: {len(colunas_existentes)}")
```

    Carregando dados...
    Total de projetos carregados: 13,198
    Total de colunas: 229

    Colunas selecionadas para an√°lise: 17

    Campos de texto para an√°lise: 4

    üîç AN√ÅLISE DE STOPWORDS EM PORTUGU√äS
    ============================================================
    üìù Processando daproj_dsprojeto...
    üìù Processando daproj_dselementotecnologico...
    üìù Processando daproj_dsdesafiotecnologico...
    üìù Processando daproj_dsmetodologiautilizada...

    üìä ESTAT√çSTICAS DE REDU√á√ÉO POR CAMPO:
    ------------------------------------------------------------

    Projeto:
      ‚Ä¢ Palavras m√©dias (original): 283.4
      ‚Ä¢ Palavras m√©dias (filtrado): 156.2
      ‚Ä¢ Redu√ß√£o m√©dia: 44.4%
      ‚Ä¢ Top 5 palavras relevantes:
        - desenvolvimento: 5,167 ocorr√™ncias
        - projeto: 4,678 ocorr√™ncias
        - produtos: 2,479 ocorr√™ncias
        - pesquisa: 2,008 ocorr√™ncias
        - empresa: 1,950 ocorr√™ncias

    Elementotecnologico:
      ‚Ä¢ Palavras m√©dias (original): 327.8
      ‚Ä¢ Palavras m√©dias (filtrado): 180.8
      ‚Ä¢ Redu√ß√£o m√©dia: 44.4%
      ‚Ä¢ Top 5 palavras relevantes:
        - desenvolvimento: 6,026 ocorr√™ncias
        - projeto: 4,779 ocorr√™ncias
        - elemento: 2,189 ocorr√™ncias
        - inovador: 2,112 ocorr√™ncias
        - produtos: 1,982 ocorr√™ncias

    Desafiotecnologico:
      ‚Ä¢ Palavras m√©dias (original): 308.7
      ‚Ä¢ Palavras m√©dias (filtrado): 170.0
      ‚Ä¢ Redu√ß√£o m√©dia: 44.4%
      ‚Ä¢ Top 5 palavras relevantes:
        - desenvolvimento: 5,397 ocorr√™ncias
        - projeto: 4,824 ocorr√™ncias
        - desafios: 3,999 ocorr√™ncias
        - desafio: 2,696 ocorr√™ncias
        - barreiras: 2,330 ocorr√™ncias

    Metodologiautilizada:
      ‚Ä¢ Palavras m√©dias (original): 296.8
      ‚Ä¢ Palavras m√©dias (filtrado): 163.8
      ‚Ä¢ Redu√ß√£o m√©dia: 43.5%
      ‚Ä¢ Top 5 palavras relevantes:
        - desenvolvimento: 7,652 ocorr√™ncias
        - projeto: 6,284 ocorr√™ncias
        - metodologia: 3,997 ocorr√™ncias
        - pesquisa: 2,910 ocorr√™ncias
        - estudo: 2,296 ocorr√™ncias

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-4-output-2.png)


    üèÜ TOP 20 PALAVRAS MAIS RELEVANTES POR CAMPO
    ============================================================

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-4-output-4.png)


    üíæ Criando DataFrame com textos processados...
    ‚úÖ DataFrame expandido com textos processados!
       Novas colunas criadas: 9

    üìù EXEMPLO DE PROCESSAMENTO:
    ------------------------------------------------------------

    daproj_dsprojeto:
    Original: No ano-base de 2021, a Jardinox investiu em pesquisa, desenvolvimento e inova√ß√£o tecnol√≥gica na procura de novas solu√ß√µes para o setor de transporte, ...
    Limpo: ano jardinox investiu pesquisa desenvolvimento inova√ß√£o tecnol√≥gica procura solu√ß√µes transporte aplicado principalmente produtos viabilizassem fluxo l...
    Redu√ß√£o: 45.3%
    Carregando dados...
    Total de projetos carregados: 13,198
    Total de colunas: 229

    Colunas selecionadas para an√°lise: 17

## 2. An√°lise Explorat√≥ria Inicial

``` {python}
# %% Estat√≠sticas b√°sicas
# Verificar valores √∫nicos nas colunas de decis√£o
print("Valores √∫nicos em 'do_taaproj_notipoavaliacaoanalise' (Analista):")
print(df_analise['do_taaproj_notipoavaliacaoanalise'].value_counts())

print("\n\nValores √∫nicos em 'p_taaproj_notipoavaliacaoanalise' (Parecerista):")
print(df_analise['p_taaproj_notipoavaliacaoanalise'].value_counts())

# Filtrar apenas projetos que passaram por ambas as fases
df_completo = df_analise[
    df_analise['do_taaproj_notipoavaliacaoanalise'].notna() & 
    df_analise['p_taaproj_notipoavaliacaoanalise'].notna()
].copy()

print(f"\n\nProjetos com an√°lise completa (DO + Parecer): {len(df_completo):,}")
```

    Valores √∫nicos em 'do_taaproj_notipoavaliacaoanalise' (Analista):
    do_taaproj_notipoavaliacaoanalise
    Recomendado        8193
    N√£o Recomendado    5001
    Abonado               3
    Name: count, dtype: int64


    Valores √∫nicos em 'p_taaproj_notipoavaliacaoanalise' (Parecerista):
    p_taaproj_notipoavaliacaoanalise
    N√£o Recomendado    7312
    Recomendado        5886
    Name: count, dtype: int64


    Projetos com an√°lise completa (DO + Parecer): 13,197

## 3. Pergunta 1: Taxa de Aprova√ß√£o por Analista

Qual percentual de projetos foram aprovados (Recomendado) por cada
analista?

### Pergunta 1

``` {python}
# Criar fun√ß√£o para padronizar as decis√µes
def padronizar_decisao(decisao):
    if pd.isna(decisao):
        return np.nan
    decisao_str = str(decisao).strip().upper()
    if 'RECOMENDADO' in decisao_str and 'N√ÉO' not in decisao_str:
        return 'Recomendado'
    elif 'N√ÉO RECOMENDADO' in decisao_str:
        return 'N√£o Recomendado'
    else:
        return 'Outro'

# Aplicar padroniza√ß√£o
df_completo['decisao_analista'] = df_completo['do_taaproj_notipoavaliacaoanalise'].apply(padronizar_decisao)
df_completo['decisao_parecerista'] = df_completo['p_taaproj_notipoavaliacaoanalise'].apply(padronizar_decisao)

# Calcular taxa de aprova√ß√£o por analista
taxa_aprovacao_analista = df_completo.groupby('do_saat_idunicopessoaanalise').agg({
    'decisao_analista': [
        'count',
        lambda x: (x == 'Recomendado').sum(),
        lambda x: (x == 'Recomendado').mean() * 100
    ]
}).round(2)

taxa_aprovacao_analista.columns = ['Total_Projetos', 'Projetos_Recomendados', 'Taxa_Aprovacao_%']
taxa_aprovacao_analista = taxa_aprovacao_analista.sort_values('Total_Projetos', ascending=False)

# Visualiza√ß√£o
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

# Top 20 analistas por volume
top20_volume = taxa_aprovacao_analista.head(20)
analistas_ids = [f"ID_{int(idx)}" for idx in top20_volume.index]

ax1.bar(range(len(top20_volume)), top20_volume['Total_Projetos'], color='skyblue', edgecolor='navy')
ax1.set_xticks(range(len(top20_volume)))
ax1.set_xticklabels(analistas_ids, rotation=45, ha='right')
ax1.set_xlabel('Analista')
ax1.set_ylabel('N√∫mero de Projetos')
ax1.set_title('Top 20 Analistas por Volume de Projetos Analisados')
ax1.grid(axis='y', alpha=0.3)

# Taxa de aprova√ß√£o dos top 20
colors = ['green' if x > 50 else 'orange' if x > 30 else 'red' for x in top20_volume['Taxa_Aprovacao_%']]
ax2.bar(range(len(top20_volume)), top20_volume['Taxa_Aprovacao_%'], color=colors, edgecolor='black')
ax2.set_xticks(range(len(top20_volume)))
ax2.set_xticklabels(analistas_ids, rotation=45, ha='right')
ax2.set_xlabel('Analista')
ax2.set_ylabel('Taxa de Aprova√ß√£o (%)')
ax2.set_title('Taxa de Aprova√ß√£o dos Top 20 Analistas')
ax2.axhline(y=50, color='black', linestyle='--', alpha=0.5, label='50%')
ax2.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.show()

# An√°lise de dispers√£o e correla√ß√£o
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Scatter plot: Volume vs Taxa de Aprova√ß√£o
top20 = taxa_aprovacao_analista.head(20)
ax1.scatter(top20['Total_Projetos'], top20['Taxa_Aprovacao_%'], 
            s=100, alpha=0.6, edgecolors='black')
ax1.set_xlabel('N√∫mero de Projetos Analisados')
ax1.set_ylabel('Taxa de Aprova√ß√£o (%)')
ax1.set_title('Correla√ß√£o: Volume vs Taxa de Aprova√ß√£o')
ax1.grid(True, alpha=0.3)

# Adicionar linha de tend√™ncia
z = np.polyfit(top20['Total_Projetos'], top20['Taxa_Aprovacao_%'], 1)
p = np.poly1d(z)
ax1.plot(top20['Total_Projetos'], p(top20['Total_Projetos']), 
         "r--", alpha=0.8, label=f'Tend√™ncia: {z[0]:.2f}')
ax1.legend()

# Histograma de distribui√ß√£o das taxas
ax2.hist(taxa_aprovacao_analista['Taxa_Aprovacao_%'], 
         bins=20, edgecolor='black', alpha=0.7)
ax2.axvline(taxa_aprovacao_analista['Taxa_Aprovacao_%'].mean(), 
            color='red', linestyle='--', linewidth=2, 
            label=f'M√©dia: {taxa_aprovacao_analista["Taxa_Aprovacao_%"].mean():.1f}%')
ax2.axvline(taxa_aprovacao_analista['Taxa_Aprovacao_%'].median(), 
            color='green', linestyle='--', linewidth=2, 
            label=f'Mediana: {taxa_aprovacao_analista["Taxa_Aprovacao_%"].median():.1f}%')
ax2.set_xlabel('Taxa de Aprova√ß√£o (%)')
ax2.set_ylabel('N√∫mero de Analistas')
ax2.set_title('Distribui√ß√£o das Taxas de Aprova√ß√£o')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Estat√≠sticas detalhadas
print("\nESTAT√çSTICAS DETALHADAS:")
print(f"Desvio padr√£o das taxas: {taxa_aprovacao_analista['Taxa_Aprovacao_%'].std():.2f}%")
print(f"Coeficiente de varia√ß√£o: {(taxa_aprovacao_analista['Taxa_Aprovacao_%'].std() / taxa_aprovacao_analista['Taxa_Aprovacao_%'].mean() * 100):.2f}%")
print(f"Amplitude (max - min): {taxa_aprovacao_analista['Taxa_Aprovacao_%'].max() - taxa_aprovacao_analista['Taxa_Aprovacao_%'].min():.2f}%")

# Tabela resumo
print("Top 10 Analistas por Volume de Projetos:")
print(taxa_aprovacao_analista.head(10))
```

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-6-output-1.png)

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-6-output-2.png)


    ESTAT√çSTICAS DETALHADAS:
    Desvio padr√£o das taxas: 27.07%
    Coeficiente de varia√ß√£o: 43.64%
    Amplitude (max - min): 100.00%
    Top 10 Analistas por Volume de Projetos:
                                  Total_Projetos  Projetos_Recomendados  \
    do_saat_idunicopessoaanalise                                          
    301040479.0                              628                    375   
    301010505.0                              565                    392   
    300713489.0                              534                    345   
    108293000.0                              438                    285   
    300603544.0                              384                    206   
    301241624.0                              367                    102   
    301043768.0                              361                    289   
    116768062.0                              347                    229   
    300788615.0                              346                    263   
    301064200.0                              306                    182   

                                  Taxa_Aprovacao_%  
    do_saat_idunicopessoaanalise                    
    301040479.0                              59.71  
    301010505.0                              69.38  
    300713489.0                              64.61  
    108293000.0                              65.07  
    300603544.0                              53.65  
    301241624.0                              27.79  
    301043768.0                              80.06  
    116768062.0                              65.99  
    300788615.0                              76.01  
    301064200.0                              59.48  

#### An√°lise e Insights da Pergunta 1

**O que foi investigado:**

Esta se√ß√£o focou em determinar a taxa de aprova√ß√£o individual de cada
analista durante a fase de An√°lise T√©cnica (DO), identificando a
exist√™ncia de disparidades significativas entre eles. A an√°lise buscou
responder se alguns analistas s√£o sistematicamente mais rigorosos ou
mais lenientes que outros, e se existe alguma correla√ß√£o entre o volume
de projetos analisados e o rigor na avalia√ß√£o.

**Principais Descobertas:**

A an√°lise revelou uma varia√ß√£o nas taxas de aprova√ß√£o individuais. Os
dados mostram que:

-   **Dispers√£o extrema**: As taxas de aprova√ß√£o variam de **27.79%**
    (analista 301241624) at√© **97.45%** (analista 121759221), uma
    amplitude de quase **70 pontos percentuais**
-   **Distribui√ß√£o bimodal**: O histograma revela uma distribui√ß√£o
    bimodal, com concentra√ß√µes em torno de 60% e 95%, indicando dois
    ‚Äúperfis‚Äù distintos de analistas
-   **M√©dia vs Mediana**: A m√©dia de **62.0%** e mediana de **60.1%**
    s√£o significativamente inferiores √† taxa geral de aprova√ß√£o da fase
    DO (84.3%), sugerindo que poucos analistas com alt√≠ssimas taxas
    elevam a m√©dia geral
-   **Correla√ß√£o neglig√≠vel com volume**: A linha de tend√™ncia mostra
    uma correla√ß√£o praticamente nula (-0.03) entre volume de projetos e
    taxa de aprova√ß√£o, indicando que a experi√™ncia quantitativa n√£o
    influencia o rigor

**Padr√µes Identificados:**

1.  **Grupo Ultra-Rigoroso** (\< 40% aprova√ß√£o):
    -   3 analistas nesta faixa
    -   Destaque: analista 301241624 com apenas 27.79% de aprova√ß√£o em
        367 projetos
2.  **Grupo Moderado** (40-70% aprova√ß√£o):
    -   Aproximadamente 40% dos analistas
    -   Pr√≥ximos √†s m√©dias estat√≠sticas
3.  **Grupo Leniente** (\> 85% aprova√ß√£o):
    -   Cerca de 30% dos analistas
    -   Alguns com taxas pr√≥ximas a 100%, questionando a efetividade da
        an√°lise

**Insights e Implica√ß√µes Pr√°ticas:**

1.  **Risco de Vi√©s Sistem√°tico**: A probabilidade de aprova√ß√£o de um
    projeto pode variar em at√© 70% dependendo exclusivamente do analista
    designado, comprometendo gravemente a equidade do processo.

2.  **Falta de Calibra√ß√£o**: A distribui√ß√£o bimodal sugere que os
    analistas operam com conjuntos de crit√©rios fundamentalmente
    diferentes, n√£o havendo um ‚Äúpadr√£o ouro‚Äù compartilhado.

3.  **Volume n√£o √© Sin√¥nimo de Consist√™ncia**: Analistas experientes
    (alto volume) apresentam a mesma variabilidade que novatos,
    indicando que o problema √© estrutural, n√£o de experi√™ncia.

4.  **Necessidade de Interven√ß√£o Urgente**: Com apenas 17 analistas
    representando a grande maioria das an√°lises (conforme mostrado no
    histograma), padronizar seus crit√©rios teria impacto imediato e
    significativo na consist√™ncia geral do processo.

**Recomenda√ß√µes Espec√≠ficas:**

-   **Implementa√ß√£o imediata** de sess√µes de calibra√ß√£o trimestral com
    casos-teste
-   **Sistema de alertas** para analistas com taxas fora do intervalo
    50-75%
-   **Revis√£o cruzada obrigat√≥ria** para decis√µes de analistas nos
    extremos
-   **An√°lise qualitativa** dos projetos do analista 301241624 para
    entender crit√©rios ultra-rigorosos
-   **Redistribui√ß√£o de carga** considerando especializa√ß√£o por √°rea,
    n√£o apenas volume

## 4. Pergunta 2: Taxa de Concord√¢ncia DO ‚Üí Parecer

Qual percentual dos projetos Recomendados por cada analista passou pela
fase do parecer com aprova√ß√£o?

### Pergunta 2

``` {python}
# Filtrar apenas projetos recomendados pelo analista
df_recomendados_analista = df_completo[df_completo['decisao_analista'] == 'Recomendado'].copy()

# Calcular taxa de concord√¢ncia
concordancia_parecer = df_recomendados_analista.groupby('do_saat_idunicopessoaanalise').agg({
    'decisao_parecerista': [
        'count',
        lambda x: (x == 'Recomendado').sum(),
        lambda x: (x == 'Recomendado').mean() * 100
    ]
}).round(2)

concordancia_parecer.columns = ['Projetos_Recom_Analista', 'Mantidos_Recom_Parecer', 'Taxa_Concordancia_%']
concordancia_parecer = concordancia_parecer.sort_values('Projetos_Recom_Analista', ascending=False)

# Merge com dados anteriores
analise_completa = taxa_aprovacao_analista.merge(
    concordancia_parecer, 
    left_index=True, 
    right_index=True, 
    how='left'
)

# Visualiza√ß√£o
fig, ax = plt.subplots(figsize=(12, 8))

# Scatter plot: Taxa de Aprova√ß√£o vs Taxa de Concord√¢ncia
scatter_data = analise_completa[analise_completa['Projetos_Recom_Analista'] >= 10]  # M√≠nimo 10 projetos

scatter = ax.scatter(
    scatter_data['Taxa_Aprovacao_%'], 
    scatter_data['Taxa_Concordancia_%'],
    s=scatter_data['Total_Projetos'] * 2,  # Tamanho proporcional ao volume
    alpha=0.6,
    c=scatter_data['Total_Projetos'],
    cmap='viridis',
    edgecolors='black',
    linewidth=1
)

ax.set_xlabel('Taxa de Aprova√ß√£o do Analista (%)')
ax.set_ylabel('Taxa de Concord√¢ncia do Parecerista (%)')
ax.set_title('Rela√ß√£o entre Taxa de Aprova√ß√£o (Analista) e Taxa de Concord√¢ncia (Parecerista)')
ax.grid(True, alpha=0.3)

# Adicionar linha de refer√™ncia
ax.plot([0, 100], [0, 100], 'r--', alpha=0.5, label='Concord√¢ncia perfeita')

# Colorbar
cbar = plt.colorbar(scatter, ax=ax)
cbar.set_label('N√∫mero de Projetos', rotation=270, labelpad=20)

# Adicionar labels para outliers
for idx, row in scatter_data.iterrows():
    if row['Taxa_Concordancia_%'] < 20 or row['Taxa_Concordancia_%'] > 80:
        ax.annotate(f"ID_{int(idx)}", 
                   (row['Taxa_Aprovacao_%'], row['Taxa_Concordancia_%']),
                   fontsize=8, alpha=0.7)

plt.legend()
plt.tight_layout()
plt.show()

# Estat√≠sticas
print("\nEstat√≠sticas de Concord√¢ncia:")
print(f"Taxa m√©dia de concord√¢ncia: {concordancia_parecer['Taxa_Concordancia_%'].mean():.1f}%")
print(f"Analistas com concord√¢ncia < 50%: {(concordancia_parecer['Taxa_Concordancia_%'] < 50).sum()}")
print(f"Analistas com concord√¢ncia > 80%: {(concordancia_parecer['Taxa_Concordancia_%'] > 80).sum()}")
```

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-7-output-1.png)


    Estat√≠sticas de Concord√¢ncia:
    Taxa m√©dia de concord√¢ncia: 66.9%
    Analistas com concord√¢ncia < 50%: 19
    Analistas com concord√¢ncia > 80%: 38

### An√°lise e Insights da Pergunta 2

**O que foi investigado:**

O estudo mediu o grau de alinhamento entre a decis√£o inicial do analista
(fase DO) e a decis√£o final do parecerista (fase Parecer),
especificamente para projetos que foram recomendados pelo analista. O
objetivo era entender a consist√™ncia das decis√µes ao longo do processo e
identificar padr√µes de concord√¢ncia/discord√¢ncia entre as duas fases de
avalia√ß√£o.

**Principais Descobertas:**

O gr√°fico de dispers√£o revela padr√µes complexos e preocupantes na
rela√ß√£o entre as taxas de aprova√ß√£o dos analistas e a concord√¢ncia dos
pareceristas:

-   **Taxa m√©dia de concord√¢ncia**: 66.9%, significativamente menor que
    os 86.1% reportados anteriormente
-   **Distribui√ß√£o heterog√™nea**: 19 analistas com concord√¢ncia inferior
    a 50% e 38 com concord√¢ncia superior a 80%
-   **Padr√£o de dispers√£o**: A maioria dos pontos est√° abaixo da linha
    de concord√¢ncia perfeita (diagonal vermelha), indicando que os
    pareceristas tendem a ser mais restritivos que os analistas
-   **Outliers not√°veis**:
    -   Analista 300603544: alta taxa de aprova√ß√£o (~65%) mas
        concord√¢ncia baix√≠ssima (~10%)
    -   Analista 301241782: baixa taxa de aprova√ß√£o (~30%) e
        concord√¢ncia pr√≥xima a zero
    -   Analista 123493017: taxa de aprova√ß√£o alt√≠ssima (~100%) mas
        concord√¢ncia de apenas 5%

**Padr√µes Identificados no Gr√°fico:**

1.  **Zona de Alta Discord√¢ncia** (canto inferior direito):
    -   Analistas que aprovam muito, mas t√™m suas decis√µes
        frequentemente revertidas
    -   Sugere crit√©rios excessivamente lenientes ou desalinhados
2.  **Zona de Concord√¢ncia Moderada** (centro):
    -   Maior concentra√ß√£o de analistas
    -   Taxas de aprova√ß√£o entre 40-70% com concord√¢ncia entre 50-80%
    -   Representa o ‚Äúpadr√£o operacional‚Äù do sistema
3.  **Correla√ß√£o Inexistente**:
    -   N√£o h√° rela√ß√£o linear clara entre taxa de aprova√ß√£o e
        concord√¢ncia
    -   Analistas com taxas similares de aprova√ß√£o t√™m concord√¢ncias
        drasticamente diferentes

**Insights e Implica√ß√µes Pr√°ticas:**

1.  **Revis√£o N√£o √© Protocolar**: Ao contr√°rio da hip√≥tese inicial, a
    fase de parecer atua como um filtro adicional significativo,
    especialmente para analistas com altas taxas de aprova√ß√£o.

2.  **Desalinhamento Sistem√°tico**: A concentra√ß√£o de pontos abaixo da
    diagonal sugere que os pareceristas aplicam crit√©rios mais rigorosos
    que os analistas, criando um ‚Äúduplo padr√£o‚Äù no processo.

3.  **Risco Reputacional para Analistas**: Analistas como 300603544 e
    123493017, cujas recomenda√ß√µes s√£o frequentemente revertidas, podem
    perder credibilidade no processo.

4.  **Inefici√™ncia Processual**: Com taxa m√©dia de concord√¢ncia de
    apenas 66.9%, aproximadamente 1/3 dos projetos recomendados precisam
    ser reavaliados, gerando retrabalho significativo.

## 5. Pergunta 3: Revers√µes de Decis√£o (N√£o Recomendado ‚Üí Recomendado)

Qual percentual dos projetos ‚ÄúN√£o Recomendado‚Äù pelos Analistas o
Parecerista aprovou depois?

### Pergunta 3 - Por Empresa

``` {python}
# Filtrar projetos n√£o recomendados pelo analista
df_nao_recom_analista = df_completo[df_completo['decisao_analista'] == 'N√£o Recomendado'].copy()

# An√°lise por empresa
reversoes_empresa = df_nao_recom_analista.groupby('lst_norazaosocial').agg({
    'decisao_parecerista': [
        'count',
        lambda x: (x == 'Recomendado').sum(),
        lambda x: (x == 'Recomendado').mean() * 100
    ]
}).round(2)

reversoes_empresa.columns = ['Projetos_Nao_Recom', 'Revertidos_para_Recom', 'Taxa_Reversao_%']

# Filtrar: m√≠nimo 5 projetos E pelo menos 1 revers√£o
reversoes_empresa = reversoes_empresa[
    (reversoes_empresa['Projetos_Nao_Recom'] >= 5) & 
    (reversoes_empresa['Revertidos_para_Recom'] > 0)
]
reversoes_empresa = reversoes_empresa.sort_values('Taxa_Reversao_%', ascending=False)

# Visualiza√ß√£o - Top 15 empresas com mais revers√µes
fig, ax = plt.subplots(figsize=(14, 9))

top15_reversao = reversoes_empresa.head(15)

# Verificar se h√° empresas para mostrar
if len(top15_reversao) == 0:
    ax.text(0.5, 0.5, 'Nenhuma empresa teve projetos revertidos\ncom o crit√©rio m√≠nimo de 5 projetos', 
            ha='center', va='center', transform=ax.transAxes, fontsize=14)
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.axis('off')
else:
    empresas_nomes = [nome[:40] + '...' if len(nome) > 40 else nome for nome in top15_reversao.index]

    bars = ax.barh(range(len(top15_reversao)), top15_reversao['Taxa_Reversao_%'], 
                    color='coral', edgecolor='darkred', linewidth=1.5)
    ax.set_yticks(range(len(top15_reversao)))
    ax.set_yticklabels(empresas_nomes)
    ax.set_xlabel('Taxa de Revers√£o (%)', fontsize=11)
    ax.set_title('Top 15 Empresas com Maior Taxa de Revers√£o (N√£o Recomendado ‚Üí Recomendado)', fontsize=13)
    ax.grid(axis='x', alpha=0.3)

    # Adicionar valores nas barras (taxa % e n√∫mero absoluto)
    for i, (idx, row) in enumerate(top15_reversao.iterrows()):
        width = row['Taxa_Reversao_%']
        revertidos = int(row['Revertidos_para_Recom'])
        total = int(row['Projetos_Nao_Recom'])
        
        # Taxa de revers√£o √† direita da barra
        ax.text(width + 0.5, bars[i].get_y() + bars[i].get_height()/2, 
                f'{width:.1f}%', ha='left', va='center', fontsize=9, fontweight='bold')
        
        # N√∫mero de projetos dentro da barra
        ax.text(width/2, bars[i].get_y() + bars[i].get_height()/2, 
                f'{revertidos}/{total}', ha='center', va='center', fontsize=9, 
                color='white', fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='darkred', alpha=0.8))

    # Adicionar legenda explicativa
    ax.text(0.02, 0.98, 'Formato: Revertidos/Total', transform=ax.transAxes,
            fontsize=9, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.show()

# Estat√≠sticas sobre empresas filtradas
total_empresas_original = df_nao_recom_analista['lst_norazaosocial'].nunique()
total_empresas_com_minimo = len(reversoes_empresa)
empresas_sem_reversao = df_nao_recom_analista.groupby('lst_norazaosocial').agg({
    'decisao_parecerista': [
        'count',
        lambda x: (x == 'Recomendado').sum()
    ]
})
empresas_sem_reversao.columns = ['Total', 'Revertidos']
empresas_zero_reversao = len(empresas_sem_reversao[(empresas_sem_reversao['Total'] >= 5) & (empresas_sem_reversao['Revertidos'] == 0)])

print(f"\nüìä Estat√≠sticas de Revers√£o por Empresa:")
print(f"Total de empresas com projetos n√£o recomendados: {total_empresas_original}")
print(f"Empresas com ‚â•5 projetos n√£o recomendados E pelo menos 1 revers√£o: {total_empresas_com_minimo}")
print(f"Empresas com ‚â•5 projetos n√£o recomendados mas ZERO revers√£o: {empresas_zero_reversao}")

if len(reversoes_empresa) > 0:
    print("\nTop 10 Empresas com Maior Taxa de Revers√£o:")
    print("-" * 80)
    for idx, row in reversoes_empresa.head(10).iterrows():
        print(f"{idx[:50]:.<50} {row['Taxa_Reversao_%']:>6.1f}% ({int(row['Revertidos_para_Recom']):>3} de {int(row['Projetos_Nao_Recom']):>3} projetos)")
    print("-" * 80)
else:
    print("\nNenhuma empresa atendeu aos crit√©rios de revers√£o.")
```

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-8-output-1.png)


    üìä Estat√≠sticas de Revers√£o por Empresa:
    Total de empresas com projetos n√£o recomendados: 1546
    Empresas com ‚â•5 projetos n√£o recomendados E pelo menos 1 revers√£o: 12
    Empresas com ‚â•5 projetos n√£o recomendados mas ZERO revers√£o: 200

    Top 10 Empresas com Maior Taxa de Revers√£o:
    --------------------------------------------------------------------------------
    ATECH - NEGOCIOS EM TECNOLOGIAS S.A...............  100.0% ( 16 de  16 projetos)
    BORGWARNER BRASIL LTDA............................  100.0% ( 31 de  31 projetos)
    CIA INDUSTRIAL H. CARLOS SCHNEIDER................  100.0% (  6 de   6 projetos)
    ELETRONUCLEAR S.A.................................  100.0% (  7 de   7 projetos)
    MASTER SISTEMAS AUTOMOTIVOS LTDA..................  100.0% (  6 de   6 projetos)
    RANDON SA IMPLEMENTOS E PARTICIPACOES.............  100.0% (  7 de   7 projetos)
    SAFRAN CABIN BRAZIL LTDA..........................  100.0% (  5 de   5 projetos)
    SCANIA LATIN AMERICA LTDA.........................  100.0% ( 26 de  26 projetos)
    VALE S.A..........................................  100.0% (  9 de   9 projetos)
    TAURUS ARMAS S.A..................................  100.0% ( 10 de  10 projetos)
    --------------------------------------------------------------------------------

### Pergunta 3 - An√°lise por √°rea

``` {python}
# An√°lise por √°rea
reversoes_area = df_nao_recom_analista.groupby('daproj_dsareaprojeto').agg({
    'decisao_parecerista': [
        'count',
        lambda x: (x == 'Recomendado').sum(),
        lambda x: (x == 'Recomendado').mean() * 100
    ]
}).round(2)

reversoes_area.columns = ['Projetos_Nao_Recom', 'Revertidos_para_Recom', 'Taxa_Reversao_%']
reversoes_area = reversoes_area[reversoes_area['Projetos_Nao_Recom'] >= 10]  # M√≠nimo 10 projetos
reversoes_area = reversoes_area.sort_values('Taxa_Reversao_%', ascending=False)

# Visualiza√ß√£o
fig, ax = plt.subplots(figsize=(12, 8))

top15_area = reversoes_area.head(15)
areas_nomes = [area[:30] + '...' if len(str(area)) > 30 else str(area) for area in top15_area.index]

bars = ax.bar(range(len(top15_area)), top15_area['Taxa_Reversao_%'], 
               color='lightgreen', edgecolor='darkgreen')
ax.set_xticks(range(len(top15_area)))
ax.set_xticklabels(areas_nomes, rotation=45, ha='right')
ax.set_ylabel('Taxa de Revers√£o (%)')
ax.set_title('Top 15 √Åreas com Maior Taxa de Revers√£o (N√£o Recomendado ‚Üí Recomendado)')
ax.grid(axis='y', alpha=0.3)

# Adicionar valores nas barras
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2, height + 0.5,
            f'{height:.1f}%', ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

print("\nTop 10 √Åreas com Maior Taxa de Revers√£o:")
print(reversoes_area.head(10))

# An√°lise espec√≠fica do parecerista
print("\n\nAn√°lise: O parecerista tem prefer√™ncia por alguma √°rea?")
media_geral_reversao = df_nao_recom_analista['decisao_parecerista'].apply(lambda x: x == 'Recomendado').mean() * 100
print(f"Taxa m√©dia geral de revers√£o: {media_geral_reversao:.1f}%")

areas_acima_media = reversoes_area[reversoes_area['Taxa_Reversao_%'] > media_geral_reversao * 1.5]
print(f"\n√Åreas com taxa de revers√£o 50% acima da m√©dia:")
for area, row in areas_acima_media.iterrows():
    print(f"- {area}: {row['Taxa_Reversao_%']:.1f}% ({row['Projetos_Nao_Recom']} projetos)")
```

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-9-output-1.png)


    Top 10 √Åreas com Maior Taxa de Revers√£o:
                          Projetos_Nao_Recom  Revertidos_para_Recom  \
    daproj_dsareaprojeto                                              
    Mec√¢nica                             346                     94   
    Transporte                            34                      6   
    Constru√ß√£o Civil                      21                      3   
    Minera√ß√£o                             43                      6   
    Petroqu√≠mica                          26                      3   
    Moveleira                             14                      1   
    Outros                              1285                     65   
    Metalurgia                           259                     11   
    Bens de Consumo                      120                      5   
    Eletroeletr√¥nica                     408                     11   

                          Taxa_Reversao_%  
    daproj_dsareaprojeto                   
    Mec√¢nica                        27.17  
    Transporte                      17.65  
    Constru√ß√£o Civil                14.29  
    Minera√ß√£o                       13.95  
    Petroqu√≠mica                    11.54  
    Moveleira                        7.14  
    Outros                           5.06  
    Metalurgia                       4.25  
    Bens de Consumo                  4.17  
    Eletroeletr√¥nica                 2.70  


    An√°lise: O parecerista tem prefer√™ncia por alguma √°rea?
    Taxa m√©dia geral de revers√£o: 4.8%

    √Åreas com taxa de revers√£o 50% acima da m√©dia:
    - Mec√¢nica: 27.2% (346.0 projetos)
    - Transporte: 17.6% (34.0 projetos)
    - Constru√ß√£o Civil: 14.3% (21.0 projetos)
    - Minera√ß√£o: 13.9% (43.0 projetos)
    - Petroqu√≠mica: 11.5% (26.0 projetos)
    - Moveleira: 7.1% (14.0 projetos)

### An√°lise e Insights da Pergunta 3

**O que foi investigado:**

Esta an√°lise foca exclusivamente nos casos em que um projeto ‚ÄúN√£o
Recomendado‚Äù pelo analista foi posteriormente ‚ÄúRecomendado‚Äù pelo
parecerista. O objetivo foi quantificar esse fen√¥meno de revers√£o e
identificar se existem padr√µes sistem√°ticos relacionados √†s empresas
proponentes ou √†s √°reas tecnol√≥gicas dos projetos para o ano de 2021.

**Principais Descobertas:**

A **taxa m√©dia geral de revers√£o √© de 4,8%**, mas essa m√©dia pode
esconder padr√µes de concentra√ß√£o:

1.  **An√°lise por Empresa - Revers√£o Total:**
    -   **10 empresas** tiveram **100% de revers√£o** em TODOS os seus
        projetos rejeitados
    -   Destaque para volumes expressivos: **Borgwarner Brasil
        (31/31)**, **Scania (26/26)**, **Atech (16/16)**, **Taurus Armas
        (10/10)** e **Vale (9/9)**
    -   Empresas de menor porte: **ThyssenKrupp (7/11 - 63.6%)** e
        **John Deere (5/7 - 71.4%)** tamb√©m apresentaram altas taxas
    -   **200 empresas** com ‚â•5 projetos rejeitados tiveram **ZERO
        revers√£o**
2.  **An√°lise por √Årea - Vi√©s T√©cnico:**
    -   **Mec√¢nica**: l√≠der absoluto com **27.2%** de revers√£o (94 de
        346 projetos)
    -   **Transporte**: **17.6%** (6 de 34 projetos)
    -   **Constru√ß√£o Civil**: **14.3%** (3 de 21 projetos)
    -   **Minera√ß√£o**: **13.9%** (6 de 43 projetos)
    -   √Åreas como **Software** e **Finan√ßa** tiveram **0%** de revers√£o

**Padr√µes Identificados:**

1.  **‚ÄúClube VIP‚Äù Corporativo**: As 10 empresas com 100% de revers√£o.

2.  **Hierarquia de √Åreas**: Existe uma certa prefer√™ncia por projetos
    de engenharia tradicional (Mec√¢nica, Transporte) versus √°reas
    digitais/servi√ßos.

3.  **Desigualdade**: Enquanto 10 empresas t√™m sucesso total, 200 outras
    n√£o conseguem ‚Äúreverter‚Äù uma decis√£o ainda na primeira fase, antes
    da contesta√ß√£o.

**Insights e Implica√ß√µes Pr√°ticas:**

1.  **Quebra da Isonomia**: O processo atual pode favorecer
    sistematicamente empresas espec√≠ficas, comprometendo a equidade
    fundamental esperada em programas governamentais de incentivo.

2.  **Crit√©rios Ocultos**: A discrep√¢ncia entre √°reas pode sugerir
    exist√™ncia de crit√©rios n√£o documentados ou press√µes setoriais
    espec√≠ficas.

## 6. Pergunta 4: Dispers√£o de √Åreas por Analista

Os analistas focaram em mais √°reas do que era pra focar? Quais analistas
pegaram mais √°reas?

### Pergunta 4

``` {python}
# An√°lise de dispers√£o por analista
dispersao_areas = df_completo.groupby('do_saat_idunicopessoaanalise').agg({
    'daproj_dsareaprojeto': ['nunique', 'count', lambda x: x.value_counts().to_dict()]
}).round(2)

dispersao_areas.columns = ['Num_Areas_Diferentes', 'Total_Projetos', 'Projetos_por_Area']
dispersao_areas['Projetos_por_Area_Media'] = dispersao_areas['Total_Projetos'] / dispersao_areas['Num_Areas_Diferentes']
dispersao_areas = dispersao_areas.sort_values('Num_Areas_Diferentes', ascending=False)

# Visualiza√ß√£o - Analistas mais dispersos
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Top 15 analistas com mais √°reas
top15_dispersao = dispersao_areas.head(15)
analistas_ids = [f"ID_{int(idx)}" for idx in top15_dispersao.index]

# Gr√°fico 1: N√∫mero de √°reas diferentes
bars1 = ax1.bar(range(len(top15_dispersao)), top15_dispersao['Num_Areas_Diferentes'], 
                 color='purple', alpha=0.7, edgecolor='black')
ax1.set_xticks(range(len(top15_dispersao)))
ax1.set_xticklabels(analistas_ids, rotation=45, ha='right')
ax1.set_ylabel('N√∫mero de √Åreas Diferentes')
ax1.set_title('Top 15 Analistas com Maior Dispers√£o de √Åreas')
ax1.grid(axis='y', alpha=0.3)

# Adicionar valores
for bar in bars1:
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2, height + 0.2,
             f'{int(height)}', ha='center', va='bottom', fontsize=9)

# Gr√°fico 2: Scatter - √Åreas vs Projetos
scatter2 = ax2.scatter(dispersao_areas['Num_Areas_Diferentes'], 
                       dispersao_areas['Total_Projetos'],
                       c=dispersao_areas['Projetos_por_Area_Media'],
                       cmap='coolwarm', s=100, alpha=0.7, edgecolors='black')
ax2.set_xlabel('N√∫mero de √Åreas Diferentes')
ax2.set_ylabel('Total de Projetos')
ax2.set_title('Rela√ß√£o entre Dispers√£o de √Åreas e Volume de Projetos')
ax2.grid(True, alpha=0.3)

# Colorbar
cbar = plt.colorbar(scatter2, ax=ax2)
cbar.set_label('M√©dia de Projetos por √Årea', rotation=270, labelpad=20)

plt.tight_layout()
plt.show()

# Detalhamento dos mais dispersos
print("Top 10 Analistas com Maior Dispers√£o de √Åreas:")
print("-" * 80)
for idx, row in dispersao_areas.head(10).iterrows():
    print(f"\nAnalista ID_{int(idx)}:")
    print(f"  - N√∫mero de √°reas: {row['Num_Areas_Diferentes']}")
    print(f"  - Total de projetos: {row['Total_Projetos']}")
    print(f"  - M√©dia projetos/√°rea: {row['Projetos_por_Area_Media']:.1f}")
    
    # Top 5 √°reas deste analista
    areas_dict = row['Projetos_por_Area']
    if isinstance(areas_dict, dict):
        areas_sorted = sorted(areas_dict.items(), key=lambda x: x[1], reverse=True)[:5]
        print("  - Top 5 √°reas:")
        for area, count in areas_sorted:
            print(f"    * {area}: {count} projetos")
```

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-10-output-1.png)

    Top 10 Analistas com Maior Dispers√£o de √Åreas:
    --------------------------------------------------------------------------------

    Analista ID_301010505:
      - N√∫mero de √°reas: 15
      - Total de projetos: 562
      - M√©dia projetos/√°rea: 37.5
      - Top 5 √°reas:
        * Software: 286 projetos
        * Outros: 139 projetos
        * TIC - Software: 31 projetos
        * Telecomunica√ß√µes: 29 projetos
        * Finan√ßa: 18 projetos

    Analista ID_128526134:
      - N√∫mero de √°reas: 10
      - Total de projetos: 198
      - M√©dia projetos/√°rea: 19.8
      - Top 5 √°reas:
        * Eletroeletr√¥nica: 73 projetos
        * Software: 64 projetos
        * Outros: 27 projetos
        * TIC - Software: 11 projetos
        * Metalurgia: 6 projetos

    Analista ID_122036205:
      - N√∫mero de √°reas: 9
      - Total de projetos: 138
      - M√©dia projetos/√°rea: 15.3
      - Top 5 √°reas:
        * Outros: 72 projetos
        * Qu√≠mica: 37 projetos
        * Bens de Consumo: 13 projetos
        * Farmac√™utica: 9 projetos
        * Alimentos: 3 projetos

    Analista ID_116768062:
      - N√∫mero de √°reas: 8
      - Total de projetos: 347
      - M√©dia projetos/√°rea: 43.4
      - Top 5 √°reas:
        * Metalurgia: 145 projetos
        * Outros: 76 projetos
        * Minera√ß√£o: 58 projetos
        * Bens de Consumo: 50 projetos
        * Papel: 7 projetos

    Analista ID_121916780:
      - N√∫mero de √°reas: 8
      - Total de projetos: 262
      - M√©dia projetos/√°rea: 32.8
      - Top 5 √°reas:
        * Outros: 146 projetos
        * Eletroeletr√¥nica: 50 projetos
        * Constru√ß√£o Civil: 24 projetos
        * Bens de Consumo: 13 projetos
        * T√™xtil : 12 projetos

    Analista ID_300249953:
      - N√∫mero de √°reas: 8
      - Total de projetos: 259
      - M√©dia projetos/√°rea: 32.4
      - Top 5 √°reas:
        * Software: 178 projetos
        * TIC - Software: 41 projetos
        * TIC: 17 projetos
        * Outros: 11 projetos
        * Finan√ßa: 5 projetos

    Analista ID_300603544:
      - N√∫mero de √°reas: 8
      - Total de projetos: 384
      - M√©dia projetos/√°rea: 48.0
      - Top 5 √°reas:
        * Software: 151 projetos
        * TIC - Software: 135 projetos
        * Finan√ßa: 41 projetos
        * TIC: 28 projetos
        * Seguro: 15 projetos

    Analista ID_301043772:
      - N√∫mero de √°reas: 8
      - Total de projetos: 163
      - M√©dia projetos/√°rea: 20.4
      - Top 5 √°reas:
        * Eletroeletr√¥nica: 91 projetos
        * Mec√¢nica: 46 projetos
        * Metalurgia: 10 projetos
        * Outros: 6 projetos
        * Transporte: 4 projetos

    Analista ID_301097658:
      - N√∫mero de √°reas: 8
      - Total de projetos: 151
      - M√©dia projetos/√°rea: 18.9
      - Top 5 √°reas:
        * Software: 106 projetos
        * TIC - Software: 35 projetos
        * Finan√ßa: 2 projetos
        * Seguro: 2 projetos
        * Outros: 2 projetos

    Analista ID_107822708:
      - N√∫mero de √°reas: 8
      - Total de projetos: 151
      - M√©dia projetos/√°rea: 18.9
      - Top 5 √°reas:
        * Software: 80 projetos
        * TIC - Software: 39 projetos
        * Finan√ßa: 14 projetos
        * TIC: 9 projetos
        * Outros: 4 projetos

### An√°lise e Insights da Pergunta 4

**O que foi investigado:**

Esta an√°lise investigou a dispers√£o de trabalho dos analistas, medindo o
n√∫mero de √°reas de conhecimento distintas que cada um avaliou. O
objetivo era identificar se os analistas atuam como especialistas
(focados em poucas √°reas) ou como generalistas (atuando em m√∫ltiplas
√°reas), e compreender como essa distribui√ß√£o pode afetar a qualidade das
avalia√ß√µes.

**Principais Descobertas:**

Os dados revelam um sistema com perfis heterog√™neos de atua√ß√£o, com
importantes nuances a considerar:

1.  **Distribui√ß√£o da Dispers√£o:**
    -   O analista com maior diversifica√ß√£o, **ID_301010505**, atua em
        **15 √°reas distintas** (562 projetos total)
    -   **10 analistas** trabalham com 8 ou mais √°reas diferentes
    -   A maioria dos analistas concentra-se em 2-5 √°reas
    -   O gr√°fico de dispers√£o mostra que n√£o h√° correla√ß√£o direta entre
        volume total e n√∫mero de √°reas
2.  **Padr√£o ‚ÄúEspecialista com Cauda Longa‚Äù:**
    -   Mesmo analistas com alta dispers√£o mant√™m √°reas de concentra√ß√£o
        principal
    -   **ID_301010505**: 50.9% dos projetos em Software (286/562)
    -   **ID_116768062**: 41.8% em Metalurgia (145/347)
    -   **ID_300603544**: 39.3% em Software + 35.2% em TIC-Software
        (74.5% combinado)
3.  **Perfis de Especializa√ß√£o Identificados:**
    -   **Especialistas Digitais**: Analistas focados em
        Software/TIC/Finan√ßa
    -   **Especialistas Industriais**: Concentrados em
        Metalurgia/Mec√¢nica/Qu√≠mica
    -   **Generalistas Controlados**: Atuam em 6-8 √°reas com
        distribui√ß√£o mais equilibrada

**Insights e Implica√ß√µes Pr√°ticas:**

1.  **Flexibilidade Operacional**: A dispers√£o observada pode refletir a
    necessidade de adapta√ß√£o √† demanda vari√°vel por √°rea, permitindo
    melhor distribui√ß√£o de carga de trabalho.

2.  **Expertise Complementar**: Analistas com forma√ß√£o multidisciplinar
    podem agregar valor em projetos que cruzam fronteiras tecnol√≥gicas,
    cada vez mais comuns em P&D moderna.

3.  **Oportunidades de Melhoria**:

    -   Desenvolver matriz de compet√™ncias para mapear expertise
        principal vs.¬†secund√°ria
    -   Implementar sistema de aloca√ß√£o que considere tanto
        especializa√ß√£o quanto disponibilidade
    -   Criar programa de capacita√ß√£o cruzada para √°reas emergentes

4.  **Equil√≠brio Necess√°rio**: O modelo atual, com m√©dia de 4.6 √°reas
    por analista, pode representar um ponto de equil√≠brio entre
    especializa√ß√£o profunda e flexibilidade operacional.

**Recomenda√ß√µes Construtivas:**

-   **Mapeamento de Compet√™ncias**: Documentar forma√ß√£o e experi√™ncia
    dos analistas para aloca√ß√£o otimizada
-   **Mentoria Cruzada**: Analistas especialistas podem orientar colegas
    em suas √°reas secund√°rias
-   **Revis√£o Colaborativa**: Para projetos em √°reas secund√°rias,
    implementar revis√£o por especialista da √°rea
-   **Forma√ß√£o Continuada**: Investir em capacita√ß√£o espec√≠fica para
    analistas que demonstrem interesse em expandir expertise
-   **Indicadores de Qualidade**: Monitorar se h√° diferen√ßa na qualidade
    das an√°lises entre √°reas prim√°rias e secund√°rias
-   **Rota√ß√£o Planejada**: Permitir que analistas expandam gradualmente
    suas √°reas de atua√ß√£o com suporte adequado

## 7. Pergunta 5: Fichas Individuais dos Top 15 Analistas

An√°lise detalhada dos 15 analistas com maior volume de projetos

### Pergunta 5

``` {python}
# Selecionar top 15 analistas por volume
top15_analistas = taxa_aprovacao_analista.head(15).index

# Preparar dados para fichas
fichas_analistas = []

for analista_id in top15_analistas:
    # Filtrar projetos do analista
    projetos_analista = df_completo[df_completo['do_saat_idunicopessoaanalise'] == analista_id]
    
    # Estat√≠sticas b√°sicas
    ficha = {
        'analista_id': int(analista_id),
        'total_projetos': len(projetos_analista),
        'projetos_recomendados_do': (projetos_analista['decisao_analista'] == 'Recomendado').sum(),
        'taxa_aprovacao_do': (projetos_analista['decisao_analista'] == 'Recomendado').mean() * 100,
        'projetos_recomendados_parecer': (projetos_analista['decisao_parecerista'] == 'Recomendado').sum(),
        'taxa_aprovacao_parecer': (projetos_analista['decisao_parecerista'] == 'Recomendado').mean() * 100,
    }
    
    # Taxa de concord√¢ncia
    recom_do = projetos_analista[projetos_analista['decisao_analista'] == 'Recomendado']
    if len(recom_do) > 0:
        ficha['taxa_concordancia'] = (recom_do['decisao_parecerista'] == 'Recomendado').mean() * 100
    else:
        ficha['taxa_concordancia'] = 0
    
    # √Åreas analisadas
    areas = projetos_analista['daproj_dsareaprojeto'].value_counts()
    ficha['num_areas'] = len(areas)
    ficha['top_5_areas'] = areas.head(5).to_dict()
    
    # Clusters (se houver coluna de cluster)
    if 'cluster' in projetos_analista.columns:
        clusters = projetos_analista['cluster'].value_counts()
        ficha['num_clusters'] = len(clusters)
        ficha['top_clusters'] = clusters.head(3).tolist()
    else:
        ficha['num_clusters'] = 'N/A'
        ficha['top_clusters'] = []
    
    fichas_analistas.append(ficha)

# Criar visualiza√ß√£o das fichas
fig = plt.figure(figsize=(16, 20))
fig.suptitle('Fichas Individuais - Top 15 Analistas por Volume', fontsize=16, y=0.995)

for i, ficha in enumerate(fichas_analistas[:15]):
    # Criar subplot para cada analista (5 linhas x 3 colunas)
    ax = plt.subplot(5, 3, i+1)
    
    # Preparar texto da ficha
    texto = f"ID: {ficha['analista_id']}\n"
    texto += f"Total Projetos: {ficha['total_projetos']}\n"
    texto += f"Taxa Aprova√ß√£o DO: {ficha['taxa_aprovacao_do']:.1f}%\n"
    texto += f"Taxa Aprova√ß√£o Parecer: {ficha['taxa_aprovacao_parecer']:.1f}%\n"
    texto += f"Taxa Concord√¢ncia: {ficha['taxa_concordancia']:.1f}%\n"
    texto += f"N√∫mero de √Åreas: {ficha['num_areas']}\n"
    texto += f"Clusters Atendidos: {ficha['num_clusters']}\n\n"
    texto += "Top 5 √Åreas:\n"
    
    for j, (area, count) in enumerate(list(ficha['top_5_areas'].items())[:5]):
        area_nome = str(area)[:25] + '...' if len(str(area)) > 25 else str(area)
        texto += f"{j+1}. {area_nome} ({count})\n"
    
    # Adicionar texto
    ax.text(0.05, 0.95, texto, transform=ax.transAxes, 
            fontsize=9, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
    
    # Remover eixos
    ax.set_xlim(0, 1)
    ax.set_ylim(0, 1)
    ax.axis('off')
    
    # Adicionar borda
    for spine in ['top', 'right', 'bottom', 'left']:
        ax.spines[spine].set_visible(True)
        ax.spines[spine].set_linewidth(2)

plt.tight_layout()
plt.show()

# Tabela resumo
df_fichas = pd.DataFrame(fichas_analistas)
df_fichas_resumo = df_fichas[['analista_id', 'total_projetos', 'taxa_aprovacao_do', 
                              'taxa_aprovacao_parecer', 'taxa_concordancia', 'num_areas']]
df_fichas_resumo = df_fichas_resumo.round(1)
print("\nResumo dos Top 15 Analistas:")
print(df_fichas_resumo)
```

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-11-output-1.png)


    Resumo dos Top 15 Analistas:
        analista_id  total_projetos  taxa_aprovacao_do  taxa_aprovacao_parecer  \
    0     301040479             628               59.7                    64.5   
    1     301010505             565               69.4                    41.4   
    2     300713489             534               64.6                    24.2   
    3     108293000             438               65.1                    50.5   
    4     300603544             384               53.6                     4.7   
    5     301241624             367               27.8                    24.5   
    6     301043768             361               80.1                    78.7   
    7     116768062             347               66.0                    59.4   
    8     300788615             346               76.0                    45.4   
    9     301064200             306               59.5                    32.4   
    10    300869814             291               63.9                    48.1   
    11    300850244             283               58.3                    56.2   
    12    301043769             270               83.0                    77.4   
    13    300869864             263               97.3                    93.2   
    14    121916780             262               72.5                    48.1   

        taxa_concordancia  num_areas  
    0                88.8          5  
    1                58.9         15  
    2                37.4          5  
    3                77.5          3  
    4                 8.7          8  
    5                73.5          6  
    6                97.6          5  
    7                87.3          8  
    8                58.6          3  
    9                54.4          2  
    10               74.2          3  
    11               89.1          3  
    12               87.9          5  
    13               95.7          4  
    14               65.3          8  

### An√°lise e Insights da Pergunta 5 (com base na imagem)

**O que foi investigado:**

As fichas de desempenho visual acima consolidam as m√©tricas-chave
(volume, taxas de aprova√ß√£o, concord√¢ncia e dispers√£o de √°reas) para os
15 analistas com maior volume de projetos. O objetivo √© transformar os
dados em um perfil individualizado, facilitando a identifica√ß√£o de
padr√µes e comportamentos at√≠picos (*outliers*).

**Principais Descobertas Visuais:**

A disposi√ß√£o em formato de ‚Äúcards‚Äù torna a compara√ß√£o entre os analistas
imediata e intuitiva, destacando a heterogeneidade do grupo:

-   **O Perfil Generalista:** A ficha do analista **ID 301010505**
    (canto superior, centro) salta aos olhos pelo seu ‚ÄúN√∫mero de √Åreas:
    15‚Äù, o maior de todos. Este card visualiza claramente o perfil de um
    analista generalista, com uma taxa de concord√¢ncia de apenas
    **58.9%**, sugerindo que a amplitude de sua atua√ß√£o pode estar
    impactando o alinhamento de suas decis√µes.

-   **O Desalinhamento Cr√≠tico:** O caso mais preocupante √© visualizado
    na ficha do analista **ID 300603544** (segunda linha, centro). O
    n√∫mero ‚ÄúTaxa Concord√¢ncia: 8.7%‚Äù √© um forte alerta visual. Ele
    indica que, apesar de analisar um grande volume de projetos (384),
    suas decis√µes de aprova√ß√£o s√£o quase sistematicamente revertidas
    pelo parecerista, apontando para um desalinhamento fundamental de
    crit√©rios.

-   **O Benchmark de Consist√™ncia:** Em forte contraste, a ficha do
    analista **ID 300869864** (pen√∫ltima linha, centro) funciona como um
    modelo de refer√™ncia. Os valores de ‚ÄúTaxa Aprova√ß√£o DO: 97.3%‚Äù e
    ‚ÄúTaxa Concord√¢ncia: 95.7%‚Äù mostram um alinhamento quase perfeito,
    representando um padr√£o de alta produtividade e consist√™ncia com a
    fase de parecer.

-   **O Perfil Rigoroso:** A ficha do **ID 301241624** (segunda linha, √†
    esquerda) ilustra o perfil do analista mais rigoroso, com uma ‚ÄúTaxa
    Aprova√ß√£o DO‚Äù de apenas **27.8%**. A visualiza√ß√£o conjunta com a
    ‚ÄúTaxa Concord√¢ncia‚Äù de **73.5%** permite inferir que, embora aprove
    poucos projetos, suas recomenda√ß√µes s√£o, em geral, bem fundamentadas
    e mantidas.

**Insights e Implica√ß√µes Pr√°ticas:**

A visualiza√ß√£o em formato de ‚Äòficha‚Äô transforma uma tabela de dados em
uma ferramenta de gest√£o de desempenho muito mais eficaz. Para um
gestor, √© poss√≠vel identificar com um r√°pido olhar os perfis que
necessitam de aten√ß√£o, como os que apresentam baixa concord√¢ncia ou
dispers√£o excessiva de √°reas. Da mesma forma, os perfis de alta
consist√™ncia podem ser estudados para disseminar boas pr√°ticas. Esta
abordagem visual √© fundamental para iniciar di√°logos focados, calibrar
crit√©rios e otimizar o processo de avalia√ß√£o de forma mais √°gil e
intuitiva.

## 8. Pergunta 6: Modelo Preditivo de Aprova√ß√£o no Parecer

An√°lise complexa: probabilidade de aprova√ß√£o baseada em caracter√≠sticas
do projeto e analista

### Pergunta 6 - Prepara√ß√£o dos dados

``` {python}
# Preparar features textuais
print("Preparando dados para o modelo preditivo...")

# Combinar campos textuais
df_modelo = df_completo.copy()
df_modelo['texto_tecnico'] = (
    df_modelo['daproj_dsmetodologiautilizada'].fillna('') + ' ' +
    df_modelo['daproj_dselementotecnologico'].fillna('') + ' ' +
    df_modelo['daproj_dsdesafiotecnologico'].fillna('')
)

# Contar palavras-chave
df_modelo['num_palavras_chave'] = df_modelo['daproj_dspalavrachave'].fillna('').apply(
    lambda x: len(str(x).split(',')) if x else 0
)

# Criar target bin√°rio
df_modelo['aprovado_parecer'] = (df_modelo['decisao_parecerista'] == 'Recomendado').astype(int)

# Filtrar dados v√°lidos
df_modelo_valido = df_modelo[
    df_modelo['texto_tecnico'].str.len() > 10
].copy()

print(f"Projetos v√°lidos para modelagem: {len(df_modelo_valido):,}")
```

    Preparando dados para o modelo preditivo...
    Projetos v√°lidos para modelagem: 13,196

### Pergunta 6 - Feature Engineering

``` {python}
# TF-IDF para texto t√©cnico
print("Criando features TF-IDF...")
tfidf = TfidfVectorizer(max_features=100, ngram_range=(1, 2), min_df=5)
tfidf_features = tfidf.fit_transform(df_modelo_valido['texto_tecnico'])
tfidf_df = pd.DataFrame(tfidf_features.toarray(), 
                        columns=[f'tfidf_{word}' for word in tfidf.get_feature_names_out()],
                        index=df_modelo_valido.index)

# Features categ√≥ricas
print("Criando features categ√≥ricas...")
le_area = LabelEncoder()
le_analista = LabelEncoder()

df_modelo_valido['area_encoded'] = le_area.fit_transform(df_modelo_valido['daproj_dsareaprojeto'].fillna('OUTROS'))
df_modelo_valido['analista_encoded'] = le_analista.fit_transform(df_modelo_valido['do_saat_idunicopessoaanalise'])

# Estat√≠sticas do analista
stats_analista = df_completo.groupby('do_saat_idunicopessoaanalise').agg({
    'decisao_analista': lambda x: (x == 'Recomendado').mean()
}).rename(columns={'decisao_analista': 'taxa_hist_analista'})

df_modelo_valido = df_modelo_valido.merge(
    stats_analista, 
    left_on='do_saat_idunicopessoaanalise', 
    right_index=True, 
    how='left'
)

# Combinar todas as features
X = pd.concat([
    df_modelo_valido[['area_encoded', 'analista_encoded', 'num_palavras_chave', 'taxa_hist_analista']],
    tfidf_df
], axis=1)

y = df_modelo_valido['aprovado_parecer']

print(f"Shape das features: {X.shape}")
print(f"Distribui√ß√£o do target: {y.value_counts(normalize=True)}")
```

    Criando features TF-IDF...
    Criando features categ√≥ricas...
    Shape das features: (13196, 104)
    Distribui√ß√£o do target: aprovado_parecer
    0    0.553956
    1    0.446044
    Name: proportion, dtype: float64

### Pergunta 6 - Treinamento do modelo

``` {python}
# Split dos dados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Treinar Random Forest
print("\nTreinando modelo Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=20,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train, y_train)

# Avalia√ß√£o
y_pred = rf_model.predict(X_test)
y_proba = rf_model.predict_proba(X_test)[:, 1]

print("\nPerformance do Modelo:")
print(classification_report(y_test, y_pred))

# Cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='roc_auc')
print(f"\nROC-AUC m√©dio (cross-validation): {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})")
```


    Treinando modelo Random Forest...

    Performance do Modelo:
                  precision    recall  f1-score   support

               0       0.85      0.77      0.81      1462
               1       0.75      0.83      0.79      1178

        accuracy                           0.80      2640
       macro avg       0.80      0.80      0.80      2640
    weighted avg       0.81      0.80      0.80      2640


    ROC-AUC m√©dio (cross-validation): 0.831 (+/- 0.021)

### Pergunta 6 - An√°lise de import√¢ncia das features

``` {python}
# Feature importance
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

# Visualiza√ß√£o - Top 20 features
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Top 20 features gerais
top20_features = feature_importance.head(20)
ax1.barh(range(len(top20_features)), top20_features['importance'], color='teal')
ax1.set_yticks(range(len(top20_features)))
ax1.set_yticklabels(top20_features['feature'])
ax1.set_xlabel('Import√¢ncia')
ax1.set_title('Top 20 Features Mais Importantes')
ax1.grid(axis='x', alpha=0.3)

# Import√¢ncia das features n√£o-textuais
non_text_features = feature_importance[~feature_importance['feature'].str.startswith('tfidf_')]
ax2.pie(non_text_features['importance'], labels=non_text_features['feature'], 
        autopct='%1.1f%%', startangle=90)
ax2.set_title('Import√¢ncia Relativa das Features Estruturais')

plt.tight_layout()
plt.show()

# Palavras mais importantes
palavras_importantes = feature_importance[feature_importance['feature'].str.startswith('tfidf_')].head(10)
print("\nTop 10 Palavras/Bigramas Mais Importantes:")
for _, row in palavras_importantes.iterrows():
    palavra = row['feature'].replace('tfidf_', '')
    print(f"- {palavra}: {row['importance']:.4f}")
```

![](analise_analistas_parecerista_files/figure-markdown_strict/cell-15-output-1.png)


    Top 10 Palavras/Bigramas Mais Importantes:
    - plataforma: 0.0401
    - integra√ß√£o: 0.0369
    - dados: 0.0292
    - de dados: 0.0275
    - em: 0.0242
    - informa√ß√µes: 0.0235
    - 2021: 0.0176
    - de: 0.0152
    - processos: 0.0109
    - as: 0.0106

### Pergunta 6 - An√°lise de cen√°rios

``` {python}
# Criar fun√ß√£o para prever probabilidade
def prever_probabilidade_aprovacao(area, analista_id, palavras_texto, num_palavras_chave):
    """
    Prev√™ a probabilidade de aprova√ß√£o no parecer dado as caracter√≠sticas do projeto
    """
    # Criar dataframe com uma observa√ß√£o
    novo_projeto = pd.DataFrame({
        'area_encoded': [le_area.transform([area])[0] if area in le_area.classes_ else -1],
        'analista_encoded': [le_analista.transform([analista_id])[0] if analista_id in le_analista.classes_ else -1],
        'num_palavras_chave': [num_palavras_chave],
        'taxa_hist_analista': [stats_analista.loc[analista_id, 'taxa_hist_analista'] if analista_id in stats_analista.index else 0.5]
    })
    
    # TF-IDF do texto
    texto_tfidf = tfidf.transform([palavras_texto])
    texto_df = pd.DataFrame(texto_tfidf.toarray(), 
                           columns=[f'tfidf_{word}' for word in tfidf.get_feature_names_out()])
    
    # Combinar features
    X_novo = pd.concat([novo_projeto, texto_df], axis=1)
    
    # Garantir que tem todas as colunas
    for col in X.columns:
        if col not in X_novo.columns:
            X_novo[col] = 0
    
    X_novo = X_novo[X.columns]
    
    # Prever probabilidade
    proba = rf_model.predict_proba(X_novo)[0, 1]
    return proba

# Testar alguns cen√°rios
print("\n" + "="*80)
print("AN√ÅLISE DE CEN√ÅRIOS - Probabilidade de Aprova√ß√£o no Parecer")
print("="*80)

# Cen√°rio 1: Projeto t√≠pico de alta aprova√ß√£o
area_alta = df_modelo_valido[df_modelo_valido['aprovado_parecer'] == 1]['daproj_dsareaprojeto'].mode()[0]
analista_alta = df_modelo_valido[df_modelo_valido['aprovado_parecer'] == 1]['do_saat_idunicopessoaanalise'].mode()[0]
palavras_alta = ' '.join(palavras_importantes['feature'].str.replace('tfidf_', '').head(5))

prob_alta = prever_probabilidade_aprovacao(area_alta, analista_alta, palavras_alta, 5)
print(f"\nCen√°rio 1 - Perfil de Alta Aprova√ß√£o:")
print(f"  √Årea: {area_alta}")
print(f"  Analista: ID_{int(analista_alta)}")
print(f"  Palavras-chave t√©cnicas: {palavras_alta}")
print(f"  N√∫mero de palavras-chave: 5")
print(f"  ‚Üí Probabilidade de Aprova√ß√£o: {prob_alta:.1%}")

# Cen√°rio 2: Projeto t√≠pico de baixa aprova√ß√£o
area_baixa = df_modelo_valido[df_modelo_valido['aprovado_parecer'] == 0]['daproj_dsareaprojeto'].mode()[0]
analista_baixa = df_modelo_valido[df_modelo_valido['aprovado_parecer'] == 0]['do_saat_idunicopessoaanalise'].mode()[0]

prob_baixa = prever_probabilidade_aprovacao(area_baixa, analista_baixa, 'projeto simples b√°sico', 1)
print(f"\nCen√°rio 2 - Perfil de Baixa Aprova√ß√£o:")
print(f"  √Årea: {area_baixa}")
print(f"  Analista: ID_{int(analista_baixa)}")
print(f"  Palavras-chave t√©cnicas: projeto simples b√°sico")
print(f"  N√∫mero de palavras-chave: 1")
print(f"  ‚Üí Probabilidade de Aprova√ß√£o: {prob_baixa:.1%}")

# Cen√°rio 3: Varia√ß√£o por analista
print(f"\nCen√°rio 3 - Mesmo Projeto, Diferentes Analistas:")
print(f"  √Årea: {area_alta}")
print(f"  Palavras: {palavras_alta}")
print(f"  N√∫mero de palavras-chave: 5")

top5_analistas = taxa_aprovacao_analista.head(5).index
for analista in top5_analistas:
    prob = prever_probabilidade_aprovacao(area_alta, analista, palavras_alta, 5)
    taxa_hist = stats_analista.loc[analista, 'taxa_hist_analista'] * 100
    print(f"  - Analista ID_{int(analista)} (taxa hist√≥rica: {taxa_hist:.1f}%): {prob:.1%}")
```


    ================================================================================
    AN√ÅLISE DE CEN√ÅRIOS - Probabilidade de Aprova√ß√£o no Parecer
    ================================================================================

    Cen√°rio 1 - Perfil de Alta Aprova√ß√£o:
      √Årea: Outros
      Analista: ID_301040479
      Palavras-chave t√©cnicas: plataforma integra√ß√£o dados de dados em
      N√∫mero de palavras-chave: 5
      ‚Üí Probabilidade de Aprova√ß√£o: 32.6%

    Cen√°rio 2 - Perfil de Baixa Aprova√ß√£o:
      √Årea: Software
      Analista: ID_300713489
      Palavras-chave t√©cnicas: projeto simples b√°sico
      N√∫mero de palavras-chave: 1
      ‚Üí Probabilidade de Aprova√ß√£o: 12.8%

    Cen√°rio 3 - Mesmo Projeto, Diferentes Analistas:
      √Årea: Outros
      Palavras: plataforma integra√ß√£o dados de dados em
      N√∫mero de palavras-chave: 5
      - Analista ID_301040479 (taxa hist√≥rica: 59.7%): 32.6%
      - Analista ID_301010505 (taxa hist√≥rica: 69.4%): 34.8%
      - Analista ID_300713489 (taxa hist√≥rica: 64.6%): 31.1%
      - Analista ID_108293000 (taxa hist√≥rica: 65.1%): 32.0%
      - Analista ID_300603544 (taxa hist√≥rica: 53.6%): 23.7%

## An√°lise e Insights da Pergunta 6: Modelo Preditivo de Aprova√ß√£o no Parecer

### O que foi investigado:

Foi desenvolvido um modelo de Machine Learning (Random Forest) para
prever a probabilidade de aprova√ß√£o de projetos na fase de Parecer,
utilizando caracter√≠sticas textuais (TF-IDF), informa√ß√µes estruturais
(√°rea, analista) e m√©tricas hist√≥ricas. O modelo busca identificar quais
fatores mais influenciam a decis√£o final e quantificar o impacto de
diferentes vari√°veis na probabilidade de aprova√ß√£o.

### Principais Descobertas Visuais:

#### 1. **Hierarquia de Import√¢ncia das Features (Gr√°fico √† esquerda):**

O gr√°fico de barras horizontais revela uma hierarquia clara de
influ√™ncia: - **Taxa hist√≥rica do analista** domina com 49.7% da
import√¢ncia relativa, confirmando que o comportamento hist√≥rico √© o
preditor mais forte - **√Årea do projeto** contribui com 38.8%, indicando
forte especializa√ß√£o setorial nas decis√µes - **Caracter√≠sticas
textuais** (termos TF-IDF) ocupam as posi√ß√µes seguintes, com destaque
para ‚Äúplataforma‚Äù (0.0401), ‚Äúintegra√ß√£o‚Äù (0.0369) e ‚Äúdados‚Äù (0.0292)

#### 2. **Distribui√ß√£o de Import√¢ncia das Features Estruturais (Gr√°fico √† direita):**

O gr√°fico de pizza decomp√µe a import√¢ncia relativa das 4 features
n√£o-textuais: - **Taxa hist√≥rica do analista**: 49.7% - O fator
individual mais determinante - **√Årea encoded**: 38.8% - Segundo fator
mais importante, refletindo especializa√ß√£o setorial - **Analista
encoded**: 9.2% - Efeito individual do analista al√©m de sua taxa
hist√≥rica - **N√∫mero de palavras-chave**: 2.3% - Impacto marginal da
quantidade de keywords

### M√©tricas de Performance do Modelo:

-   **ROC-AUC**: 0.831 (¬±0.021) - Excelente capacidade discriminativa
-   **Acur√°cia**: 80% - 4 em cada 5 previs√µes corretas
-   **Recall classe positiva**: 83% - Alta sensibilidade para projetos
    aprovados
-   **Precision classe negativa**: 85% - Alta especificidade para
    projetos reprovados

### An√°lise do Vocabul√°rio Preditivo:

Os termos mais importantes revelam padr√µes lingu√≠sticos associados ao
sucesso: 1. **‚Äúplataforma‚Äù** (0.0401) - Sugere solu√ß√µes tecnol√≥gicas
integradas 2. **‚Äúintegra√ß√£o‚Äù** (0.0369) - Indica complexidade e valor
agregado 3. **‚Äúdados‚Äù** (0.0292) - Reflete foco em transforma√ß√£o digital
4. **‚Äúinforma√ß√µes‚Äù** (0.0235) - Complementa o tema de gest√£o de dados 5.
**‚Äú2021‚Äù** (0.0176) - Poss√≠vel indicador de projetos
recentes/atualizados

### An√°lise de Cen√°rios Preditivos:

#### **Cen√°rio 1 - Perfil Teoricamente Favor√°vel:**

-   √Årea: ‚ÄúOutros‚Äù + Analista top performer + Vocabul√°rio otimizado
-   **Resultado surpreendente**: Apenas 32.6% de probabilidade
-   **Insight**: Mesmo com condi√ß√µes ‚Äúideais‚Äù, o modelo √© conservador,
    sugerindo que fatores n√£o capturados t√™m peso significativo

#### **Cen√°rio 2 - Perfil Desfavor√°vel:**

-   √Årea: ‚ÄúSoftware‚Äù + Analista com hist√≥rico misto + Texto gen√©rico
-   **Resultado**: 12.8% de probabilidade
-   **Insight**: Confirma penaliza√ß√£o forte para projetos com descri√ß√µes
    vagas

#### **Cen√°rio 3 - Varia√ß√£o por Analista:**

-   Mesmo projeto testado com 5 analistas diferentes
-   **Varia√ß√£o**: 23.7% a 34.8% (amplitude de 11.1 pontos percentuais)
-   **Insight cr√≠tico**: A identidade do analista pode alterar a
    probabilidade em at√© 46% (de 23.7% para 34.8%)

### Implica√ß√µes Pr√°ticas e Recomenda√ß√µes:

#### **1. Para Gestores do Programa:**

-   **Sistema de Apoio √† Decis√£o**: Implementar o modelo como ferramenta
    de triagem pr√©via
-   **Calibra√ß√£o de Analistas**: Usar as previs√µes para identificar e
    corrigir vieses individuais
-   **Aloca√ß√£o Inteligente**: Direcionar projetos para analistas com
    perfil adequado

#### **2. Para Empresas Proponentes:**

-   **Otimiza√ß√£o Textual**: Incorporar vocabul√°rio t√©cnico espec√≠fico
    (‚Äúplataforma‚Äù, ‚Äúintegra√ß√£o‚Äù, ‚Äúdados‚Äù)
-   **Sele√ß√£o de √Årea**: Considerar o impacto da classifica√ß√£o setorial
    na probabilidade
-   **Detalhamento T√©cnico**: Evitar descri√ß√µes gen√©ricas; quanto mais
    espec√≠fico, melhor

#### **3. Para Melhoria do Modelo:**

-   **Features Adicionais**: Incorporar m√©tricas financeiras, hist√≥rico
    da empresa, complexidade t√©cnica
-   **Ensemble Methods**: Combinar com outros algoritmos para aumentar
    robustez
-   **An√°lise Temporal**: Incluir tend√™ncias e sazonalidades nas
    aprova√ß√µes

### Limita√ß√µes e Considera√ß√µes:

1.  **Probabilidades Conservadoras**: Mesmo cen√°rios favor√°veis resultam
    em probabilidades \<35%, sugerindo:
    -   Desequil√≠brio nos dados de treino
    -   Fatores ocultos n√£o capturados
    -   Necessidade de recalibra√ß√£o do modelo
2.  **Depend√™ncia do Analista**: A taxa hist√≥rica domina com ~50% da
    import√¢ncia, indicando:
    -   Risco de perpetuar vieses existentes
    -   Necessidade de estrat√©gias de ‚Äúdebiasing‚Äù
    -   Oportunidade para padroniza√ß√£o de crit√©rios
3.  **Vocabul√°rio Dominante**: Termos gen√©ricos (‚Äúdados‚Äù, ‚Äúem‚Äù, ‚Äúde‚Äù) no
    top 10 sugerem:
    -   Necessidade de pr√©-processamento mais sofisticado
    -   Potencial para n-gramas de ordem superior
    -   An√°lise sem√¢ntica al√©m de frequ√™ncia

### Conclus√£o:

O modelo demonstra viabilidade t√©cnica robusta (ROC-AUC 0.831) para
predi√ß√£o automatizada, mas revela depend√™ncia preocupante de fatores
hist√≥ricos e individuais. A domin√¢ncia da ‚Äútaxa hist√≥rica do analista‚Äù
(49.7%) sobre caracter√≠sticas intr√≠nsecas do projeto sugere que o
sistema atual pode estar perpetuando padr√µes passados em vez de avaliar
objetivamente o m√©rito t√©cnico. A implementa√ß√£o pr√°tica deve focar em
usar o modelo como ferramenta de apoio e alerta, n√£o substitui√ß√£o da
an√°lise humana, priorizando a identifica√ß√£o e corre√ß√£o de vieses
sist√™micos.

## 9. Conclus√µes e Insights Principais

``` {python}
# %% Conclus√µes
print("\n" + "="*80)
print("PRINCIPAIS CONCLUS√ïES DA AN√ÅLISE")
print("="*80)

print("\n1. TAXA DE APROVA√á√ÉO POR ANALISTA:")
print(f"   - Taxa m√©dia de aprova√ß√£o (DO): {taxa_aprovacao_analista['Taxa_Aprovacao_%'].mean():.1f}%")
print(f"   - Varia√ß√£o: {taxa_aprovacao_analista['Taxa_Aprovacao_%'].min():.1f}% a {taxa_aprovacao_analista['Taxa_Aprovacao_%'].max():.1f}%")
print(f"   - Desvio padr√£o: {taxa_aprovacao_analista['Taxa_Aprovacao_%'].std():.1f}%")

print("\n2. CONCORD√ÇNCIA DO ‚Üí PARECER:")
print(f"   - Taxa m√©dia de concord√¢ncia: {concordancia_parecer['Taxa_Concordancia_%'].mean():.1f}%")
print(f"   - Analistas com concord√¢ncia < 50%: {(concordancia_parecer['Taxa_Concordancia_%'] < 50).sum()}")

print("\n3. REVERS√ïES DE DECIS√ÉO:")
print(f"   - Taxa m√©dia de revers√£o (N√£o Recom ‚Üí Recom): {media_geral_reversao:.1f}%")
print(f"   - √Åreas com maior revers√£o: {', '.join(reversoes_area.head(3).index)}")

print("\n4. DISPERS√ÉO DE √ÅREAS:")
print(f"   - M√©dia de √°reas por analista: {dispersao_areas['Num_Areas_Diferentes'].mean():.1f}")
print(f"   - M√°ximo de √°reas atendidas: {dispersao_areas['Num_Areas_Diferentes'].max()}")
print(f"   - Analistas com >10 √°reas: {(dispersao_areas['Num_Areas_Diferentes'] > 10).sum()}")

print("\n5. MODELO PREDITIVO:")
print(f"   - Acur√°cia do modelo: {(y_pred == y_test).mean():.1%}")
print(f"   - ROC-AUC: {cv_scores.mean():.3f}")
print(f"   - Features mais importantes: Taxa hist√≥rica do analista, √Årea do projeto, Palavras t√©cnicas")

print("\n6. RECOMENDA√á√ïES:")
print("   - Implementar especializa√ß√£o por √°rea para reduzir dispers√£o")
print("   - Revisar crit√©rios de avalia√ß√£o para aumentar concord√¢ncia DO-Parecer")
print("   - Investigar √°reas com alta taxa de revers√£o para padronizar crit√©rios")
print("   - Considerar balanceamento de carga entre analistas")

# %% Salvar resultados
# Salvar tabelas principais em Excel
with pd.ExcelWriter('analise_analistas_parecerista_resultados.xlsx') as writer:
    taxa_aprovacao_analista.to_excel(writer, sheet_name='Taxa_Aprovacao')
    analise_completa.to_excel(writer, sheet_name='Analise_Completa')
    reversoes_empresa.head(20).to_excel(writer, sheet_name='Reversoes_Empresa')
    reversoes_area.head(20).to_excel(writer, sheet_name='Reversoes_Area')
    dispersao_areas.head(20).to_excel(writer, sheet_name='Dispersao_Areas')
    df_fichas_resumo.to_excel(writer, sheet_name='Fichas_Top15')
    feature_importance.head(30).to_excel(writer, sheet_name='Feature_Importance')

print("\n‚úÖ Resultados salvos em 'analise_analistas_parecerista_resultados.xlsx'")
print("‚úÖ An√°lise conclu√≠da!")
```


    ================================================================================
    PRINCIPAIS CONCLUS√ïES DA AN√ÅLISE
    ================================================================================

    1. TAXA DE APROVA√á√ÉO POR ANALISTA:
       - Taxa m√©dia de aprova√ß√£o (DO): 62.0%
       - Varia√ß√£o: 0.0% a 100.0%
       - Desvio padr√£o: 27.1%

    2. CONCORD√ÇNCIA DO ‚Üí PARECER:
       - Taxa m√©dia de concord√¢ncia: 66.9%
       - Analistas com concord√¢ncia < 50%: 19

    3. REVERS√ïES DE DECIS√ÉO:
       - Taxa m√©dia de revers√£o (N√£o Recom ‚Üí Recom): 4.8%
       - √Åreas com maior revers√£o: Mec√¢nica, Transporte, Constru√ß√£o Civil

    4. DISPERS√ÉO DE √ÅREAS:
       - M√©dia de √°reas por analista: 4.6
       - M√°ximo de √°reas atendidas: 15
       - Analistas com >10 √°reas: 1

    5. MODELO PREDITIVO:
       - Acur√°cia do modelo: 80.0%
       - ROC-AUC: 0.831
       - Features mais importantes: Taxa hist√≥rica do analista, √Årea do projeto, Palavras t√©cnicas

    6. RECOMENDA√á√ïES:
       - Implementar especializa√ß√£o por √°rea para reduzir dispers√£o
       - Revisar crit√©rios de avalia√ß√£o para aumentar concord√¢ncia DO-Parecer
       - Investigar √°reas com alta taxa de revers√£o para padronizar crit√©rios
       - Considerar balanceamento de carga entre analistas

    ‚úÖ Resultados salvos em 'analise_analistas_parecerista_resultados.xlsx'
    ‚úÖ An√°lise conclu√≠da!
