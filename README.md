# An√°lise e Clusteriza√ß√£o de Projetos da Lei do Bem

Este projeto extrai, processa e analisa dados de projetos da Lei do Bem usando t√©cnicas avan√ßadas de Machine Learning. Ele opera em um pipeline de **tr√™s est√°gios** para consolidar dados de um banco PostgreSQL, aplicar clusteriza√ß√£o sem√¢ntica e extrair insights anal√≠ticos dos clusters formados.

## üìã Pr√©-requisitos

- Python 3.9+
- PostgreSQL rodando localmente na porta 5432
- Banco de dados `dbs_mctic2` configurado
- Credenciais: usu√°rio `ia_budy`, senha `ia_budy`
- [uv](https://docs.astral.sh/uv/) - Gerenciador de pacotes Python moderno

## üöÄ Instala√ß√£o

### 1. **Instalar uv (se ainda n√£o tiver):**

```bash
# macOS / Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Windows
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

### 2. **Configurar o projeto:**

```bash
# Clonar/navegar para o diret√≥rio do projeto
cd /home/leomcamilo/projects/test_pgai

# Instalar depend√™ncias e criar ambiente virtual
# Isso instalar√° pandas, sqlalchemy, sentence-transformers, umap-learn, hdbscan, etc.
uv sync

# Ativar ambiente virtual
source .venv/bin/activate  # Linux/macOS
# ou
.venv\Scripts\activate     # Windows
```

## üèóÔ∏è Estrutura do Projeto

```
test_pgai/
‚îú‚îÄ‚îÄ pyproject.toml                   # Configura√ß√£o e depend√™ncias
‚îú‚îÄ‚îÄ .venv/                           # Ambiente virtual (criado pelo uv)
‚îú‚îÄ‚îÄ 1.csv_analise_projetos.py        # Etapa 1: Extra√ß√£o de dados do PostgreSQL
‚îú‚îÄ‚îÄ 2.csv_embedding_cluster.py       # Etapa 2: Clusteriza√ß√£o com embeddings sem√¢nticos
‚îú‚îÄ‚îÄ 3.csv_analise_cluster.py         # Etapa 3: An√°lise e extra√ß√£o de insights
‚îú‚îÄ‚îÄ tabelas_csv_xlsx/                # Dados brutos extra√≠dos (CSV/Excel)
‚îú‚îÄ‚îÄ json_outputs/                    # Sa√≠das em formato JSON
‚îî‚îÄ‚îÄ README.md                        # Este arquivo
```

## üìä Pipeline Completo de An√°lise

O processo √© dividido em **tr√™s etapas** que devem ser executadas sequencialmente:

### üîß Etapa 1: Extra√ß√£o de Dados do Banco

**Arquivo:** `1.csv_analise_projetos.py`

Conecta ao PostgreSQL, executa consulta otimizada e exporta dados consolidados dos projetos da Lei do Bem de 2023.

```bash
# Ativar ambiente (se n√£o estiver ativo)
source .venv/bin/activate

# Executar extra√ß√£o de dados
python 1.csv_analise_projetos.py
```

**Sa√≠das geradas:**
- `tabelas_csv_xlsx/projetos_lei_do_bem_2023_TODAS_AS_EMPRESAS.csv`
- `tabelas_csv_xlsx/projetos_lei_do_bem_2023_TODAS_AS_EMPRESAS.xlsx`

### ü§ñ Etapa 2: Clusteriza√ß√£o com IA

**Arquivo:** `2.csv_embedding_cluster.py`

Aplica t√©cnicas avan√ßadas de NLP e Machine Learning para agrupar projetos similares semanticamente.

```bash
# Executar clusteriza√ß√£o
python 2.csv_embedding_cluster.py
```

**Caracter√≠sticas t√©cnicas:**
- **Modelo de Embeddings:** SERAFIM (portugu√™s especializado) - `PORTULAN/serafim-335m-portuguese-pt-sentence-encoder-ir`
- **Pr√©-processamento:** Stop words customizadas (NLTK + dom√≠nio P&D)
- **Clustering:** Aglomerativo com threshold otimizado (0.58)
- **Extra√ß√£o:** Foco em conte√∫do t√©cnico (descri√ß√£o, metodologia, desafio tecnol√≥gico)

**Sa√≠das geradas:**
- `projetos_com_clusters_final.csv` - Dataset completo com clusters
- Estat√≠sticas detalhadas de todos os clusters

### üìà Etapa 3: An√°lise de Clusters

**Arquivo:** `3.csv_analise_cluster.py`

Extrai informa√ß√µes estruturadas dos clusters e gera an√°lises estat√≠sticas detalhadas.

```bash
# Executar an√°lise de clusters
python 3.csv_analise_cluster.py
```

**An√°lises realizadas:**
- Extra√ß√£o de nomes e descri√ß√µes dos projetos
- Identifica√ß√£o de raz√µes sociais das empresas
- Distribui√ß√£o por tamanho de cluster
- Top empresas por quantidade de projetos
- Estat√≠sticas de qualidade da clusteriza√ß√£o

**Sa√≠da gerada:**
- `clusters_projetos.csv` - Dados estruturados para an√°lise

## ü§ñ Detalhes T√©cnicos do Pipeline de ML

### Processamento de Texto
1. **Extra√ß√£o de Conte√∫do T√©cnico**: Regex patterns para capturar descri√ß√£o, metodologia, desafio tecnol√≥gico
2. **Limpeza com Stop Words**: Uni√£o de NLTK (portugu√™s) + palavras espec√≠ficas do dom√≠nio P&D
3. **Preserva√ß√£o de Contexto**: Mant√©m termos t√©cnicos, c√≥digos e palavras > 3 caracteres

### Gera√ß√£o de Embeddings
- **Modelo:** SERAFIM-335M (especializado em portugu√™s)
- **Dimensionalidade:** 768 dimens√µes por projeto
- **Foco:** Apenas conte√∫do t√©cnico relevante

### Clustering Aglomerativo
- **Algoritmo:** AgglomerativeClustering (scikit-learn)
- **M√©trica:** Cosine similarity
- **Linkage:** Average (otimizado para embeddings sem√¢nticos)
- **Threshold:** 0.58 (otimizado para cobertura 100%)

## üìÅ Arquivos de Sa√≠da Detalhados

### Etapa 1 - Extra√ß√£o
- **CSV:** `tabelas_csv_xlsx/projetos_lei_do_bem_2023_TODAS_AS_EMPRESAS.csv`
- **Excel:** `tabelas_csv_xlsx/projetos_lei_do_bem_2023_TODAS_AS_EMPRESAS.xlsx`
- **Conte√∫do:** Dados brutos consolidados do PostgreSQL

### Etapa 2 - Clusteriza√ß√£o
- **Principal:** `projetos_com_clusters_final.csv`
- **Colunas adicionais:** `texto_tecnico_bruto`, `texto_tecnico`, `cluster`
- **Estat√≠sticas:** Distribui√ß√£o completa de todos os clusters

### Etapa 3 - An√°lise
- **Resultado:** `clusters_projetos.csv`
- **Colunas:** `nome_projeto`, `descricao_projeto`, `razaosocial_empresa`, `cluster`
- **Formato:** Dados estruturados para an√°lise e visualiza√ß√£o

## üìä Exemplo de Uso Completo

```bash
# Pipeline completo
source .venv/bin/activate

# 1. Extrair dados do banco
python 1.csv_analise_projetos.py

# 2. Aplicar clusteriza√ß√£o
python 2.csv_embedding_cluster.py

# 3. Analisar resultados
python 3.csv_analise_cluster.py

# Verificar resultados
ls -la projetos_com_clusters_final.csv
ls -la clusters_projetos.csv
```

## üÜò Troubleshooting

### Erro de Conex√£o com Banco (Etapa 1)
```
psycopg2.OperationalError: could not connect to server
```
**Solu√ß√£o**: Verifique se PostgreSQL est√° ativo e credenciais est√£o corretas.

### Erro de Arquivo N√£o Encontrado (Etapa 2)
```
FileNotFoundError: Nenhum arquivo CSV encontrado na pasta tabelas_csv_xlsx
```
**Solu√ß√£o**: Execute primeiro a Etapa 1 (`1.csv_analise_projetos.py`).

### Erro de Parser (Etapa 3)
```
FileNotFoundError: Arquivo 'projetos_com_clusters_final.csv' n√£o encontrado
```
**Solu√ß√£o**: Execute primeiro a Etapa 2 (`2.csv_embedding_cluster.py`).

### Problemas com NLTK
```bash
# Download manual das stopwords
python -c "import nltk; nltk.download('stopwords')"
```

### Problemas com `uv`
```bash
# Recriar ambiente virtual
rm -rf .venv
uv sync

# Limpar cache do uv
uv clean
```

## üîÑ Desenvolvimento e Customiza√ß√£o

### Ajustar Threshold de Clustering
Edite `2.csv_embedding_cluster.py`, linha com `threshold = 0.58`:
```python
# Para mais clusters (menor threshold)
threshold = 0.45

# Para menos clusters (maior threshold)  
threshold = 0.70
```

### Adicionar Novas Stop Words
Edite `stop_words_personalizadas` em `2.csv_embedding_cluster.py`:
```python
stop_words_personalizadas.update({
    'nova_palavra', 'outro_termo', 'palavra_especifica'
})
```

### Adicionando Depend√™ncias
```bash
source .venv/bin/activate
uv add matplotlib seaborn plotly  # Para visualiza√ß√µes
uv sync
```

## üìà M√©tricas de Performance

- **Cobertura:** 100% dos projetos s√£o clusterizados
- **Escalabilidade:** Testado com at√© 75k projetos
- **Qualidade:** Threshold otimizado para m√°xima separa√ß√£o sem√¢ntica
- **Velocidade:** Processamento eficiente com embeddings pr√©-treinados

## üéØ Pr√≥ximos Passos

1. **Visualiza√ß√£o:** Implementar dashboard interativo dos clusters
2. **API:** Criar endpoint para clusteriza√ß√£o em tempo real
3. **Modelos:** Testar outros modelos de embedding portugu√™s
4. **An√°lise:** Adicionar an√°lise de tend√™ncias temporais
